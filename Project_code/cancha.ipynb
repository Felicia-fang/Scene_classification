{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\dl\\lib\\site-packages\\ipykernel_launcher.py:50: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "E:\\anaconda\\envs\\dl\\lib\\site-packages\\ipykernel_launcher.py:60: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小： 86  测试集大小： 80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import random\n",
    "\n",
    "\n",
    "num_classes = 80  # 分类数量\n",
    "batch_size = 2\n",
    "num_epochs = 10  # 训练轮次\n",
    "lr = 0.02\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 读取并展示图片\n",
    "\n",
    "file_root = \"D:/aaaaaaaaaaaaaaaaa/nus/fyp/Q2A-master/Q2A-master/encoder/configs/train\"\n",
    "classes = ['aircon_utr3b','airfryer_gye82','airfryer_pe2j7','airfryer_w9rzm','bicycle_g8h94','blender_d4og8','blender_tg2xq','blender_zuw28','camera_4paj0','camera_9awdp','camera_a409h','coffeemachine_d2stw','dehydrator_jvzgp','diffuser_lxcd4','dryer_am5jp','dryer_d4uqs','dryer_vc1kl','inductioncooker_bjye3','inductioncooker_v0jzx','inductioncooker_veifx','kiettle_rc3pf','kitchenscale_025qs','kitchenscale_7shbw','kitchenscale_jsgih','kitchenscale_pqejy','kitchenscale_wk150','kitchentimer_fr3ld','kitchentimer_nr0vk','lightstand_ro8fj','microwave_clhpa','microwave_etrc9','microwave_gz61t','microwave_h43qm','microwave_kflra','microwave_ljbak','microwave_m0fgh','microwave_m5vq3','microwave_r5h7q','microwave_y3fpx','microwave_y693a','microwave_yw0gr','microwave_znl6u','mixer_muhce','oven_968hd','oven_e7fsy','oven_g6xvo','oven_lhap5','oven_un32d','oven_wa67l','oven_wn85g','printer_5jpry','printer_bf69n','rangehood_2pk8j','ricecooker_26ax0','ricecooker_5apek','ricecooker_af46c','ricecooker_tlvys','ricecooker_tomn0','ricecooker_zxuqy','toaster_ja9zg','toaster_xwc03','treadmill_npdev','treadmill_nu3ob','vacuum_1csuz','washingmachine_735oe','washingmachine_8fzkt','washingmachine_dz980','washingmachine_gxblk','washingmachine_h4n8j','washingmachine_kc4eb','washingmachine_kstcf','washingmachine_m79j0','washingmachine_tyap1','washingmachine_ujs4r','washingmachine_uomyf','washingmachine_wtbih','watch_0ku25','watch_c8zhg','watch_yw2mz','waterpurifier_b2j3o']\n",
    "nums = [2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]  # 每种类别的个数\n",
    "\n",
    "def read_data(path):\n",
    "    file_name = os.listdir(path)  # 获取所有文件的文件名称\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    # 每个类别随机抽取20%作为测试集\n",
    "    train_num = [int(num * 1 / 2) for num in nums]\n",
    "    test_num = [nums[i] - train_num[i] for i in range(len(nums))]\n",
    "\n",
    "    for idx, f_name in enumerate(file_name):  # 每个类别一个idx，即以idx作为标签\n",
    "        im_dirs = path + '/' + f_name+'/images'\n",
    "        im_path = os.listdir(im_dirs)  # 每个不同类别图像文件夹下所有图像的名称\n",
    "\n",
    "        index = list(range(len(im_path)))\n",
    "        random.shuffle(index)  # 打乱顺序\n",
    "        im_path_ = list(np.array(im_path)[index])\n",
    "        test_path = im_path_[:test_num[idx]]  # 测试数据的路径\n",
    "        train_path = im_path_[test_num[idx]:]  # 训练数据的路径\n",
    "\n",
    "        for img_name in train_path:\n",
    "            # 会读到desktop.ini,要去掉\n",
    "            if img_name == 'desktop.ini':\n",
    "                continue\n",
    "            img = Image.open(im_dirs + '/' + img_name).convert(\"RGB\")  # img shape: (120, 85, 3) 高、宽、通道\n",
    "            # 对图片进行变形\n",
    "            img = img.resize((256, 256), Image.ANTIALIAS)  # 宽、高\n",
    "            train_data.append(img)\n",
    "            train_labels.append(idx)\n",
    "\n",
    "        for img_name in test_path:\n",
    "            # 会读到desktop.ini,要去掉\n",
    "            if img_name == 'desktop.ini':\n",
    "                continue\n",
    "            img = Image.open(im_dirs + '/' + img_name).convert(\"RGB\")  # img shape: (120, 85, 3) 高、宽、通道\n",
    "            # 对图片进行变形\n",
    "            img = img.resize((256, 256), Image.ANTIALIAS)  # 宽、高\n",
    "            test_data.append(img)\n",
    "            test_labels.append(idx)\n",
    "\n",
    "    print('训练集大小：', len(train_data), ' 测试集大小：', len(test_data))\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# 一次性读取全部的数据\n",
    "train_data, train_labels, test_data, test_labels = read_data(file_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # 变为tensor\n",
    "     # 对数据按通道进行标准化，即减去均值，再除以方差, [0-1]->[-1,1]\n",
    "     transforms.Normalize(mean=[0.4686, 0.4853, 0.5193], std=[0.1720, 0.1863, 0.2175])\n",
    "     ]\n",
    ")\n",
    "\n",
    "\n",
    "# 自定义Dataset类实现每次取出图片，将PIL转换为Tensor\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label, trans):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):  # 根据索引返回数据和对应的标签\n",
    "        return self.trans(self.data[index]), self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 调用自己创建的Dataset\n",
    "train_dataset = MyDataset(train_data, train_labels, transform)\n",
    "test_dataset = MyDataset(test_data, test_labels, transform)\n",
    "\n",
    "# 生成data loader\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "def train(net, data_loader, device):\n",
    "    net.train()  # 指定为训练模式\n",
    "    train_batch_num = len(data_loader)\n",
    "    total_loss = 0.0\n",
    "    correct = 0  # 记录共有多少个样本被正确分类\n",
    "    sample_num = 0\n",
    "\n",
    "    # 遍历每个batch进行训练\n",
    "    for data, target in data_loader:\n",
    "        # 将图片和标签放入指定的device中\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # 将当前梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 使用模型计算出结果\n",
    "        y_hat = net(data)\n",
    "        # 计算损失\n",
    "        loss_ = loss(y_hat, target)\n",
    "        # 进行反向传播\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_.item()\n",
    "        cor = (torch.argmax(y_hat, 1) == target).sum().item()\n",
    "        correct += cor\n",
    "        # 累加当前的样本总数\n",
    "        sample_num += target.shape[0]\n",
    "        print('loss: %.4f  acc: %.4f' % (loss_.item(), cor / target.shape[0]))\n",
    "    # 平均loss和准确率\n",
    "    loss_ = total_loss / train_batch_num\n",
    "    acc = correct / sample_num\n",
    "    return loss_, acc\n",
    "\n",
    "\n",
    "# 测试\n",
    "def test(net, data_loader, device):\n",
    "    net.eval()  # 指定当前模式为测试模式（针对BN层和dropout层）\n",
    "    test_batch_num = len(data_loader)\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    sample_num = 0\n",
    "    # 指定不进行梯度计算（没有反向传播也会计算梯度，增大GPU开销\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = net(data)\n",
    "            loss_ = loss(output, target)\n",
    "            total_loss += loss_.item()\n",
    "            correct += (torch.argmax(output, 1) == target).sum().item()\n",
    "            sample_num += target.shape[0]\n",
    "    loss_ = total_loss / test_batch_num\n",
    "    acc = correct / sample_num\n",
    "    return loss_, acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.0006  acc: 0.5000\n",
      "loss: 5.1686  acc: 0.0000\n",
      "loss: 5.0278  acc: 0.0000\n",
      "loss: 5.6936  acc: 0.0000\n",
      "loss: 5.6209  acc: 0.0000\n",
      "loss: 5.1336  acc: 0.0000\n",
      "loss: 4.8419  acc: 0.0000\n",
      "loss: 6.1677  acc: 0.0000\n",
      "loss: 5.5139  acc: 0.0000\n",
      "loss: 5.6611  acc: 0.0000\n",
      "loss: 5.6047  acc: 0.0000\n",
      "loss: 6.4731  acc: 0.0000\n",
      "loss: 5.6566  acc: 0.0000\n",
      "loss: 5.9834  acc: 0.0000\n",
      "loss: 5.3027  acc: 0.0000\n",
      "loss: 5.4392  acc: 0.0000\n",
      "loss: 5.5734  acc: 0.0000\n",
      "loss: 5.3309  acc: 0.0000\n",
      "loss: 5.0468  acc: 0.0000\n",
      "loss: 5.9213  acc: 0.0000\n",
      "loss: 5.3065  acc: 0.0000\n",
      "loss: 5.4870  acc: 0.0000\n",
      "loss: 5.4636  acc: 0.0000\n",
      "loss: 5.3463  acc: 0.0000\n",
      "loss: 5.7878  acc: 0.0000\n",
      "loss: 5.4157  acc: 0.0000\n",
      "loss: 5.4062  acc: 0.0000\n",
      "loss: 5.8862  acc: 0.0000\n",
      "loss: 5.5646  acc: 0.0000\n",
      "loss: 6.0762  acc: 0.0000\n",
      "loss: 5.0983  acc: 0.0000\n",
      "loss: 5.7588  acc: 0.0000\n",
      "loss: 5.4123  acc: 0.0000\n",
      "loss: 5.2159  acc: 0.0000\n",
      "loss: 5.4301  acc: 0.0000\n",
      "loss: 5.7508  acc: 0.0000\n",
      "loss: 5.2808  acc: 0.0000\n",
      "loss: 5.5327  acc: 0.0000\n",
      "loss: 4.7753  acc: 0.0000\n",
      "loss: 5.5485  acc: 0.0000\n",
      "loss: 5.8406  acc: 0.0000\n",
      "loss: 4.8267  acc: 0.0000\n",
      "loss: 5.6458  acc: 0.0000\n",
      "epoch 1, train loss: 5.4655, train acc: 0.012\n",
      "test loss: 22.8289, test acc: 0.013\n",
      "loss: 4.9624  acc: 0.0000\n",
      "loss: 5.1724  acc: 0.0000\n",
      "loss: 3.1233  acc: 0.0000\n",
      "loss: 5.6788  acc: 0.0000\n",
      "loss: 4.6151  acc: 0.0000\n",
      "loss: 4.2777  acc: 0.0000\n",
      "loss: 4.8092  acc: 0.0000\n",
      "loss: 4.1937  acc: 0.0000\n",
      "loss: 4.9754  acc: 0.0000\n",
      "loss: 4.9971  acc: 0.0000\n",
      "loss: 5.7732  acc: 0.0000\n",
      "loss: 5.1063  acc: 0.0000\n",
      "loss: 4.8538  acc: 0.0000\n",
      "loss: 4.9199  acc: 0.0000\n",
      "loss: 5.1253  acc: 0.0000\n",
      "loss: 5.2289  acc: 0.0000\n",
      "loss: 4.4437  acc: 0.0000\n",
      "loss: 3.5183  acc: 0.5000\n",
      "loss: 5.8630  acc: 0.0000\n",
      "loss: 5.2933  acc: 0.0000\n",
      "loss: 4.1719  acc: 0.0000\n",
      "loss: 5.0177  acc: 0.0000\n",
      "loss: 5.4425  acc: 0.0000\n",
      "loss: 5.3413  acc: 0.0000\n",
      "loss: 4.6777  acc: 0.0000\n",
      "loss: 5.2286  acc: 0.0000\n",
      "loss: 4.5971  acc: 0.0000\n",
      "loss: 4.8489  acc: 0.0000\n",
      "loss: 4.6889  acc: 0.0000\n",
      "loss: 5.0269  acc: 0.0000\n",
      "loss: 5.0741  acc: 0.0000\n",
      "loss: 4.7772  acc: 0.0000\n",
      "loss: 4.5128  acc: 0.0000\n",
      "loss: 5.2102  acc: 0.0000\n",
      "loss: 4.8247  acc: 0.0000\n",
      "loss: 3.9447  acc: 0.0000\n",
      "loss: 5.6923  acc: 0.0000\n",
      "loss: 5.2213  acc: 0.0000\n",
      "loss: 5.2162  acc: 0.0000\n",
      "loss: 5.1997  acc: 0.0000\n",
      "loss: 5.4081  acc: 0.0000\n",
      "loss: 5.0674  acc: 0.0000\n",
      "loss: 5.0317  acc: 0.0000\n",
      "epoch 2, train loss: 4.9105, train acc: 0.012\n",
      "test loss: 5.1381, test acc: 0.025\n",
      "loss: 4.6056  acc: 0.0000\n",
      "loss: 4.0946  acc: 0.0000\n",
      "loss: 4.1566  acc: 0.0000\n",
      "loss: 4.6277  acc: 0.0000\n",
      "loss: 3.7526  acc: 0.0000\n",
      "loss: 3.8384  acc: 0.0000\n",
      "loss: 4.7119  acc: 0.0000\n",
      "loss: 4.3008  acc: 0.0000\n",
      "loss: 4.8185  acc: 0.0000\n",
      "loss: 4.8069  acc: 0.0000\n",
      "loss: 4.5702  acc: 0.0000\n",
      "loss: 4.6753  acc: 0.0000\n",
      "loss: 4.9351  acc: 0.0000\n",
      "loss: 5.0461  acc: 0.0000\n",
      "loss: 4.7467  acc: 0.0000\n",
      "loss: 4.5125  acc: 0.0000\n",
      "loss: 4.5991  acc: 0.0000\n",
      "loss: 4.5315  acc: 0.0000\n",
      "loss: 4.4685  acc: 0.0000\n",
      "loss: 3.9561  acc: 0.5000\n",
      "loss: 4.1975  acc: 0.0000\n",
      "loss: 4.2592  acc: 0.0000\n",
      "loss: 3.5643  acc: 0.5000\n",
      "loss: 5.0500  acc: 0.0000\n",
      "loss: 4.6211  acc: 0.0000\n",
      "loss: 4.9117  acc: 0.0000\n",
      "loss: 4.8733  acc: 0.0000\n",
      "loss: 4.3840  acc: 0.0000\n",
      "loss: 5.0607  acc: 0.0000\n",
      "loss: 5.2480  acc: 0.0000\n",
      "loss: 4.7504  acc: 0.0000\n",
      "loss: 4.8643  acc: 0.0000\n",
      "loss: 4.8412  acc: 0.0000\n",
      "loss: 4.9102  acc: 0.0000\n",
      "loss: 4.6383  acc: 0.0000\n",
      "loss: 5.4287  acc: 0.0000\n",
      "loss: 4.6892  acc: 0.0000\n",
      "loss: 4.0371  acc: 0.5000\n",
      "loss: 4.3173  acc: 0.0000\n",
      "loss: 5.3597  acc: 0.0000\n",
      "loss: 4.9053  acc: 0.0000\n",
      "loss: 4.8633  acc: 0.0000\n",
      "loss: 5.0966  acc: 0.0000\n",
      "epoch 3, train loss: 4.6192, train acc: 0.035\n",
      "test loss: 4.8526, test acc: 0.025\n",
      "loss: 3.7994  acc: 0.5000\n",
      "loss: 4.2315  acc: 0.0000\n",
      "loss: 4.2920  acc: 0.0000\n",
      "loss: 3.8722  acc: 0.0000\n",
      "loss: 4.4440  acc: 0.0000\n",
      "loss: 4.6786  acc: 0.0000\n",
      "loss: 4.4407  acc: 0.0000\n",
      "loss: 4.2290  acc: 0.0000\n",
      "loss: 4.2010  acc: 0.0000\n",
      "loss: 4.1693  acc: 0.0000\n",
      "loss: 5.0185  acc: 0.0000\n",
      "loss: 3.6032  acc: 0.5000\n",
      "loss: 3.3623  acc: 0.5000\n",
      "loss: 4.4845  acc: 0.0000\n",
      "loss: 4.4571  acc: 0.0000\n",
      "loss: 4.6107  acc: 0.0000\n",
      "loss: 5.6033  acc: 0.0000\n",
      "loss: 4.5119  acc: 0.0000\n",
      "loss: 4.6111  acc: 0.0000\n",
      "loss: 4.7262  acc: 0.0000\n",
      "loss: 4.7140  acc: 0.0000\n",
      "loss: 4.8920  acc: 0.0000\n",
      "loss: 4.4139  acc: 0.0000\n",
      "loss: 4.6799  acc: 0.0000\n",
      "loss: 4.3811  acc: 0.0000\n",
      "loss: 4.8072  acc: 0.0000\n",
      "loss: 4.5256  acc: 0.0000\n",
      "loss: 4.7707  acc: 0.0000\n",
      "loss: 4.8007  acc: 0.0000\n",
      "loss: 3.8555  acc: 0.0000\n",
      "loss: 4.0355  acc: 0.0000\n",
      "loss: 4.1240  acc: 0.0000\n",
      "loss: 5.2309  acc: 0.0000\n",
      "loss: 5.0405  acc: 0.0000\n",
      "loss: 4.7221  acc: 0.0000\n",
      "loss: 4.5834  acc: 0.0000\n",
      "loss: 3.8233  acc: 0.5000\n",
      "loss: 5.0708  acc: 0.0000\n",
      "loss: 4.5283  acc: 0.0000\n",
      "loss: 4.9515  acc: 0.0000\n",
      "loss: 5.0004  acc: 0.0000\n",
      "loss: 4.8413  acc: 0.0000\n",
      "loss: 5.0435  acc: 0.0000\n",
      "epoch 4, train loss: 4.5159, train acc: 0.047\n",
      "test loss: 5.7866, test acc: 0.013\n",
      "loss: 4.2479  acc: 0.0000\n",
      "loss: 4.3120  acc: 0.0000\n",
      "loss: 3.6401  acc: 0.0000\n",
      "loss: 4.3324  acc: 0.0000\n",
      "loss: 3.7418  acc: 0.0000\n",
      "loss: 4.0583  acc: 0.0000\n",
      "loss: 2.9084  acc: 0.5000\n",
      "loss: 5.0034  acc: 0.0000\n",
      "loss: 3.6775  acc: 0.0000\n",
      "loss: 4.4359  acc: 0.0000\n",
      "loss: 4.0694  acc: 0.0000\n",
      "loss: 4.6146  acc: 0.0000\n",
      "loss: 3.9400  acc: 0.0000\n",
      "loss: 4.3883  acc: 0.0000\n",
      "loss: 3.4703  acc: 0.5000\n",
      "loss: 4.2511  acc: 0.0000\n",
      "loss: 4.4874  acc: 0.0000\n",
      "loss: 4.5893  acc: 0.0000\n",
      "loss: 4.7139  acc: 0.0000\n",
      "loss: 4.2820  acc: 0.0000\n",
      "loss: 4.6735  acc: 0.0000\n",
      "loss: 4.9087  acc: 0.0000\n",
      "loss: 4.5532  acc: 0.0000\n",
      "loss: 4.3819  acc: 0.0000\n",
      "loss: 4.7111  acc: 0.0000\n",
      "loss: 4.5099  acc: 0.0000\n",
      "loss: 4.5205  acc: 0.0000\n",
      "loss: 4.8301  acc: 0.0000\n",
      "loss: 4.5880  acc: 0.0000\n",
      "loss: 4.6700  acc: 0.0000\n",
      "loss: 4.5468  acc: 0.0000\n",
      "loss: 5.3036  acc: 0.0000\n",
      "loss: 4.7298  acc: 0.0000\n",
      "loss: 4.6385  acc: 0.0000\n",
      "loss: 4.7484  acc: 0.0000\n",
      "loss: 4.1845  acc: 0.5000\n",
      "loss: 4.6240  acc: 0.0000\n",
      "loss: 4.1388  acc: 0.0000\n",
      "loss: 4.8275  acc: 0.0000\n",
      "loss: 4.8029  acc: 0.0000\n",
      "loss: 4.6787  acc: 0.0000\n",
      "loss: 4.4644  acc: 0.0000\n",
      "loss: 4.9608  acc: 0.0000\n",
      "epoch 5, train loss: 4.4223, train acc: 0.035\n",
      "test loss: 4.4461, test acc: 0.013\n",
      "loss: 4.1533  acc: 0.0000\n",
      "loss: 3.7929  acc: 0.0000\n",
      "loss: 4.0948  acc: 0.0000\n",
      "loss: 4.2446  acc: 0.0000\n",
      "loss: 3.5139  acc: 0.5000\n",
      "loss: 4.6161  acc: 0.0000\n",
      "loss: 4.4951  acc: 0.0000\n",
      "loss: 4.2606  acc: 0.0000\n",
      "loss: 4.0202  acc: 0.0000\n",
      "loss: 3.2438  acc: 0.5000\n",
      "loss: 4.4599  acc: 0.0000\n",
      "loss: 4.7670  acc: 0.0000\n",
      "loss: 4.2557  acc: 0.0000\n",
      "loss: 4.0708  acc: 0.0000\n",
      "loss: 4.7558  acc: 0.0000\n",
      "loss: 3.9933  acc: 0.0000\n",
      "loss: 4.5018  acc: 0.0000\n",
      "loss: 4.1305  acc: 0.0000\n",
      "loss: 4.2068  acc: 0.0000\n",
      "loss: 4.3908  acc: 0.0000\n",
      "loss: 4.2808  acc: 0.0000\n",
      "loss: 4.3919  acc: 0.0000\n",
      "loss: 4.5018  acc: 0.0000\n",
      "loss: 4.7541  acc: 0.0000\n",
      "loss: 3.9150  acc: 0.0000\n",
      "loss: 4.8798  acc: 0.0000\n",
      "loss: 4.4986  acc: 0.0000\n",
      "loss: 4.0685  acc: 0.0000\n",
      "loss: 3.4079  acc: 0.0000\n",
      "loss: 4.2443  acc: 0.0000\n",
      "loss: 4.4442  acc: 0.0000\n",
      "loss: 4.4932  acc: 0.0000\n",
      "loss: 4.0204  acc: 0.0000\n",
      "loss: 4.6932  acc: 0.0000\n",
      "loss: 3.5744  acc: 0.5000\n",
      "loss: 4.6963  acc: 0.0000\n",
      "loss: 4.6385  acc: 0.0000\n",
      "loss: 4.6661  acc: 0.0000\n",
      "loss: 3.9752  acc: 0.0000\n",
      "loss: 5.7172  acc: 0.0000\n",
      "loss: 4.7375  acc: 0.0000\n",
      "loss: 3.2327  acc: 0.5000\n",
      "loss: 4.6573  acc: 0.0000\n",
      "epoch 6, train loss: 4.2897, train acc: 0.047\n",
      "test loss: 4.4207, test acc: 0.013\n",
      "loss: 3.8398  acc: 0.0000\n",
      "loss: 3.6211  acc: 0.5000\n",
      "loss: 4.1878  acc: 0.0000\n",
      "loss: 3.9105  acc: 0.0000\n",
      "loss: 4.3859  acc: 0.0000\n",
      "loss: 3.9543  acc: 0.0000\n",
      "loss: 4.5790  acc: 0.0000\n",
      "loss: 3.3666  acc: 0.0000\n",
      "loss: 4.0346  acc: 0.0000\n",
      "loss: 4.0999  acc: 0.0000\n",
      "loss: 4.7888  acc: 0.0000\n",
      "loss: 4.2118  acc: 0.0000\n",
      "loss: 4.5898  acc: 0.0000\n",
      "loss: 3.6189  acc: 0.5000\n",
      "loss: 4.2899  acc: 0.0000\n",
      "loss: 4.4826  acc: 0.0000\n",
      "loss: 4.2422  acc: 0.0000\n",
      "loss: 4.2144  acc: 0.0000\n",
      "loss: 4.4747  acc: 0.0000\n",
      "loss: 3.9591  acc: 0.0000\n",
      "loss: 4.7833  acc: 0.0000\n",
      "loss: 3.9672  acc: 0.0000\n",
      "loss: 4.5428  acc: 0.0000\n",
      "loss: 4.8254  acc: 0.0000\n",
      "loss: 4.3495  acc: 0.0000\n",
      "loss: 3.8659  acc: 0.0000\n",
      "loss: 4.0017  acc: 0.0000\n",
      "loss: 3.2765  acc: 0.0000\n",
      "loss: 4.2162  acc: 0.0000\n",
      "loss: 4.3015  acc: 0.0000\n",
      "loss: 2.8268  acc: 0.5000\n",
      "loss: 4.2920  acc: 0.0000\n",
      "loss: 4.7457  acc: 0.0000\n",
      "loss: 4.4339  acc: 0.0000\n",
      "loss: 4.1233  acc: 0.0000\n",
      "loss: 4.5506  acc: 0.0000\n",
      "loss: 4.9172  acc: 0.0000\n",
      "loss: 4.0595  acc: 0.0000\n",
      "loss: 3.1382  acc: 0.0000\n",
      "loss: 4.9823  acc: 0.0000\n",
      "loss: 3.3511  acc: 0.5000\n",
      "loss: 4.5588  acc: 0.0000\n",
      "loss: 4.7267  acc: 0.0000\n",
      "epoch 7, train loss: 4.1788, train acc: 0.047\n",
      "test loss: 4.4040, test acc: 0.025\n",
      "loss: 2.5623  acc: 0.5000\n",
      "loss: 4.1827  acc: 0.0000\n",
      "loss: 4.0860  acc: 0.0000\n",
      "loss: 3.0598  acc: 0.0000\n",
      "loss: 3.9504  acc: 0.0000\n",
      "loss: 3.3584  acc: 0.0000\n",
      "loss: 4.3190  acc: 0.0000\n",
      "loss: 3.2016  acc: 0.0000\n",
      "loss: 4.1631  acc: 0.0000\n",
      "loss: 3.9865  acc: 0.0000\n",
      "loss: 3.9554  acc: 0.0000\n",
      "loss: 4.5749  acc: 0.0000\n",
      "loss: 3.3523  acc: 0.0000\n",
      "loss: 3.7777  acc: 0.0000\n",
      "loss: 4.0263  acc: 0.0000\n",
      "loss: 4.3332  acc: 0.0000\n",
      "loss: 4.4875  acc: 0.0000\n",
      "loss: 4.1763  acc: 0.0000\n",
      "loss: 3.2447  acc: 0.5000\n",
      "loss: 3.2192  acc: 0.0000\n",
      "loss: 4.1201  acc: 0.0000\n",
      "loss: 4.5132  acc: 0.0000\n",
      "loss: 4.2832  acc: 0.0000\n",
      "loss: 3.3516  acc: 0.0000\n",
      "loss: 3.4853  acc: 0.0000\n",
      "loss: 4.6766  acc: 0.0000\n",
      "loss: 4.8682  acc: 0.0000\n",
      "loss: 3.9433  acc: 0.0000\n",
      "loss: 4.6418  acc: 0.0000\n",
      "loss: 2.8902  acc: 0.5000\n",
      "loss: 4.4018  acc: 0.0000\n",
      "loss: 4.3220  acc: 0.0000\n",
      "loss: 4.7066  acc: 0.0000\n",
      "loss: 4.8472  acc: 0.0000\n",
      "loss: 4.8236  acc: 0.0000\n",
      "loss: 4.0487  acc: 0.0000\n",
      "loss: 4.5736  acc: 0.0000\n",
      "loss: 4.6738  acc: 0.0000\n",
      "loss: 4.4041  acc: 0.0000\n",
      "loss: 4.5029  acc: 0.0000\n",
      "loss: 4.5521  acc: 0.0000\n",
      "loss: 3.6418  acc: 0.5000\n",
      "loss: 4.5458  acc: 0.0000\n",
      "epoch 8, train loss: 4.0659, train acc: 0.047\n",
      "test loss: 4.4877, test acc: 0.062\n",
      "loss: 3.8444  acc: 0.0000\n",
      "loss: 4.1503  acc: 0.0000\n",
      "loss: 4.1344  acc: 0.0000\n",
      "loss: 2.9904  acc: 0.5000\n",
      "loss: 3.3497  acc: 0.0000\n",
      "loss: 3.3421  acc: 0.0000\n",
      "loss: 4.1264  acc: 0.0000\n",
      "loss: 4.0957  acc: 0.0000\n",
      "loss: 3.6264  acc: 0.5000\n",
      "loss: 3.8182  acc: 0.0000\n",
      "loss: 4.1571  acc: 0.0000\n",
      "loss: 3.6211  acc: 0.0000\n",
      "loss: 4.1254  acc: 0.0000\n",
      "loss: 3.1801  acc: 0.0000\n",
      "loss: 2.6930  acc: 0.5000\n",
      "loss: 4.4548  acc: 0.0000\n",
      "loss: 3.4540  acc: 0.0000\n",
      "loss: 4.6119  acc: 0.0000\n",
      "loss: 4.2312  acc: 0.0000\n",
      "loss: 3.7415  acc: 0.0000\n",
      "loss: 5.0206  acc: 0.0000\n",
      "loss: 2.6892  acc: 0.5000\n",
      "loss: 4.1243  acc: 0.0000\n",
      "loss: 3.3429  acc: 0.0000\n",
      "loss: 4.2287  acc: 0.0000\n",
      "loss: 3.5702  acc: 0.5000\n",
      "loss: 4.6983  acc: 0.0000\n",
      "loss: 4.4827  acc: 0.0000\n",
      "loss: 4.3975  acc: 0.0000\n",
      "loss: 4.3271  acc: 0.0000\n",
      "loss: 4.2471  acc: 0.0000\n",
      "loss: 3.0890  acc: 0.0000\n",
      "loss: 4.5456  acc: 0.0000\n",
      "loss: 4.4728  acc: 0.0000\n",
      "loss: 3.6778  acc: 0.5000\n",
      "loss: 3.9077  acc: 0.0000\n",
      "loss: 4.3696  acc: 0.0000\n",
      "loss: 4.2943  acc: 0.0000\n",
      "loss: 4.1129  acc: 0.0000\n",
      "loss: 5.3597  acc: 0.0000\n",
      "loss: 4.8641  acc: 0.0000\n",
      "loss: 4.2763  acc: 0.0000\n",
      "loss: 4.3356  acc: 0.0000\n",
      "epoch 9, train loss: 4.0042, train acc: 0.070\n",
      "test loss: 4.2773, test acc: 0.062\n",
      "loss: 3.7928  acc: 0.5000\n",
      "loss: 2.9975  acc: 0.5000\n",
      "loss: 2.1599  acc: 0.0000\n",
      "loss: 2.3695  acc: 0.5000\n",
      "loss: 3.9453  acc: 0.0000\n",
      "loss: 2.9658  acc: 0.0000\n",
      "loss: 3.6214  acc: 0.0000\n",
      "loss: 3.5915  acc: 0.0000\n",
      "loss: 3.3809  acc: 0.0000\n",
      "loss: 3.4865  acc: 0.0000\n",
      "loss: 4.1112  acc: 0.0000\n",
      "loss: 3.8898  acc: 0.0000\n",
      "loss: 4.3165  acc: 0.0000\n",
      "loss: 3.4254  acc: 0.5000\n",
      "loss: 4.7290  acc: 0.0000\n",
      "loss: 4.1941  acc: 0.0000\n",
      "loss: 4.3093  acc: 0.0000\n",
      "loss: 3.2545  acc: 0.0000\n",
      "loss: 4.0081  acc: 0.0000\n",
      "loss: 4.5733  acc: 0.0000\n",
      "loss: 4.1769  acc: 0.0000\n",
      "loss: 4.7400  acc: 0.0000\n",
      "loss: 4.1909  acc: 0.0000\n",
      "loss: 3.5335  acc: 0.0000\n",
      "loss: 3.6078  acc: 0.5000\n",
      "loss: 3.8871  acc: 0.0000\n",
      "loss: 3.7058  acc: 0.0000\n",
      "loss: 4.1473  acc: 0.0000\n",
      "loss: 3.7750  acc: 0.0000\n",
      "loss: 4.1422  acc: 0.0000\n",
      "loss: 4.0238  acc: 0.0000\n",
      "loss: 4.2797  acc: 0.0000\n",
      "loss: 4.4522  acc: 0.0000\n",
      "loss: 3.2685  acc: 0.0000\n",
      "loss: 4.4291  acc: 0.0000\n",
      "loss: 3.6314  acc: 0.5000\n",
      "loss: 4.5250  acc: 0.0000\n",
      "loss: 3.9520  acc: 0.0000\n",
      "loss: 4.3543  acc: 0.0000\n",
      "loss: 4.6501  acc: 0.0000\n",
      "loss: 5.0006  acc: 0.0000\n",
      "loss: 4.1734  acc: 0.0000\n",
      "loss: 3.4703  acc: 0.0000\n",
      "epoch 10, train loss: 3.8893, train acc: 0.070\n",
      "test loss: 4.5664, test acc: 0.075\n",
      "loss: 3.1931  acc: 0.5000\n",
      "loss: 3.3181  acc: 0.0000\n",
      "loss: 3.5255  acc: 0.0000\n",
      "loss: 2.0903  acc: 0.5000\n",
      "loss: 3.9694  acc: 0.0000\n",
      "loss: 3.9036  acc: 0.0000\n",
      "loss: 4.2468  acc: 0.0000\n",
      "loss: 3.3740  acc: 0.5000\n",
      "loss: 2.1298  acc: 0.5000\n",
      "loss: 3.5273  acc: 0.0000\n",
      "loss: 2.5588  acc: 0.0000\n",
      "loss: 3.3905  acc: 0.0000\n",
      "loss: 3.8490  acc: 0.0000\n",
      "loss: 3.3800  acc: 0.0000\n",
      "loss: 4.3045  acc: 0.0000\n",
      "loss: 3.0247  acc: 0.5000\n",
      "loss: 4.5106  acc: 0.0000\n",
      "loss: 4.0089  acc: 0.0000\n",
      "loss: 3.4918  acc: 0.0000\n",
      "loss: 3.8632  acc: 0.0000\n",
      "loss: 3.9199  acc: 0.0000\n",
      "loss: 3.5853  acc: 0.0000\n",
      "loss: 4.2286  acc: 0.0000\n",
      "loss: 4.4746  acc: 0.0000\n",
      "loss: 4.3961  acc: 0.0000\n",
      "loss: 3.2762  acc: 0.0000\n",
      "loss: 3.9373  acc: 0.0000\n",
      "loss: 4.2888  acc: 0.0000\n",
      "loss: 2.9638  acc: 0.0000\n",
      "loss: 4.1916  acc: 0.0000\n",
      "loss: 4.5491  acc: 0.0000\n",
      "loss: 4.0089  acc: 0.0000\n",
      "loss: 4.1598  acc: 0.0000\n",
      "loss: 4.3168  acc: 0.0000\n",
      "loss: 5.0122  acc: 0.0000\n",
      "loss: 3.6400  acc: 0.0000\n",
      "loss: 3.9044  acc: 0.0000\n",
      "loss: 4.1754  acc: 0.0000\n",
      "loss: 4.5786  acc: 0.0000\n",
      "loss: 4.5033  acc: 0.0000\n",
      "loss: 2.8330  acc: 0.0000\n",
      "loss: 4.6427  acc: 0.0000\n",
      "loss: 4.2650  acc: 0.0000\n",
      "epoch 11, train loss: 3.8026, train acc: 0.058\n",
      "test loss: 4.9439, test acc: 0.087\n",
      "loss: 2.1483  acc: 0.5000\n",
      "loss: 3.0568  acc: 0.5000\n",
      "loss: 4.0902  acc: 0.0000\n",
      "loss: 3.0924  acc: 0.0000\n",
      "loss: 2.7604  acc: 0.5000\n",
      "loss: 2.7715  acc: 0.0000\n",
      "loss: 2.1746  acc: 0.5000\n",
      "loss: 2.3626  acc: 0.5000\n",
      "loss: 3.7319  acc: 0.0000\n",
      "loss: 4.2647  acc: 0.0000\n",
      "loss: 2.5197  acc: 0.5000\n",
      "loss: 3.4839  acc: 0.0000\n",
      "loss: 3.3922  acc: 0.0000\n",
      "loss: 4.3938  acc: 0.0000\n",
      "loss: 2.6125  acc: 0.5000\n",
      "loss: 3.8543  acc: 0.0000\n",
      "loss: 3.4298  acc: 0.0000\n",
      "loss: 4.4103  acc: 0.0000\n",
      "loss: 3.3913  acc: 0.0000\n",
      "loss: 3.0077  acc: 0.5000\n",
      "loss: 4.7450  acc: 0.0000\n",
      "loss: 3.9848  acc: 0.0000\n",
      "loss: 3.3236  acc: 0.0000\n",
      "loss: 3.5631  acc: 0.0000\n",
      "loss: 4.4700  acc: 0.5000\n",
      "loss: 4.2453  acc: 0.0000\n",
      "loss: 4.4571  acc: 0.0000\n",
      "loss: 4.1361  acc: 0.0000\n",
      "loss: 4.2044  acc: 0.0000\n",
      "loss: 3.8946  acc: 0.0000\n",
      "loss: 4.1217  acc: 0.0000\n",
      "loss: 3.9304  acc: 0.0000\n",
      "loss: 1.5605  acc: 1.0000\n",
      "loss: 4.2021  acc: 0.0000\n",
      "loss: 2.5559  acc: 0.5000\n",
      "loss: 3.5475  acc: 0.0000\n",
      "loss: 3.6804  acc: 0.0000\n",
      "loss: 4.0621  acc: 0.0000\n",
      "loss: 4.7375  acc: 0.0000\n",
      "loss: 4.2657  acc: 0.0000\n",
      "loss: 4.7434  acc: 0.0000\n",
      "loss: 5.0857  acc: 0.0000\n",
      "loss: 3.9178  acc: 0.0000\n",
      "epoch 12, train loss: 3.6368, train acc: 0.140\n",
      "test loss: 5.2568, test acc: 0.100\n",
      "loss: 5.0435  acc: 0.0000\n",
      "loss: 2.7148  acc: 0.5000\n",
      "loss: 3.8825  acc: 0.0000\n",
      "loss: 2.0941  acc: 0.5000\n",
      "loss: 2.0101  acc: 1.0000\n",
      "loss: 3.9619  acc: 0.0000\n",
      "loss: 3.7685  acc: 0.0000\n",
      "loss: 3.0761  acc: 0.0000\n",
      "loss: 3.0115  acc: 0.0000\n",
      "loss: 4.2751  acc: 0.0000\n",
      "loss: 2.9871  acc: 0.5000\n",
      "loss: 3.1282  acc: 0.0000\n",
      "loss: 3.9326  acc: 0.0000\n",
      "loss: 3.2462  acc: 0.0000\n",
      "loss: 3.2371  acc: 0.0000\n",
      "loss: 3.5245  acc: 0.0000\n",
      "loss: 3.8706  acc: 0.0000\n",
      "loss: 3.3817  acc: 0.0000\n",
      "loss: 3.3702  acc: 0.0000\n",
      "loss: 3.3653  acc: 0.0000\n",
      "loss: 4.0398  acc: 0.0000\n",
      "loss: 4.0289  acc: 0.0000\n",
      "loss: 2.7953  acc: 0.5000\n",
      "loss: 3.9207  acc: 0.0000\n",
      "loss: 3.0594  acc: 0.0000\n",
      "loss: 2.9156  acc: 0.0000\n",
      "loss: 3.2408  acc: 0.0000\n",
      "loss: 3.4433  acc: 0.5000\n",
      "loss: 4.6985  acc: 0.0000\n",
      "loss: 3.8469  acc: 0.0000\n",
      "loss: 3.5113  acc: 0.0000\n",
      "loss: 3.5720  acc: 0.0000\n",
      "loss: 2.0831  acc: 0.5000\n",
      "loss: 3.3056  acc: 0.0000\n",
      "loss: 4.3486  acc: 0.0000\n",
      "loss: 4.0145  acc: 0.0000\n",
      "loss: 2.2711  acc: 0.5000\n",
      "loss: 3.3902  acc: 0.0000\n",
      "loss: 3.6546  acc: 0.0000\n",
      "loss: 1.5518  acc: 0.5000\n",
      "loss: 3.8849  acc: 0.0000\n",
      "loss: 3.5808  acc: 0.0000\n",
      "loss: 3.7841  acc: 0.0000\n",
      "epoch 13, train loss: 3.4145, train acc: 0.116\n",
      "test loss: 4.3842, test acc: 0.138\n",
      "loss: 2.5410  acc: 0.0000\n",
      "loss: 4.3662  acc: 0.0000\n",
      "loss: 3.4699  acc: 0.0000\n",
      "loss: 3.6030  acc: 0.0000\n",
      "loss: 4.1340  acc: 0.0000\n",
      "loss: 1.8236  acc: 0.5000\n",
      "loss: 2.4346  acc: 0.5000\n",
      "loss: 2.2027  acc: 0.0000\n",
      "loss: 3.8846  acc: 0.0000\n",
      "loss: 3.1552  acc: 0.5000\n",
      "loss: 0.8741  acc: 1.0000\n",
      "loss: 2.4606  acc: 0.5000\n",
      "loss: 2.2332  acc: 0.5000\n",
      "loss: 5.0920  acc: 0.0000\n",
      "loss: 3.9410  acc: 0.0000\n",
      "loss: 4.4332  acc: 0.0000\n",
      "loss: 2.7853  acc: 0.0000\n",
      "loss: 4.4420  acc: 0.0000\n",
      "loss: 2.7506  acc: 0.5000\n",
      "loss: 3.2207  acc: 0.5000\n",
      "loss: 2.0487  acc: 0.5000\n",
      "loss: 3.9298  acc: 0.0000\n",
      "loss: 3.4666  acc: 0.0000\n",
      "loss: 4.0308  acc: 0.0000\n",
      "loss: 2.9526  acc: 0.5000\n",
      "loss: 4.0675  acc: 0.0000\n",
      "loss: 4.0236  acc: 0.0000\n",
      "loss: 3.1284  acc: 0.0000\n",
      "loss: 3.7369  acc: 0.0000\n",
      "loss: 3.0738  acc: 0.0000\n",
      "loss: 1.3086  acc: 1.0000\n",
      "loss: 4.5079  acc: 0.0000\n",
      "loss: 3.2688  acc: 0.0000\n",
      "loss: 4.2027  acc: 0.0000\n",
      "loss: 4.3888  acc: 0.0000\n",
      "loss: 3.8126  acc: 0.0000\n",
      "loss: 4.5940  acc: 0.0000\n",
      "loss: 2.9738  acc: 0.5000\n",
      "loss: 3.0423  acc: 0.0000\n",
      "loss: 4.2942  acc: 0.0000\n",
      "loss: 3.8791  acc: 0.0000\n",
      "loss: 3.6797  acc: 0.0000\n",
      "loss: 2.5794  acc: 0.5000\n",
      "epoch 14, train loss: 3.3683, train acc: 0.174\n",
      "test loss: 4.6279, test acc: 0.175\n",
      "loss: 2.2231  acc: 0.5000\n",
      "loss: 3.2988  acc: 0.0000\n",
      "loss: 3.2781  acc: 0.0000\n",
      "loss: 2.9948  acc: 0.0000\n",
      "loss: 2.7799  acc: 0.0000\n",
      "loss: 2.6319  acc: 0.5000\n",
      "loss: 2.4783  acc: 0.5000\n",
      "loss: 3.0393  acc: 0.0000\n",
      "loss: 3.4105  acc: 0.0000\n",
      "loss: 3.1783  acc: 0.0000\n",
      "loss: 2.2891  acc: 0.5000\n",
      "loss: 3.8618  acc: 0.0000\n",
      "loss: 3.4435  acc: 0.0000\n",
      "loss: 3.2957  acc: 0.0000\n",
      "loss: 3.8668  acc: 0.0000\n",
      "loss: 2.8864  acc: 0.0000\n",
      "loss: 3.6248  acc: 0.0000\n",
      "loss: 2.9353  acc: 0.5000\n",
      "loss: 2.3767  acc: 0.5000\n",
      "loss: 3.6226  acc: 0.0000\n",
      "loss: 2.5097  acc: 0.5000\n",
      "loss: 1.9867  acc: 0.0000\n",
      "loss: 4.1153  acc: 0.0000\n",
      "loss: 3.4948  acc: 0.0000\n",
      "loss: 3.4950  acc: 0.0000\n",
      "loss: 3.8625  acc: 0.0000\n",
      "loss: 2.4350  acc: 0.0000\n",
      "loss: 2.9642  acc: 0.0000\n",
      "loss: 4.0134  acc: 0.0000\n",
      "loss: 2.9235  acc: 0.5000\n",
      "loss: 3.3858  acc: 0.0000\n",
      "loss: 3.7861  acc: 0.5000\n",
      "loss: 3.2903  acc: 0.5000\n",
      "loss: 4.5257  acc: 0.0000\n",
      "loss: 2.8062  acc: 0.5000\n",
      "loss: 3.0731  acc: 0.0000\n",
      "loss: 3.9537  acc: 0.0000\n",
      "loss: 3.0590  acc: 0.0000\n",
      "loss: 3.1844  acc: 0.5000\n",
      "loss: 3.5497  acc: 0.0000\n",
      "loss: 3.7268  acc: 0.0000\n",
      "loss: 3.7098  acc: 0.0000\n",
      "loss: 2.8897  acc: 0.5000\n",
      "epoch 15, train loss: 3.2153, train acc: 0.151\n",
      "test loss: 4.4300, test acc: 0.113\n",
      "loss: 1.9201  acc: 0.5000\n",
      "loss: 2.5428  acc: 0.5000\n",
      "loss: 3.4662  acc: 0.0000\n",
      "loss: 1.4702  acc: 0.5000\n",
      "loss: 2.3096  acc: 0.5000\n",
      "loss: 2.8981  acc: 0.0000\n",
      "loss: 3.5483  acc: 0.0000\n",
      "loss: 1.6465  acc: 0.5000\n",
      "loss: 2.6982  acc: 0.5000\n",
      "loss: 3.0132  acc: 0.0000\n",
      "loss: 1.8057  acc: 0.5000\n",
      "loss: 2.6898  acc: 0.5000\n",
      "loss: 2.8898  acc: 0.0000\n",
      "loss: 4.1598  acc: 0.0000\n",
      "loss: 1.6092  acc: 0.5000\n",
      "loss: 3.1306  acc: 0.0000\n",
      "loss: 3.4263  acc: 0.0000\n",
      "loss: 2.3105  acc: 0.5000\n",
      "loss: 3.7917  acc: 0.0000\n",
      "loss: 3.2707  acc: 0.0000\n",
      "loss: 2.9442  acc: 0.0000\n",
      "loss: 2.9465  acc: 0.5000\n",
      "loss: 4.7036  acc: 0.0000\n",
      "loss: 1.0747  acc: 1.0000\n",
      "loss: 4.7405  acc: 0.0000\n",
      "loss: 3.4931  acc: 0.0000\n",
      "loss: 2.2310  acc: 0.5000\n",
      "loss: 4.3892  acc: 0.0000\n",
      "loss: 4.0033  acc: 0.0000\n",
      "loss: 2.5301  acc: 0.5000\n",
      "loss: 3.5122  acc: 0.0000\n",
      "loss: 2.4925  acc: 0.5000\n",
      "loss: 2.3961  acc: 0.5000\n",
      "loss: 3.0326  acc: 0.0000\n",
      "loss: 4.2612  acc: 0.0000\n",
      "loss: 3.2497  acc: 0.0000\n",
      "loss: 3.4409  acc: 0.0000\n",
      "loss: 1.9167  acc: 0.5000\n",
      "loss: 3.8365  acc: 0.0000\n",
      "loss: 4.7386  acc: 0.0000\n",
      "loss: 3.4791  acc: 0.0000\n",
      "loss: 3.3900  acc: 0.0000\n",
      "loss: 3.5542  acc: 0.0000\n",
      "epoch 16, train loss: 3.0454, train acc: 0.209\n",
      "test loss: 6.3199, test acc: 0.138\n",
      "loss: 2.6131  acc: 0.5000\n",
      "loss: 2.5897  acc: 0.5000\n",
      "loss: 2.4240  acc: 0.5000\n",
      "loss: 3.0691  acc: 0.0000\n",
      "loss: 2.0222  acc: 0.0000\n",
      "loss: 3.0904  acc: 0.5000\n",
      "loss: 2.0677  acc: 0.5000\n",
      "loss: 2.6308  acc: 0.0000\n",
      "loss: 3.5364  acc: 0.0000\n",
      "loss: 3.5259  acc: 0.0000\n",
      "loss: 3.0124  acc: 0.0000\n",
      "loss: 2.8555  acc: 0.0000\n",
      "loss: 1.9928  acc: 0.5000\n",
      "loss: 3.0950  acc: 0.5000\n",
      "loss: 1.8718  acc: 0.5000\n",
      "loss: 2.9523  acc: 0.0000\n",
      "loss: 2.6022  acc: 0.0000\n",
      "loss: 2.3728  acc: 0.0000\n",
      "loss: 2.6970  acc: 0.0000\n",
      "loss: 3.1649  acc: 0.5000\n",
      "loss: 3.3630  acc: 0.0000\n",
      "loss: 3.5278  acc: 0.0000\n",
      "loss: 3.9529  acc: 0.0000\n",
      "loss: 2.8857  acc: 0.0000\n",
      "loss: 2.9257  acc: 0.5000\n",
      "loss: 2.5768  acc: 0.0000\n",
      "loss: 2.9866  acc: 0.0000\n",
      "loss: 1.9300  acc: 0.5000\n",
      "loss: 3.2227  acc: 0.0000\n",
      "loss: 0.8300  acc: 1.0000\n",
      "loss: 1.9382  acc: 0.5000\n",
      "loss: 2.2912  acc: 0.5000\n",
      "loss: 3.7245  acc: 0.0000\n",
      "loss: 2.9968  acc: 0.0000\n",
      "loss: 4.1917  acc: 0.0000\n",
      "loss: 4.1517  acc: 0.0000\n",
      "loss: 2.8769  acc: 0.5000\n",
      "loss: 2.8398  acc: 0.5000\n",
      "loss: 3.8406  acc: 0.0000\n",
      "loss: 1.9469  acc: 0.5000\n",
      "loss: 2.5626  acc: 0.5000\n",
      "loss: 3.5809  acc: 0.0000\n",
      "loss: 3.1122  acc: 0.5000\n",
      "epoch 17, train loss: 2.8475, train acc: 0.233\n",
      "test loss: 4.6761, test acc: 0.138\n",
      "loss: 1.9417  acc: 0.5000\n",
      "loss: 1.4633  acc: 0.5000\n",
      "loss: 2.3391  acc: 0.5000\n",
      "loss: 3.3977  acc: 0.0000\n",
      "loss: 1.4178  acc: 0.5000\n",
      "loss: 2.3431  acc: 0.5000\n",
      "loss: 2.3825  acc: 0.5000\n",
      "loss: 3.0109  acc: 0.0000\n",
      "loss: 3.2422  acc: 0.0000\n",
      "loss: 1.5504  acc: 0.5000\n",
      "loss: 2.5522  acc: 0.0000\n",
      "loss: 2.9423  acc: 0.0000\n",
      "loss: 2.2459  acc: 0.5000\n",
      "loss: 3.2211  acc: 0.5000\n",
      "loss: 2.5662  acc: 0.0000\n",
      "loss: 2.5511  acc: 0.5000\n",
      "loss: 1.8329  acc: 0.5000\n",
      "loss: 3.1239  acc: 0.5000\n",
      "loss: 3.0707  acc: 0.0000\n",
      "loss: 0.6473  acc: 1.0000\n",
      "loss: 2.6527  acc: 0.0000\n",
      "loss: 1.7206  acc: 0.5000\n",
      "loss: 0.5607  acc: 1.0000\n",
      "loss: 3.1347  acc: 0.0000\n",
      "loss: 1.6852  acc: 0.5000\n",
      "loss: 2.3234  acc: 0.0000\n",
      "loss: 3.0741  acc: 0.5000\n",
      "loss: 1.8553  acc: 1.0000\n",
      "loss: 3.9131  acc: 0.0000\n",
      "loss: 3.6928  acc: 0.0000\n",
      "loss: 3.3822  acc: 0.0000\n",
      "loss: 3.1795  acc: 0.0000\n",
      "loss: 2.6436  acc: 0.0000\n",
      "loss: 3.7066  acc: 0.0000\n",
      "loss: 2.0890  acc: 0.5000\n",
      "loss: 1.9374  acc: 0.5000\n",
      "loss: 3.5119  acc: 0.0000\n",
      "loss: 2.3461  acc: 0.0000\n",
      "loss: 2.6910  acc: 0.0000\n",
      "loss: 3.7905  acc: 0.0000\n",
      "loss: 2.9685  acc: 0.0000\n",
      "loss: 2.4073  acc: 0.0000\n",
      "loss: 3.2452  acc: 0.0000\n",
      "epoch 18, train loss: 2.5664, train acc: 0.267\n",
      "test loss: 5.4593, test acc: 0.163\n",
      "loss: 1.4370  acc: 1.0000\n",
      "loss: 0.6191  acc: 1.0000\n",
      "loss: 0.8762  acc: 1.0000\n",
      "loss: 2.6191  acc: 0.0000\n",
      "loss: 2.1450  acc: 0.5000\n",
      "loss: 3.0415  acc: 0.0000\n",
      "loss: 2.8243  acc: 0.0000\n",
      "loss: 2.4236  acc: 0.0000\n",
      "loss: 2.7140  acc: 0.5000\n",
      "loss: 2.6195  acc: 0.0000\n",
      "loss: 2.9958  acc: 0.5000\n",
      "loss: 2.3822  acc: 0.5000\n",
      "loss: 0.9363  acc: 0.5000\n",
      "loss: 2.1410  acc: 0.5000\n",
      "loss: 2.3081  acc: 0.5000\n",
      "loss: 2.3678  acc: 0.5000\n",
      "loss: 3.2882  acc: 0.0000\n",
      "loss: 0.8289  acc: 1.0000\n",
      "loss: 3.3745  acc: 0.0000\n",
      "loss: 3.0435  acc: 0.0000\n",
      "loss: 2.1301  acc: 0.5000\n",
      "loss: 3.3937  acc: 0.0000\n",
      "loss: 3.2970  acc: 0.0000\n",
      "loss: 2.6065  acc: 0.0000\n",
      "loss: 2.7107  acc: 0.0000\n",
      "loss: 1.6788  acc: 0.5000\n",
      "loss: 2.0735  acc: 0.5000\n",
      "loss: 3.1921  acc: 0.0000\n",
      "loss: 3.0403  acc: 0.0000\n",
      "loss: 2.2541  acc: 0.5000\n",
      "loss: 1.4428  acc: 0.5000\n",
      "loss: 2.4350  acc: 0.5000\n",
      "loss: 3.1600  acc: 0.0000\n",
      "loss: 1.8861  acc: 1.0000\n",
      "loss: 2.1175  acc: 0.5000\n",
      "loss: 2.3100  acc: 0.0000\n",
      "loss: 2.3796  acc: 0.5000\n",
      "loss: 3.4588  acc: 0.5000\n",
      "loss: 3.1546  acc: 0.0000\n",
      "loss: 1.9242  acc: 0.5000\n",
      "loss: 3.4323  acc: 0.0000\n",
      "loss: 4.3810  acc: 0.0000\n",
      "loss: 2.7676  acc: 0.5000\n",
      "epoch 19, train loss: 2.4700, train acc: 0.337\n",
      "test loss: 4.1412, test acc: 0.212\n",
      "loss: 1.6400  acc: 1.0000\n",
      "loss: 1.7292  acc: 0.5000\n",
      "loss: 1.1537  acc: 1.0000\n",
      "loss: 2.5226  acc: 0.5000\n",
      "loss: 2.4070  acc: 0.5000\n",
      "loss: 2.2261  acc: 0.0000\n",
      "loss: 1.9889  acc: 0.5000\n",
      "loss: 2.0856  acc: 0.5000\n",
      "loss: 0.9912  acc: 1.0000\n",
      "loss: 0.8783  acc: 1.0000\n",
      "loss: 2.6785  acc: 0.0000\n",
      "loss: 1.0695  acc: 0.5000\n",
      "loss: 1.2094  acc: 0.5000\n",
      "loss: 1.4799  acc: 0.5000\n",
      "loss: 2.1816  acc: 0.5000\n",
      "loss: 1.0012  acc: 1.0000\n",
      "loss: 2.2907  acc: 0.0000\n",
      "loss: 2.3976  acc: 0.0000\n",
      "loss: 1.4873  acc: 0.5000\n",
      "loss: 0.7036  acc: 1.0000\n",
      "loss: 1.4604  acc: 0.5000\n",
      "loss: 1.9373  acc: 0.5000\n",
      "loss: 2.1563  acc: 0.5000\n",
      "loss: 2.2517  acc: 0.0000\n",
      "loss: 3.3988  acc: 0.0000\n",
      "loss: 2.5042  acc: 0.5000\n",
      "loss: 3.4881  acc: 0.0000\n",
      "loss: 3.1032  acc: 0.0000\n",
      "loss: 0.9491  acc: 0.5000\n",
      "loss: 2.3223  acc: 0.0000\n",
      "loss: 3.3067  acc: 0.0000\n",
      "loss: 2.8732  acc: 0.0000\n",
      "loss: 1.6093  acc: 1.0000\n",
      "loss: 2.0456  acc: 0.5000\n",
      "loss: 2.8822  acc: 0.0000\n",
      "loss: 2.6018  acc: 0.5000\n",
      "loss: 1.9871  acc: 0.5000\n",
      "loss: 1.9474  acc: 0.5000\n",
      "loss: 2.2815  acc: 0.5000\n",
      "loss: 2.6535  acc: 0.0000\n",
      "loss: 5.1512  acc: 0.0000\n",
      "loss: 2.6146  acc: 0.0000\n",
      "loss: 2.4297  acc: 0.5000\n",
      "epoch 20, train loss: 2.1413, train acc: 0.407\n",
      "test loss: 4.1589, test acc: 0.225\n",
      "loss: 1.7908  acc: 0.5000\n",
      "loss: 1.4981  acc: 0.5000\n",
      "loss: 1.4940  acc: 0.5000\n",
      "loss: 0.6748  acc: 1.0000\n",
      "loss: 1.3831  acc: 0.5000\n",
      "loss: 1.7027  acc: 0.5000\n",
      "loss: 2.3391  acc: 0.5000\n",
      "loss: 2.4221  acc: 0.0000\n",
      "loss: 1.3183  acc: 0.5000\n",
      "loss: 1.1742  acc: 1.0000\n",
      "loss: 1.3974  acc: 0.5000\n",
      "loss: 1.5477  acc: 0.5000\n",
      "loss: 2.4126  acc: 0.5000\n",
      "loss: 3.0259  acc: 0.0000\n",
      "loss: 3.0925  acc: 0.5000\n",
      "loss: 2.6035  acc: 0.5000\n",
      "loss: 2.2096  acc: 0.0000\n",
      "loss: 2.3643  acc: 0.5000\n",
      "loss: 1.8538  acc: 0.0000\n",
      "loss: 1.6672  acc: 0.5000\n",
      "loss: 2.6691  acc: 0.0000\n",
      "loss: 1.5307  acc: 0.5000\n",
      "loss: 1.2102  acc: 1.0000\n",
      "loss: 0.2807  acc: 1.0000\n",
      "loss: 2.5352  acc: 0.0000\n",
      "loss: 1.6061  acc: 0.5000\n",
      "loss: 1.4042  acc: 0.5000\n",
      "loss: 2.3940  acc: 0.5000\n",
      "loss: 1.1290  acc: 1.0000\n",
      "loss: 1.8206  acc: 0.5000\n",
      "loss: 2.8796  acc: 0.0000\n",
      "loss: 2.4995  acc: 0.5000\n",
      "loss: 3.8896  acc: 0.0000\n",
      "loss: 5.5124  acc: 0.5000\n",
      "loss: 2.6456  acc: 0.0000\n",
      "loss: 1.9292  acc: 0.0000\n",
      "loss: 1.8048  acc: 0.5000\n",
      "loss: 1.8849  acc: 0.5000\n",
      "loss: 2.2381  acc: 0.5000\n",
      "loss: 2.5224  acc: 0.5000\n",
      "loss: 2.2867  acc: 0.5000\n",
      "loss: 2.5932  acc: 0.0000\n",
      "loss: 2.2847  acc: 0.5000\n",
      "epoch 21, train loss: 2.0819, train acc: 0.430\n",
      "test loss: 4.7897, test acc: 0.225\n",
      "loss: 1.7122  acc: 1.0000\n",
      "loss: 2.1148  acc: 0.5000\n",
      "loss: 1.4819  acc: 1.0000\n",
      "loss: 1.2266  acc: 0.5000\n",
      "loss: 0.9163  acc: 1.0000\n",
      "loss: 0.9569  acc: 0.5000\n",
      "loss: 2.0080  acc: 0.5000\n",
      "loss: 0.6660  acc: 1.0000\n",
      "loss: 1.5120  acc: 0.5000\n",
      "loss: 1.5505  acc: 0.5000\n",
      "loss: 0.7299  acc: 1.0000\n",
      "loss: 1.8941  acc: 0.5000\n",
      "loss: 0.8351  acc: 1.0000\n",
      "loss: 2.2546  acc: 0.5000\n",
      "loss: 1.4775  acc: 0.5000\n",
      "loss: 2.0174  acc: 0.0000\n",
      "loss: 2.9052  acc: 0.0000\n",
      "loss: 1.6040  acc: 0.5000\n",
      "loss: 1.3832  acc: 0.5000\n",
      "loss: 1.5606  acc: 0.5000\n",
      "loss: 1.7028  acc: 0.5000\n",
      "loss: 2.0301  acc: 0.0000\n",
      "loss: 1.9184  acc: 0.5000\n",
      "loss: 2.8957  acc: 0.0000\n",
      "loss: 1.2634  acc: 0.5000\n",
      "loss: 2.1094  acc: 0.5000\n",
      "loss: 1.3970  acc: 1.0000\n",
      "loss: 2.4784  acc: 0.5000\n",
      "loss: 2.5860  acc: 0.5000\n",
      "loss: 2.2627  acc: 0.5000\n",
      "loss: 1.6749  acc: 0.5000\n",
      "loss: 1.3721  acc: 1.0000\n",
      "loss: 0.4092  acc: 1.0000\n",
      "loss: 1.3618  acc: 1.0000\n",
      "loss: 2.3606  acc: 0.5000\n",
      "loss: 1.3700  acc: 0.5000\n",
      "loss: 1.6125  acc: 1.0000\n",
      "loss: 1.1605  acc: 0.5000\n",
      "loss: 2.9667  acc: 0.0000\n",
      "loss: 2.7344  acc: 0.5000\n",
      "loss: 2.1225  acc: 0.5000\n",
      "loss: 2.9527  acc: 0.0000\n",
      "loss: 0.6814  acc: 1.0000\n",
      "epoch 22, train loss: 1.7263, train acc: 0.570\n",
      "test loss: 4.8591, test acc: 0.263\n",
      "loss: 1.6836  acc: 0.5000\n",
      "loss: 1.0024  acc: 0.5000\n",
      "loss: 1.1870  acc: 0.5000\n",
      "loss: 1.6548  acc: 0.0000\n",
      "loss: 0.6566  acc: 1.0000\n",
      "loss: 0.9202  acc: 0.5000\n",
      "loss: 0.6021  acc: 1.0000\n",
      "loss: 0.8954  acc: 1.0000\n",
      "loss: 0.6149  acc: 1.0000\n",
      "loss: 0.6039  acc: 1.0000\n",
      "loss: 2.3299  acc: 0.5000\n",
      "loss: 3.0828  acc: 0.0000\n",
      "loss: 0.7712  acc: 0.5000\n",
      "loss: 1.8847  acc: 0.5000\n",
      "loss: 2.6132  acc: 0.0000\n",
      "loss: 1.6332  acc: 0.5000\n",
      "loss: 0.5423  acc: 1.0000\n",
      "loss: 0.6054  acc: 1.0000\n",
      "loss: 0.6072  acc: 1.0000\n",
      "loss: 2.3521  acc: 0.0000\n",
      "loss: 1.6603  acc: 0.5000\n",
      "loss: 1.9678  acc: 0.5000\n",
      "loss: 1.4630  acc: 0.5000\n",
      "loss: 1.6576  acc: 0.5000\n",
      "loss: 2.0526  acc: 0.0000\n",
      "loss: 1.8270  acc: 0.5000\n",
      "loss: 0.5651  acc: 1.0000\n",
      "loss: 1.8407  acc: 0.0000\n",
      "loss: 1.2938  acc: 0.5000\n",
      "loss: 1.8071  acc: 0.5000\n",
      "loss: 3.3696  acc: 0.0000\n",
      "loss: 0.9282  acc: 1.0000\n",
      "loss: 1.6482  acc: 0.5000\n",
      "loss: 1.6686  acc: 0.0000\n",
      "loss: 2.9672  acc: 0.0000\n",
      "loss: 1.6306  acc: 0.5000\n",
      "loss: 2.1999  acc: 0.5000\n",
      "loss: 1.3963  acc: 0.5000\n",
      "loss: 1.6747  acc: 0.5000\n",
      "loss: 3.5271  acc: 0.0000\n",
      "loss: 1.1283  acc: 1.0000\n",
      "loss: 1.0554  acc: 1.0000\n",
      "loss: 1.9722  acc: 0.5000\n",
      "epoch 23, train loss: 1.5708, train acc: 0.523\n",
      "test loss: 5.3710, test acc: 0.225\n",
      "loss: 1.0083  acc: 0.5000\n",
      "loss: 2.4159  acc: 0.5000\n",
      "loss: 1.7167  acc: 0.5000\n",
      "loss: 1.0365  acc: 1.0000\n",
      "loss: 2.4810  acc: 0.5000\n",
      "loss: 2.2934  acc: 0.0000\n",
      "loss: 1.8357  acc: 0.5000\n",
      "loss: 0.9240  acc: 1.0000\n",
      "loss: 0.8538  acc: 1.0000\n",
      "loss: 0.8051  acc: 1.0000\n",
      "loss: 7.2110  acc: 0.5000\n",
      "loss: 1.7467  acc: 0.5000\n",
      "loss: 0.6497  acc: 1.0000\n",
      "loss: 1.2472  acc: 1.0000\n",
      "loss: 2.8592  acc: 0.5000\n",
      "loss: 1.5407  acc: 0.5000\n",
      "loss: 1.9330  acc: 0.5000\n",
      "loss: 0.5365  acc: 1.0000\n",
      "loss: 2.1845  acc: 0.5000\n",
      "loss: 1.6392  acc: 0.5000\n",
      "loss: 0.8715  acc: 1.0000\n",
      "loss: 2.0312  acc: 0.5000\n",
      "loss: 1.5669  acc: 0.0000\n",
      "loss: 2.1878  acc: 0.0000\n",
      "loss: 2.3458  acc: 0.0000\n",
      "loss: 1.8113  acc: 0.5000\n",
      "loss: 2.4558  acc: 0.0000\n",
      "loss: 1.7023  acc: 0.5000\n",
      "loss: 2.9142  acc: 0.0000\n",
      "loss: 1.3602  acc: 0.5000\n",
      "loss: 1.5269  acc: 0.5000\n",
      "loss: 1.2382  acc: 0.5000\n",
      "loss: 2.8375  acc: 0.5000\n",
      "loss: 1.7940  acc: 0.5000\n",
      "loss: 1.6387  acc: 0.5000\n",
      "loss: 2.0150  acc: 0.0000\n",
      "loss: 1.1198  acc: 0.5000\n",
      "loss: 2.2764  acc: 0.5000\n",
      "loss: 1.1867  acc: 0.5000\n",
      "loss: 1.0091  acc: 1.0000\n",
      "loss: 1.5128  acc: 0.5000\n",
      "loss: 1.0052  acc: 1.0000\n",
      "loss: 1.0155  acc: 1.0000\n",
      "epoch 24, train loss: 1.7754, train acc: 0.547\n",
      "test loss: 5.4150, test acc: 0.225\n",
      "loss: 2.2595  acc: 0.5000\n",
      "loss: 0.6629  acc: 1.0000\n",
      "loss: 1.7591  acc: 0.5000\n",
      "loss: 1.5238  acc: 0.5000\n",
      "loss: 1.9915  acc: 0.5000\n",
      "loss: 1.9282  acc: 0.5000\n",
      "loss: 2.7964  acc: 0.0000\n",
      "loss: 1.4455  acc: 0.5000\n",
      "loss: 1.6708  acc: 0.5000\n",
      "loss: 1.1789  acc: 0.5000\n",
      "loss: 1.3915  acc: 0.5000\n",
      "loss: 0.8699  acc: 0.5000\n",
      "loss: 2.5725  acc: 0.5000\n",
      "loss: 1.2182  acc: 0.5000\n",
      "loss: 0.6889  acc: 1.0000\n",
      "loss: 1.7754  acc: 0.0000\n",
      "loss: 0.8658  acc: 0.5000\n",
      "loss: 0.8122  acc: 1.0000\n",
      "loss: 0.7449  acc: 1.0000\n",
      "loss: 0.2283  acc: 1.0000\n",
      "loss: 1.4702  acc: 0.5000\n",
      "loss: 1.6534  acc: 0.5000\n",
      "loss: 1.0395  acc: 1.0000\n",
      "loss: 1.1952  acc: 1.0000\n",
      "loss: 1.0692  acc: 1.0000\n",
      "loss: 2.3036  acc: 0.5000\n",
      "loss: 1.9822  acc: 0.5000\n",
      "loss: 1.3379  acc: 0.5000\n",
      "loss: 1.2503  acc: 0.5000\n",
      "loss: 1.0684  acc: 0.5000\n",
      "loss: 1.7690  acc: 0.5000\n",
      "loss: 1.6892  acc: 0.5000\n",
      "loss: 1.6030  acc: 0.5000\n",
      "loss: 4.5885  acc: 0.5000\n",
      "loss: 1.1626  acc: 0.5000\n",
      "loss: 0.7117  acc: 1.0000\n",
      "loss: 0.8609  acc: 1.0000\n",
      "loss: 0.9946  acc: 1.0000\n",
      "loss: 1.0228  acc: 1.0000\n",
      "loss: 2.7611  acc: 0.5000\n",
      "loss: 1.3306  acc: 1.0000\n",
      "loss: 0.7614  acc: 0.5000\n",
      "loss: 1.1415  acc: 0.5000\n",
      "epoch 25, train loss: 1.4686, train acc: 0.628\n",
      "test loss: 4.9800, test acc: 0.138\n",
      "loss: 0.7345  acc: 1.0000\n",
      "loss: 1.7575  acc: 0.5000\n",
      "loss: 0.3117  acc: 1.0000\n",
      "loss: 1.1437  acc: 0.5000\n",
      "loss: 1.7477  acc: 0.5000\n",
      "loss: 1.6218  acc: 0.5000\n",
      "loss: 1.0067  acc: 1.0000\n",
      "loss: 1.8146  acc: 0.5000\n",
      "loss: 2.1604  acc: 0.5000\n",
      "loss: 0.6405  acc: 1.0000\n",
      "loss: 0.9516  acc: 1.0000\n",
      "loss: 0.7358  acc: 1.0000\n",
      "loss: 0.8815  acc: 1.0000\n",
      "loss: 0.5739  acc: 1.0000\n",
      "loss: 0.2917  acc: 1.0000\n",
      "loss: 2.1564  acc: 0.5000\n",
      "loss: 1.2937  acc: 0.0000\n",
      "loss: 1.4937  acc: 0.5000\n",
      "loss: 1.0647  acc: 1.0000\n",
      "loss: 0.8151  acc: 1.0000\n",
      "loss: 1.7277  acc: 0.5000\n",
      "loss: 0.6228  acc: 1.0000\n",
      "loss: 1.2690  acc: 0.5000\n",
      "loss: 0.4746  acc: 1.0000\n",
      "loss: 0.8322  acc: 1.0000\n",
      "loss: 0.3576  acc: 1.0000\n",
      "loss: 0.9347  acc: 1.0000\n",
      "loss: 1.4696  acc: 0.5000\n",
      "loss: 1.1749  acc: 0.5000\n",
      "loss: 2.2456  acc: 0.5000\n",
      "loss: 0.6253  acc: 0.5000\n",
      "loss: 0.5364  acc: 1.0000\n",
      "loss: 1.7482  acc: 0.5000\n",
      "loss: 1.0216  acc: 1.0000\n",
      "loss: 2.0341  acc: 0.5000\n",
      "loss: 1.1902  acc: 1.0000\n",
      "loss: 1.7008  acc: 0.5000\n",
      "loss: 1.4372  acc: 1.0000\n",
      "loss: 1.1851  acc: 0.5000\n",
      "loss: 1.0467  acc: 1.0000\n",
      "loss: 0.8587  acc: 1.0000\n",
      "loss: 1.1626  acc: 1.0000\n",
      "loss: 2.0333  acc: 0.5000\n",
      "epoch 26, train loss: 1.1834, train acc: 0.756\n",
      "test loss: 4.3378, test acc: 0.275\n",
      "loss: 0.2455  acc: 1.0000\n",
      "loss: 0.2184  acc: 1.0000\n",
      "loss: 0.4891  acc: 1.0000\n",
      "loss: 0.7446  acc: 1.0000\n",
      "loss: 0.6095  acc: 1.0000\n",
      "loss: 0.0696  acc: 1.0000\n",
      "loss: 0.1826  acc: 1.0000\n",
      "loss: 0.6331  acc: 1.0000\n",
      "loss: 0.5710  acc: 1.0000\n",
      "loss: 0.3794  acc: 1.0000\n",
      "loss: 0.5701  acc: 1.0000\n",
      "loss: 1.8271  acc: 0.5000\n",
      "loss: 0.4658  acc: 1.0000\n",
      "loss: 1.9898  acc: 0.5000\n",
      "loss: 0.4511  acc: 1.0000\n",
      "loss: 1.1011  acc: 1.0000\n",
      "loss: 0.2297  acc: 1.0000\n",
      "loss: 1.6070  acc: 0.5000\n",
      "loss: 1.6654  acc: 0.5000\n",
      "loss: 0.4554  acc: 1.0000\n",
      "loss: 0.0538  acc: 1.0000\n",
      "loss: 0.7849  acc: 1.0000\n",
      "loss: 1.1769  acc: 0.5000\n",
      "loss: 0.5915  acc: 1.0000\n",
      "loss: 0.1491  acc: 1.0000\n",
      "loss: 0.8050  acc: 1.0000\n",
      "loss: 1.7979  acc: 0.5000\n",
      "loss: 0.3977  acc: 1.0000\n",
      "loss: 0.5940  acc: 1.0000\n",
      "loss: 0.9678  acc: 0.5000\n",
      "loss: 1.0091  acc: 0.5000\n",
      "loss: 0.7858  acc: 1.0000\n",
      "loss: 0.2723  acc: 1.0000\n",
      "loss: 0.8309  acc: 0.5000\n",
      "loss: 1.4970  acc: 0.5000\n",
      "loss: 1.1100  acc: 0.5000\n",
      "loss: 0.5955  acc: 1.0000\n",
      "loss: 0.9223  acc: 1.0000\n",
      "loss: 1.5788  acc: 0.5000\n",
      "loss: 1.5355  acc: 0.5000\n",
      "loss: 1.6328  acc: 0.0000\n",
      "loss: 1.1704  acc: 0.5000\n",
      "loss: 0.5345  acc: 1.0000\n",
      "epoch 27, train loss: 0.8209, train acc: 0.814\n",
      "test loss: 5.1399, test acc: 0.250\n",
      "loss: 0.5851  acc: 1.0000\n",
      "loss: 1.0931  acc: 0.5000\n",
      "loss: 0.7610  acc: 0.5000\n",
      "loss: 0.7427  acc: 1.0000\n",
      "loss: 0.6818  acc: 1.0000\n",
      "loss: 0.6794  acc: 1.0000\n",
      "loss: 0.8936  acc: 1.0000\n",
      "loss: 1.2638  acc: 1.0000\n",
      "loss: 2.5111  acc: 0.5000\n",
      "loss: 0.0475  acc: 1.0000\n",
      "loss: 0.4424  acc: 1.0000\n",
      "loss: 2.8501  acc: 0.5000\n",
      "loss: 0.2894  acc: 1.0000\n",
      "loss: 0.6523  acc: 0.5000\n",
      "loss: 0.4270  acc: 1.0000\n",
      "loss: 1.3091  acc: 0.5000\n",
      "loss: 0.8498  acc: 1.0000\n",
      "loss: 1.9946  acc: 0.5000\n",
      "loss: 0.8739  acc: 1.0000\n",
      "loss: 1.5811  acc: 0.5000\n",
      "loss: 1.2049  acc: 0.5000\n",
      "loss: 1.5638  acc: 0.5000\n",
      "loss: 0.8539  acc: 1.0000\n",
      "loss: 0.8716  acc: 1.0000\n",
      "loss: 0.1326  acc: 1.0000\n",
      "loss: 1.0426  acc: 0.5000\n",
      "loss: 1.2061  acc: 1.0000\n",
      "loss: 1.0056  acc: 0.5000\n",
      "loss: 1.6730  acc: 0.0000\n",
      "loss: 1.0289  acc: 0.5000\n",
      "loss: 1.9464  acc: 0.5000\n",
      "loss: 1.5763  acc: 0.0000\n",
      "loss: 2.2119  acc: 0.5000\n",
      "loss: 0.6160  acc: 1.0000\n",
      "loss: 1.0911  acc: 1.0000\n",
      "loss: 2.4267  acc: 0.5000\n",
      "loss: 0.3334  acc: 1.0000\n",
      "loss: 0.4898  acc: 1.0000\n",
      "loss: 1.3644  acc: 0.5000\n",
      "loss: 1.0068  acc: 1.0000\n",
      "loss: 1.3855  acc: 1.0000\n",
      "loss: 1.8646  acc: 0.5000\n",
      "loss: 1.6343  acc: 0.5000\n",
      "epoch 28, train loss: 1.1409, train acc: 0.733\n",
      "test loss: 4.4529, test acc: 0.325\n",
      "loss: 0.2081  acc: 1.0000\n",
      "loss: 0.3903  acc: 1.0000\n",
      "loss: 0.2778  acc: 1.0000\n",
      "loss: 0.3965  acc: 1.0000\n",
      "loss: 0.5993  acc: 1.0000\n",
      "loss: 0.4233  acc: 1.0000\n",
      "loss: 0.5321  acc: 1.0000\n",
      "loss: 1.1060  acc: 0.5000\n",
      "loss: 0.1862  acc: 1.0000\n",
      "loss: 1.6913  acc: 1.0000\n",
      "loss: 0.7144  acc: 1.0000\n",
      "loss: 0.2296  acc: 1.0000\n",
      "loss: 0.1954  acc: 1.0000\n",
      "loss: 0.7012  acc: 1.0000\n",
      "loss: 0.3963  acc: 1.0000\n",
      "loss: 0.5189  acc: 1.0000\n",
      "loss: 0.2573  acc: 1.0000\n",
      "loss: 0.6866  acc: 1.0000\n",
      "loss: 0.0864  acc: 1.0000\n",
      "loss: 1.3332  acc: 0.5000\n",
      "loss: 0.7153  acc: 0.5000\n",
      "loss: 1.3925  acc: 0.5000\n",
      "loss: 0.1162  acc: 1.0000\n",
      "loss: 0.1184  acc: 1.0000\n",
      "loss: 0.5695  acc: 1.0000\n",
      "loss: 0.0399  acc: 1.0000\n",
      "loss: 0.4851  acc: 1.0000\n",
      "loss: 0.7802  acc: 1.0000\n",
      "loss: 1.2825  acc: 0.5000\n",
      "loss: 0.5133  acc: 1.0000\n",
      "loss: 0.3227  acc: 1.0000\n",
      "loss: 0.8843  acc: 1.0000\n",
      "loss: 0.6723  acc: 1.0000\n",
      "loss: 1.4369  acc: 0.5000\n",
      "loss: 2.1292  acc: 0.5000\n",
      "loss: 1.1620  acc: 0.5000\n",
      "loss: 0.3073  acc: 1.0000\n",
      "loss: 0.2105  acc: 1.0000\n",
      "loss: 0.1564  acc: 1.0000\n",
      "loss: 0.5703  acc: 1.0000\n",
      "loss: 0.5295  acc: 1.0000\n",
      "loss: 1.0162  acc: 0.5000\n",
      "loss: 0.4211  acc: 1.0000\n",
      "epoch 29, train loss: 0.6224, train acc: 0.895\n",
      "test loss: 4.6333, test acc: 0.312\n",
      "loss: 0.0900  acc: 1.0000\n",
      "loss: 0.6267  acc: 1.0000\n",
      "loss: 0.9160  acc: 0.5000\n",
      "loss: 0.4517  acc: 1.0000\n",
      "loss: 0.3480  acc: 1.0000\n",
      "loss: 0.2562  acc: 1.0000\n",
      "loss: 0.2044  acc: 1.0000\n",
      "loss: 0.2357  acc: 1.0000\n",
      "loss: 0.3182  acc: 1.0000\n",
      "loss: 0.3222  acc: 1.0000\n",
      "loss: 0.5692  acc: 0.5000\n",
      "loss: 1.3974  acc: 0.5000\n",
      "loss: 0.8449  acc: 1.0000\n",
      "loss: 1.1305  acc: 1.0000\n",
      "loss: 0.5933  acc: 0.5000\n",
      "loss: 0.1048  acc: 1.0000\n",
      "loss: 1.1987  acc: 0.5000\n",
      "loss: 1.0207  acc: 1.0000\n",
      "loss: 0.6460  acc: 1.0000\n",
      "loss: 0.7961  acc: 0.5000\n",
      "loss: 0.6147  acc: 1.0000\n",
      "loss: 0.2136  acc: 1.0000\n",
      "loss: 0.4039  acc: 1.0000\n",
      "loss: 0.4037  acc: 1.0000\n",
      "loss: 0.3998  acc: 1.0000\n",
      "loss: 0.1896  acc: 1.0000\n",
      "loss: 1.0054  acc: 0.5000\n",
      "loss: 0.2950  acc: 1.0000\n",
      "loss: 0.5205  acc: 1.0000\n",
      "loss: 0.9129  acc: 1.0000\n",
      "loss: 1.0724  acc: 1.0000\n",
      "loss: 1.0570  acc: 0.5000\n",
      "loss: 2.5483  acc: 0.5000\n",
      "loss: 0.5411  acc: 1.0000\n",
      "loss: 2.0141  acc: 0.5000\n",
      "loss: 0.9076  acc: 1.0000\n",
      "loss: 0.3911  acc: 1.0000\n",
      "loss: 0.1539  acc: 1.0000\n",
      "loss: 0.2265  acc: 1.0000\n",
      "loss: 0.7139  acc: 1.0000\n",
      "loss: 0.6746  acc: 1.0000\n",
      "loss: 0.9689  acc: 1.0000\n",
      "loss: 1.2971  acc: 0.5000\n",
      "epoch 30, train loss: 0.6883, train acc: 0.872\n",
      "test loss: 4.7366, test acc: 0.275\n",
      "loss: 0.7992  acc: 0.5000\n",
      "loss: 0.6341  acc: 1.0000\n",
      "loss: 0.4336  acc: 1.0000\n",
      "loss: 0.0764  acc: 1.0000\n",
      "loss: 0.4316  acc: 1.0000\n",
      "loss: 0.7415  acc: 0.5000\n",
      "loss: 0.6298  acc: 0.5000\n",
      "loss: 0.1312  acc: 1.0000\n",
      "loss: 0.8803  acc: 0.5000\n",
      "loss: 0.1775  acc: 1.0000\n",
      "loss: 0.4917  acc: 1.0000\n",
      "loss: 0.7040  acc: 1.0000\n",
      "loss: 0.3873  acc: 1.0000\n",
      "loss: 0.3068  acc: 1.0000\n",
      "loss: 0.1654  acc: 1.0000\n",
      "loss: 0.9478  acc: 1.0000\n",
      "loss: 0.4141  acc: 1.0000\n",
      "loss: 0.1656  acc: 1.0000\n",
      "loss: 1.0096  acc: 1.0000\n",
      "loss: 0.1065  acc: 1.0000\n",
      "loss: 0.2073  acc: 1.0000\n",
      "loss: 0.2839  acc: 1.0000\n",
      "loss: 0.5188  acc: 1.0000\n",
      "loss: 0.6332  acc: 1.0000\n",
      "loss: 0.7966  acc: 1.0000\n",
      "loss: 0.4601  acc: 1.0000\n",
      "loss: 0.1800  acc: 1.0000\n",
      "loss: 0.1800  acc: 1.0000\n",
      "loss: 0.9840  acc: 0.5000\n",
      "loss: 0.2970  acc: 1.0000\n",
      "loss: 0.6275  acc: 0.5000\n",
      "loss: 0.1063  acc: 1.0000\n",
      "loss: 0.1908  acc: 1.0000\n",
      "loss: 0.5787  acc: 1.0000\n",
      "loss: 0.3330  acc: 1.0000\n",
      "loss: 0.3972  acc: 1.0000\n",
      "loss: 0.6344  acc: 1.0000\n",
      "loss: 1.1136  acc: 1.0000\n",
      "loss: 0.5741  acc: 1.0000\n",
      "loss: 0.4520  acc: 1.0000\n",
      "loss: 0.2489  acc: 1.0000\n",
      "loss: 0.7785  acc: 0.5000\n",
      "loss: 0.9391  acc: 0.5000\n",
      "epoch 31, train loss: 0.4918, train acc: 0.907\n",
      "test loss: 4.7858, test acc: 0.263\n",
      "loss: 0.1545  acc: 1.0000\n",
      "loss: 0.0743  acc: 1.0000\n",
      "loss: 0.1515  acc: 1.0000\n",
      "loss: 0.2054  acc: 1.0000\n",
      "loss: 0.3463  acc: 1.0000\n",
      "loss: 0.3487  acc: 1.0000\n",
      "loss: 0.6241  acc: 0.5000\n",
      "loss: 0.0459  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.1805  acc: 1.0000\n",
      "loss: 0.1624  acc: 1.0000\n",
      "loss: 0.1012  acc: 1.0000\n",
      "loss: 0.0911  acc: 1.0000\n",
      "loss: 0.1551  acc: 1.0000\n",
      "loss: 0.1608  acc: 1.0000\n",
      "loss: 1.2484  acc: 0.5000\n",
      "loss: 0.7169  acc: 1.0000\n",
      "loss: 0.9914  acc: 0.5000\n",
      "loss: 0.7557  acc: 1.0000\n",
      "loss: 0.5693  acc: 1.0000\n",
      "loss: 0.0599  acc: 1.0000\n",
      "loss: 0.2379  acc: 1.0000\n",
      "loss: 0.8330  acc: 1.0000\n",
      "loss: 0.2092  acc: 1.0000\n",
      "loss: 0.1694  acc: 1.0000\n",
      "loss: 0.1437  acc: 1.0000\n",
      "loss: 0.1130  acc: 1.0000\n",
      "loss: 0.1899  acc: 1.0000\n",
      "loss: 0.3088  acc: 1.0000\n",
      "loss: 0.0818  acc: 1.0000\n",
      "loss: 0.9728  acc: 1.0000\n",
      "loss: 0.2784  acc: 1.0000\n",
      "loss: 1.0562  acc: 0.5000\n",
      "loss: 0.4747  acc: 1.0000\n",
      "loss: 0.3097  acc: 1.0000\n",
      "loss: 0.1875  acc: 1.0000\n",
      "loss: 0.4543  acc: 1.0000\n",
      "loss: 0.2559  acc: 1.0000\n",
      "loss: 0.2466  acc: 1.0000\n",
      "loss: 0.2245  acc: 1.0000\n",
      "loss: 0.0759  acc: 1.0000\n",
      "loss: 2.2177  acc: 0.5000\n",
      "loss: 0.3143  acc: 1.0000\n",
      "epoch 32, train loss: 0.3839, train acc: 0.942\n",
      "test loss: 4.7745, test acc: 0.287\n",
      "loss: 0.0457  acc: 1.0000\n",
      "loss: 0.6058  acc: 1.0000\n",
      "loss: 0.3373  acc: 1.0000\n",
      "loss: 0.1608  acc: 1.0000\n",
      "loss: 0.1062  acc: 1.0000\n",
      "loss: 0.0972  acc: 1.0000\n",
      "loss: 0.4537  acc: 1.0000\n",
      "loss: 0.0559  acc: 1.0000\n",
      "loss: 0.4941  acc: 1.0000\n",
      "loss: 0.0705  acc: 1.0000\n",
      "loss: 0.3842  acc: 1.0000\n",
      "loss: 0.1017  acc: 1.0000\n",
      "loss: 0.5246  acc: 1.0000\n",
      "loss: 0.3640  acc: 1.0000\n",
      "loss: 0.1147  acc: 1.0000\n",
      "loss: 0.1481  acc: 1.0000\n",
      "loss: 0.7017  acc: 0.5000\n",
      "loss: 0.1245  acc: 1.0000\n",
      "loss: 0.2606  acc: 1.0000\n",
      "loss: 0.2749  acc: 1.0000\n",
      "loss: 0.0751  acc: 1.0000\n",
      "loss: 0.4693  acc: 1.0000\n",
      "loss: 0.8786  acc: 1.0000\n",
      "loss: 0.0481  acc: 1.0000\n",
      "loss: 0.4072  acc: 1.0000\n",
      "loss: 0.5643  acc: 1.0000\n",
      "loss: 0.1491  acc: 1.0000\n",
      "loss: 0.0824  acc: 1.0000\n",
      "loss: 0.9980  acc: 0.5000\n",
      "loss: 0.1410  acc: 1.0000\n",
      "loss: 0.2613  acc: 1.0000\n",
      "loss: 0.1206  acc: 1.0000\n",
      "loss: 0.2741  acc: 1.0000\n",
      "loss: 0.1223  acc: 1.0000\n",
      "loss: 0.2044  acc: 1.0000\n",
      "loss: 0.2431  acc: 1.0000\n",
      "loss: 0.8099  acc: 1.0000\n",
      "loss: 0.1329  acc: 1.0000\n",
      "loss: 0.1961  acc: 1.0000\n",
      "loss: 0.6459  acc: 1.0000\n",
      "loss: 0.5018  acc: 1.0000\n",
      "loss: 0.0400  acc: 1.0000\n",
      "loss: 3.1224  acc: 0.5000\n",
      "epoch 33, train loss: 0.3701, train acc: 0.965\n",
      "test loss: 5.1904, test acc: 0.275\n",
      "loss: 0.2537  acc: 1.0000\n",
      "loss: 0.0327  acc: 1.0000\n",
      "loss: 0.1811  acc: 1.0000\n",
      "loss: 0.1908  acc: 1.0000\n",
      "loss: 0.1419  acc: 1.0000\n",
      "loss: 0.1911  acc: 1.0000\n",
      "loss: 0.2556  acc: 1.0000\n",
      "loss: 0.3263  acc: 1.0000\n",
      "loss: 0.4576  acc: 1.0000\n",
      "loss: 0.0312  acc: 1.0000\n",
      "loss: 0.1239  acc: 1.0000\n",
      "loss: 0.1995  acc: 1.0000\n",
      "loss: 0.1457  acc: 1.0000\n",
      "loss: 0.0674  acc: 1.0000\n",
      "loss: 0.1300  acc: 1.0000\n",
      "loss: 0.1463  acc: 1.0000\n",
      "loss: 0.6641  acc: 1.0000\n",
      "loss: 0.2088  acc: 1.0000\n",
      "loss: 0.0943  acc: 1.0000\n",
      "loss: 0.1663  acc: 1.0000\n",
      "loss: 0.6195  acc: 1.0000\n",
      "loss: 0.3947  acc: 1.0000\n",
      "loss: 0.0249  acc: 1.0000\n",
      "loss: 0.1608  acc: 1.0000\n",
      "loss: 0.1699  acc: 1.0000\n",
      "loss: 0.1066  acc: 1.0000\n",
      "loss: 0.0396  acc: 1.0000\n",
      "loss: 0.3405  acc: 1.0000\n",
      "loss: 0.5788  acc: 0.5000\n",
      "loss: 0.6140  acc: 1.0000\n",
      "loss: 0.4833  acc: 1.0000\n",
      "loss: 0.1976  acc: 1.0000\n",
      "loss: 0.1102  acc: 1.0000\n",
      "loss: 0.1489  acc: 1.0000\n",
      "loss: 0.0449  acc: 1.0000\n",
      "loss: 0.1076  acc: 1.0000\n",
      "loss: 0.3190  acc: 1.0000\n",
      "loss: 0.7746  acc: 1.0000\n",
      "loss: 0.0999  acc: 1.0000\n",
      "loss: 0.2769  acc: 1.0000\n",
      "loss: 0.0502  acc: 1.0000\n",
      "loss: 0.3107  acc: 1.0000\n",
      "loss: 0.1401  acc: 1.0000\n",
      "epoch 34, train loss: 0.2354, train acc: 0.988\n",
      "test loss: 4.6118, test acc: 0.312\n",
      "loss: 0.2249  acc: 1.0000\n",
      "loss: 0.0489  acc: 1.0000\n",
      "loss: 0.1295  acc: 1.0000\n",
      "loss: 0.6073  acc: 1.0000\n",
      "loss: 0.1248  acc: 1.0000\n",
      "loss: 0.1434  acc: 1.0000\n",
      "loss: 0.1180  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0553  acc: 1.0000\n",
      "loss: 0.0770  acc: 1.0000\n",
      "loss: 0.2345  acc: 1.0000\n",
      "loss: 0.8314  acc: 1.0000\n",
      "loss: 0.1223  acc: 1.0000\n",
      "loss: 0.2019  acc: 1.0000\n",
      "loss: 0.1171  acc: 1.0000\n",
      "loss: 0.0303  acc: 1.0000\n",
      "loss: 0.0656  acc: 1.0000\n",
      "loss: 0.0540  acc: 1.0000\n",
      "loss: 0.0884  acc: 1.0000\n",
      "loss: 0.1605  acc: 1.0000\n",
      "loss: 0.1855  acc: 1.0000\n",
      "loss: 0.4771  acc: 1.0000\n",
      "loss: 0.2890  acc: 1.0000\n",
      "loss: 0.0895  acc: 1.0000\n",
      "loss: 0.2583  acc: 1.0000\n",
      "loss: 0.0294  acc: 1.0000\n",
      "loss: 0.2270  acc: 1.0000\n",
      "loss: 0.2373  acc: 1.0000\n",
      "loss: 0.0859  acc: 1.0000\n",
      "loss: 0.1739  acc: 1.0000\n",
      "loss: 0.3156  acc: 1.0000\n",
      "loss: 0.1508  acc: 1.0000\n",
      "loss: 0.1714  acc: 1.0000\n",
      "loss: 0.2647  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0639  acc: 1.0000\n",
      "loss: 0.0609  acc: 1.0000\n",
      "loss: 0.0739  acc: 1.0000\n",
      "loss: 0.0371  acc: 1.0000\n",
      "loss: 0.2198  acc: 1.0000\n",
      "loss: 0.1591  acc: 1.0000\n",
      "loss: 0.5109  acc: 1.0000\n",
      "loss: 0.1298  acc: 1.0000\n",
      "epoch 35, train loss: 0.1786, train acc: 1.000\n",
      "test loss: 4.5933, test acc: 0.338\n",
      "loss: 0.0847  acc: 1.0000\n",
      "loss: 0.2173  acc: 1.0000\n",
      "loss: 0.0883  acc: 1.0000\n",
      "loss: 0.0780  acc: 1.0000\n",
      "loss: 0.1162  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.2467  acc: 1.0000\n",
      "loss: 0.0968  acc: 1.0000\n",
      "loss: 0.0270  acc: 1.0000\n",
      "loss: 0.1682  acc: 1.0000\n",
      "loss: 0.0437  acc: 1.0000\n",
      "loss: 0.0480  acc: 1.0000\n",
      "loss: 0.0359  acc: 1.0000\n",
      "loss: 0.1247  acc: 1.0000\n",
      "loss: 0.0906  acc: 1.0000\n",
      "loss: 0.0381  acc: 1.0000\n",
      "loss: 0.2067  acc: 1.0000\n",
      "loss: 0.0558  acc: 1.0000\n",
      "loss: 0.0453  acc: 1.0000\n",
      "loss: 0.2888  acc: 1.0000\n",
      "loss: 0.0998  acc: 1.0000\n",
      "loss: 0.6548  acc: 1.0000\n",
      "loss: 0.0161  acc: 1.0000\n",
      "loss: 0.0973  acc: 1.0000\n",
      "loss: 0.1740  acc: 1.0000\n",
      "loss: 0.1865  acc: 1.0000\n",
      "loss: 0.0362  acc: 1.0000\n",
      "loss: 0.2845  acc: 1.0000\n",
      "loss: 0.2442  acc: 1.0000\n",
      "loss: 0.2338  acc: 1.0000\n",
      "loss: 0.1475  acc: 1.0000\n",
      "loss: 0.1841  acc: 1.0000\n",
      "loss: 0.3920  acc: 1.0000\n",
      "loss: 0.1291  acc: 1.0000\n",
      "loss: 0.0900  acc: 1.0000\n",
      "loss: 0.1844  acc: 1.0000\n",
      "loss: 0.0326  acc: 1.0000\n",
      "loss: 0.3899  acc: 1.0000\n",
      "loss: 0.2179  acc: 1.0000\n",
      "loss: 0.0701  acc: 1.0000\n",
      "loss: 0.1071  acc: 1.0000\n",
      "loss: 0.0228  acc: 1.0000\n",
      "loss: 0.0381  acc: 1.0000\n",
      "epoch 36, train loss: 0.1429, train acc: 1.000\n",
      "test loss: 4.7271, test acc: 0.325\n",
      "loss: 0.0413  acc: 1.0000\n",
      "loss: 0.1069  acc: 1.0000\n",
      "loss: 0.0379  acc: 1.0000\n",
      "loss: 0.0569  acc: 1.0000\n",
      "loss: 0.0596  acc: 1.0000\n",
      "loss: 0.0264  acc: 1.0000\n",
      "loss: 0.0513  acc: 1.0000\n",
      "loss: 0.1310  acc: 1.0000\n",
      "loss: 0.0522  acc: 1.0000\n",
      "loss: 0.1151  acc: 1.0000\n",
      "loss: 0.1216  acc: 1.0000\n",
      "loss: 0.0211  acc: 1.0000\n",
      "loss: 0.1154  acc: 1.0000\n",
      "loss: 0.0427  acc: 1.0000\n",
      "loss: 0.1764  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0512  acc: 1.0000\n",
      "loss: 0.0928  acc: 1.0000\n",
      "loss: 0.1543  acc: 1.0000\n",
      "loss: 0.0240  acc: 1.0000\n",
      "loss: 0.2223  acc: 1.0000\n",
      "loss: 0.1806  acc: 1.0000\n",
      "loss: 0.0881  acc: 1.0000\n",
      "loss: 0.0643  acc: 1.0000\n",
      "loss: 0.0578  acc: 1.0000\n",
      "loss: 0.1742  acc: 1.0000\n",
      "loss: 0.1476  acc: 1.0000\n",
      "loss: 0.0659  acc: 1.0000\n",
      "loss: 0.1635  acc: 1.0000\n",
      "loss: 0.0785  acc: 1.0000\n",
      "loss: 0.0251  acc: 1.0000\n",
      "loss: 0.0806  acc: 1.0000\n",
      "loss: 0.4901  acc: 1.0000\n",
      "loss: 0.0637  acc: 1.0000\n",
      "loss: 0.0495  acc: 1.0000\n",
      "loss: 0.0803  acc: 1.0000\n",
      "loss: 0.1125  acc: 1.0000\n",
      "loss: 0.1531  acc: 1.0000\n",
      "loss: 0.0646  acc: 1.0000\n",
      "loss: 0.3524  acc: 1.0000\n",
      "loss: 0.1304  acc: 1.0000\n",
      "loss: 0.1540  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "epoch 37, train loss: 0.1045, train acc: 1.000\n",
      "test loss: 4.4661, test acc: 0.287\n",
      "loss: 0.0377  acc: 1.0000\n",
      "loss: 0.0901  acc: 1.0000\n",
      "loss: 0.0535  acc: 1.0000\n",
      "loss: 0.0624  acc: 1.0000\n",
      "loss: 0.1072  acc: 1.0000\n",
      "loss: 0.0805  acc: 1.0000\n",
      "loss: 0.3584  acc: 1.0000\n",
      "loss: 0.1500  acc: 1.0000\n",
      "loss: 0.0824  acc: 1.0000\n",
      "loss: 0.0448  acc: 1.0000\n",
      "loss: 0.0908  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0939  acc: 1.0000\n",
      "loss: 0.0533  acc: 1.0000\n",
      "loss: 0.0288  acc: 1.0000\n",
      "loss: 0.0777  acc: 1.0000\n",
      "loss: 0.1646  acc: 1.0000\n",
      "loss: 0.0913  acc: 1.0000\n",
      "loss: 0.0149  acc: 1.0000\n",
      "loss: 0.0661  acc: 1.0000\n",
      "loss: 0.7038  acc: 0.5000\n",
      "loss: 0.1160  acc: 1.0000\n",
      "loss: 0.1564  acc: 1.0000\n",
      "loss: 0.1483  acc: 1.0000\n",
      "loss: 0.3024  acc: 1.0000\n",
      "loss: 0.0576  acc: 1.0000\n",
      "loss: 0.1127  acc: 1.0000\n",
      "loss: 0.0335  acc: 1.0000\n",
      "loss: 0.1302  acc: 1.0000\n",
      "loss: 0.2769  acc: 1.0000\n",
      "loss: 0.1274  acc: 1.0000\n",
      "loss: 0.0884  acc: 1.0000\n",
      "loss: 0.1726  acc: 1.0000\n",
      "loss: 0.0694  acc: 1.0000\n",
      "loss: 0.6335  acc: 1.0000\n",
      "loss: 0.1097  acc: 1.0000\n",
      "loss: 0.1688  acc: 1.0000\n",
      "loss: 0.0981  acc: 1.0000\n",
      "loss: 0.1412  acc: 1.0000\n",
      "loss: 0.4297  acc: 1.0000\n",
      "loss: 0.0640  acc: 1.0000\n",
      "loss: 0.0181  acc: 1.0000\n",
      "epoch 38, train loss: 0.1381, train acc: 0.988\n",
      "test loss: 4.7353, test acc: 0.263\n",
      "loss: 0.2384  acc: 1.0000\n",
      "loss: 0.0408  acc: 1.0000\n",
      "loss: 0.1580  acc: 1.0000\n",
      "loss: 0.0472  acc: 1.0000\n",
      "loss: 0.1599  acc: 1.0000\n",
      "loss: 0.0564  acc: 1.0000\n",
      "loss: 0.0596  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.1798  acc: 1.0000\n",
      "loss: 0.2045  acc: 1.0000\n",
      "loss: 0.0470  acc: 1.0000\n",
      "loss: 0.0250  acc: 1.0000\n",
      "loss: 0.0446  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.1896  acc: 1.0000\n",
      "loss: 0.0687  acc: 1.0000\n",
      "loss: 0.0243  acc: 1.0000\n",
      "loss: 0.1399  acc: 1.0000\n",
      "loss: 0.2142  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.3339  acc: 1.0000\n",
      "loss: 0.0661  acc: 1.0000\n",
      "loss: 0.0512  acc: 1.0000\n",
      "loss: 1.9362  acc: 0.5000\n",
      "loss: 0.2042  acc: 1.0000\n",
      "loss: 0.0930  acc: 1.0000\n",
      "loss: 0.1970  acc: 1.0000\n",
      "loss: 0.2367  acc: 1.0000\n",
      "loss: 0.1233  acc: 1.0000\n",
      "loss: 0.3670  acc: 1.0000\n",
      "loss: 0.0362  acc: 1.0000\n",
      "loss: 0.2391  acc: 1.0000\n",
      "loss: 0.7402  acc: 1.0000\n",
      "loss: 0.6877  acc: 1.0000\n",
      "loss: 0.0608  acc: 1.0000\n",
      "loss: 0.1891  acc: 1.0000\n",
      "loss: 0.1894  acc: 1.0000\n",
      "loss: 0.8917  acc: 1.0000\n",
      "loss: 0.1968  acc: 1.0000\n",
      "loss: 1.4070  acc: 0.5000\n",
      "loss: 0.1088  acc: 1.0000\n",
      "loss: 0.2551  acc: 1.0000\n",
      "loss: 0.1067  acc: 1.0000\n",
      "epoch 39, train loss: 0.2479, train acc: 0.977\n",
      "test loss: 4.6511, test acc: 0.300\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.1387  acc: 1.0000\n",
      "loss: 0.1193  acc: 1.0000\n",
      "loss: 0.0777  acc: 1.0000\n",
      "loss: 0.0255  acc: 1.0000\n",
      "loss: 0.0431  acc: 1.0000\n",
      "loss: 0.0859  acc: 1.0000\n",
      "loss: 0.1423  acc: 1.0000\n",
      "loss: 0.2041  acc: 1.0000\n",
      "loss: 0.5406  acc: 1.0000\n",
      "loss: 0.0324  acc: 1.0000\n",
      "loss: 0.0418  acc: 1.0000\n",
      "loss: 0.0464  acc: 1.0000\n",
      "loss: 0.2139  acc: 1.0000\n",
      "loss: 0.2661  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.0480  acc: 1.0000\n",
      "loss: 0.0512  acc: 1.0000\n",
      "loss: 0.0836  acc: 1.0000\n",
      "loss: 0.1009  acc: 1.0000\n",
      "loss: 0.0563  acc: 1.0000\n",
      "loss: 0.0299  acc: 1.0000\n",
      "loss: 0.0562  acc: 1.0000\n",
      "loss: 0.1099  acc: 1.0000\n",
      "loss: 0.0531  acc: 1.0000\n",
      "loss: 0.1050  acc: 1.0000\n",
      "loss: 0.1567  acc: 1.0000\n",
      "loss: 1.2133  acc: 0.5000\n",
      "loss: 0.2008  acc: 1.0000\n",
      "loss: 0.2747  acc: 1.0000\n",
      "loss: 0.1526  acc: 1.0000\n",
      "loss: 0.5079  acc: 0.5000\n",
      "loss: 0.5161  acc: 0.5000\n",
      "loss: 0.2977  acc: 1.0000\n",
      "loss: 0.0754  acc: 1.0000\n",
      "loss: 0.3366  acc: 1.0000\n",
      "loss: 0.3295  acc: 1.0000\n",
      "loss: 0.6233  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0447  acc: 1.0000\n",
      "epoch 40, train loss: 0.1744, train acc: 0.965\n",
      "test loss: 5.0308, test acc: 0.287\n",
      "loss: 0.1621  acc: 1.0000\n",
      "loss: 0.1509  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0915  acc: 1.0000\n",
      "loss: 0.0824  acc: 1.0000\n",
      "loss: 1.3710  acc: 0.5000\n",
      "loss: 0.0584  acc: 1.0000\n",
      "loss: 0.1853  acc: 1.0000\n",
      "loss: 0.2194  acc: 1.0000\n",
      "loss: 0.0491  acc: 1.0000\n",
      "loss: 0.1483  acc: 1.0000\n",
      "loss: 0.2074  acc: 1.0000\n",
      "loss: 0.0752  acc: 1.0000\n",
      "loss: 0.5599  acc: 1.0000\n",
      "loss: 0.0686  acc: 1.0000\n",
      "loss: 0.0928  acc: 1.0000\n",
      "loss: 0.2702  acc: 1.0000\n",
      "loss: 0.1474  acc: 1.0000\n",
      "loss: 1.2231  acc: 0.5000\n",
      "loss: 0.1300  acc: 1.0000\n",
      "loss: 0.1496  acc: 1.0000\n",
      "loss: 0.1946  acc: 1.0000\n",
      "loss: 0.1037  acc: 1.0000\n",
      "loss: 0.0661  acc: 1.0000\n",
      "loss: 0.0286  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.1158  acc: 1.0000\n",
      "loss: 0.0732  acc: 1.0000\n",
      "loss: 0.1405  acc: 1.0000\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.2309  acc: 1.0000\n",
      "loss: 0.0297  acc: 1.0000\n",
      "loss: 0.0608  acc: 1.0000\n",
      "loss: 0.0803  acc: 1.0000\n",
      "loss: 0.0437  acc: 1.0000\n",
      "loss: 0.1123  acc: 1.0000\n",
      "loss: 0.0429  acc: 1.0000\n",
      "loss: 0.1867  acc: 1.0000\n",
      "loss: 0.1688  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0176  acc: 1.0000\n",
      "loss: 0.0236  acc: 1.0000\n",
      "epoch 41, train loss: 0.1688, train acc: 0.977\n",
      "test loss: 4.6306, test acc: 0.325\n",
      "loss: 0.0459  acc: 1.0000\n",
      "loss: 0.0506  acc: 1.0000\n",
      "loss: 0.3099  acc: 1.0000\n",
      "loss: 0.1536  acc: 1.0000\n",
      "loss: 0.0445  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0742  acc: 1.0000\n",
      "loss: 0.0446  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0206  acc: 1.0000\n",
      "loss: 0.2685  acc: 1.0000\n",
      "loss: 0.2774  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0426  acc: 1.0000\n",
      "loss: 0.0488  acc: 1.0000\n",
      "loss: 0.0539  acc: 1.0000\n",
      "loss: 0.1053  acc: 1.0000\n",
      "loss: 0.4129  acc: 1.0000\n",
      "loss: 0.1188  acc: 1.0000\n",
      "loss: 0.0694  acc: 1.0000\n",
      "loss: 0.1238  acc: 1.0000\n",
      "loss: 0.0235  acc: 1.0000\n",
      "loss: 1.0891  acc: 1.0000\n",
      "loss: 0.3661  acc: 1.0000\n",
      "loss: 0.0669  acc: 1.0000\n",
      "loss: 0.1497  acc: 1.0000\n",
      "loss: 0.0612  acc: 1.0000\n",
      "loss: 0.0774  acc: 1.0000\n",
      "loss: 0.1289  acc: 1.0000\n",
      "loss: 0.3235  acc: 1.0000\n",
      "loss: 0.0757  acc: 1.0000\n",
      "loss: 0.6366  acc: 1.0000\n",
      "loss: 0.1906  acc: 1.0000\n",
      "loss: 0.1960  acc: 1.0000\n",
      "loss: 0.1006  acc: 1.0000\n",
      "loss: 0.0618  acc: 1.0000\n",
      "loss: 0.1051  acc: 1.0000\n",
      "loss: 0.0755  acc: 1.0000\n",
      "loss: 0.1619  acc: 1.0000\n",
      "loss: 0.0902  acc: 1.0000\n",
      "loss: 0.0987  acc: 1.0000\n",
      "loss: 0.0876  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "epoch 42, train loss: 0.1510, train acc: 1.000\n",
      "test loss: 4.4623, test acc: 0.312\n",
      "loss: 0.0305  acc: 1.0000\n",
      "loss: 0.0352  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.1280  acc: 1.0000\n",
      "loss: 0.1023  acc: 1.0000\n",
      "loss: 0.0804  acc: 1.0000\n",
      "loss: 0.0159  acc: 1.0000\n",
      "loss: 0.4421  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0150  acc: 1.0000\n",
      "loss: 0.0966  acc: 1.0000\n",
      "loss: 0.0564  acc: 1.0000\n",
      "loss: 0.0293  acc: 1.0000\n",
      "loss: 0.1694  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0264  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0761  acc: 1.0000\n",
      "loss: 0.1881  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0231  acc: 1.0000\n",
      "loss: 0.0808  acc: 1.0000\n",
      "loss: 0.0783  acc: 1.0000\n",
      "loss: 0.5208  acc: 1.0000\n",
      "loss: 0.0351  acc: 1.0000\n",
      "loss: 0.0712  acc: 1.0000\n",
      "loss: 0.0224  acc: 1.0000\n",
      "loss: 0.0482  acc: 1.0000\n",
      "loss: 0.0609  acc: 1.0000\n",
      "loss: 0.0896  acc: 1.0000\n",
      "loss: 0.0396  acc: 1.0000\n",
      "loss: 0.1641  acc: 1.0000\n",
      "loss: 0.2105  acc: 1.0000\n",
      "loss: 0.0672  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0551  acc: 1.0000\n",
      "loss: 0.0541  acc: 1.0000\n",
      "loss: 0.0454  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "loss: 0.0554  acc: 1.0000\n",
      "loss: 0.2254  acc: 1.0000\n",
      "epoch 43, train loss: 0.0829, train acc: 1.000\n",
      "test loss: 4.2635, test acc: 0.325\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0272  acc: 1.0000\n",
      "loss: 0.0362  acc: 1.0000\n",
      "loss: 0.0286  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0283  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0479  acc: 1.0000\n",
      "loss: 0.1626  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0357  acc: 1.0000\n",
      "loss: 0.0427  acc: 1.0000\n",
      "loss: 0.0639  acc: 1.0000\n",
      "loss: 0.1310  acc: 1.0000\n",
      "loss: 0.0572  acc: 1.0000\n",
      "loss: 0.0287  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0778  acc: 1.0000\n",
      "loss: 0.0565  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0572  acc: 1.0000\n",
      "loss: 0.0717  acc: 1.0000\n",
      "loss: 0.0391  acc: 1.0000\n",
      "loss: 0.0340  acc: 1.0000\n",
      "loss: 0.0366  acc: 1.0000\n",
      "loss: 0.1084  acc: 1.0000\n",
      "loss: 0.1547  acc: 1.0000\n",
      "loss: 0.0542  acc: 1.0000\n",
      "loss: 0.0736  acc: 1.0000\n",
      "loss: 0.0836  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0501  acc: 1.0000\n",
      "loss: 0.0314  acc: 1.0000\n",
      "loss: 0.9036  acc: 1.0000\n",
      "loss: 0.1266  acc: 1.0000\n",
      "loss: 0.2741  acc: 1.0000\n",
      "loss: 0.0286  acc: 1.0000\n",
      "loss: 0.0809  acc: 1.0000\n",
      "loss: 0.1646  acc: 1.0000\n",
      "epoch 44, train loss: 0.0780, train acc: 1.000\n",
      "test loss: 4.9364, test acc: 0.237\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.0693  acc: 1.0000\n",
      "loss: 0.1157  acc: 1.0000\n",
      "loss: 0.0465  acc: 1.0000\n",
      "loss: 0.0717  acc: 1.0000\n",
      "loss: 0.0528  acc: 1.0000\n",
      "loss: 0.0567  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0828  acc: 1.0000\n",
      "loss: 0.0553  acc: 1.0000\n",
      "loss: 0.0289  acc: 1.0000\n",
      "loss: 0.0620  acc: 1.0000\n",
      "loss: 0.0470  acc: 1.0000\n",
      "loss: 0.0862  acc: 1.0000\n",
      "loss: 0.0573  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0399  acc: 1.0000\n",
      "loss: 0.0340  acc: 1.0000\n",
      "loss: 0.0745  acc: 1.0000\n",
      "loss: 0.0535  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.2415  acc: 1.0000\n",
      "loss: 0.0334  acc: 1.0000\n",
      "loss: 0.0563  acc: 1.0000\n",
      "loss: 0.0530  acc: 1.0000\n",
      "loss: 0.0323  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0942  acc: 1.0000\n",
      "loss: 0.4015  acc: 1.0000\n",
      "loss: 0.0299  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0358  acc: 1.0000\n",
      "loss: 0.0345  acc: 1.0000\n",
      "loss: 0.0508  acc: 1.0000\n",
      "loss: 0.0311  acc: 1.0000\n",
      "loss: 0.2154  acc: 1.0000\n",
      "loss: 0.0772  acc: 1.0000\n",
      "loss: 0.0263  acc: 1.0000\n",
      "loss: 0.0404  acc: 1.0000\n",
      "loss: 0.0538  acc: 1.0000\n",
      "loss: 0.2408  acc: 1.0000\n",
      "loss: 0.1675  acc: 1.0000\n",
      "epoch 45, train loss: 0.0714, train acc: 1.000\n",
      "test loss: 4.4298, test acc: 0.312\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0504  acc: 1.0000\n",
      "loss: 0.0237  acc: 1.0000\n",
      "loss: 0.0352  acc: 1.0000\n",
      "loss: 0.0400  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0383  acc: 1.0000\n",
      "loss: 0.0860  acc: 1.0000\n",
      "loss: 0.2204  acc: 1.0000\n",
      "loss: 0.0230  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0385  acc: 1.0000\n",
      "loss: 0.0364  acc: 1.0000\n",
      "loss: 0.0409  acc: 1.0000\n",
      "loss: 0.0342  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0393  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0271  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0217  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.1316  acc: 1.0000\n",
      "loss: 0.0159  acc: 1.0000\n",
      "loss: 0.0398  acc: 1.0000\n",
      "loss: 0.0333  acc: 1.0000\n",
      "loss: 0.1524  acc: 1.0000\n",
      "loss: 0.0518  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0367  acc: 1.0000\n",
      "loss: 0.0731  acc: 1.0000\n",
      "loss: 0.1702  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.2691  acc: 1.0000\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0221  acc: 1.0000\n",
      "loss: 0.0566  acc: 1.0000\n",
      "loss: 0.0735  acc: 1.0000\n",
      "loss: 0.0420  acc: 1.0000\n",
      "loss: 0.2626  acc: 1.0000\n",
      "epoch 46, train loss: 0.0548, train acc: 1.000\n",
      "test loss: 4.4435, test acc: 0.325\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0172  acc: 1.0000\n",
      "loss: 0.0309  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "loss: 0.0168  acc: 1.0000\n",
      "loss: 0.0203  acc: 1.0000\n",
      "loss: 0.0193  acc: 1.0000\n",
      "loss: 0.0529  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0629  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.3398  acc: 1.0000\n",
      "loss: 0.1340  acc: 1.0000\n",
      "loss: 0.0355  acc: 1.0000\n",
      "loss: 0.0799  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0655  acc: 1.0000\n",
      "loss: 0.1921  acc: 1.0000\n",
      "loss: 0.0594  acc: 1.0000\n",
      "loss: 0.0274  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0795  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0407  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0364  acc: 1.0000\n",
      "loss: 0.0388  acc: 1.0000\n",
      "loss: 0.0791  acc: 1.0000\n",
      "loss: 0.0212  acc: 1.0000\n",
      "loss: 0.0903  acc: 1.0000\n",
      "loss: 0.0329  acc: 1.0000\n",
      "loss: 0.1590  acc: 1.0000\n",
      "loss: 0.0404  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0726  acc: 1.0000\n",
      "loss: 0.0897  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0588  acc: 1.0000\n",
      "loss: 0.0374  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0229  acc: 1.0000\n",
      "epoch 47, train loss: 0.0518, train acc: 1.000\n",
      "test loss: 4.6647, test acc: 0.312\n",
      "loss: 0.0450  acc: 1.0000\n",
      "loss: 0.0184  acc: 1.0000\n",
      "loss: 0.0264  acc: 1.0000\n",
      "loss: 0.0225  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0304  acc: 1.0000\n",
      "loss: 0.0337  acc: 1.0000\n",
      "loss: 0.0320  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0381  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0552  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.0851  acc: 1.0000\n",
      "loss: 0.0187  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0142  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.1292  acc: 1.0000\n",
      "loss: 0.0989  acc: 1.0000\n",
      "loss: 0.0602  acc: 1.0000\n",
      "loss: 0.0548  acc: 1.0000\n",
      "loss: 0.0172  acc: 1.0000\n",
      "loss: 0.1070  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0174  acc: 1.0000\n",
      "loss: 0.0799  acc: 1.0000\n",
      "loss: 0.0495  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0233  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.3097  acc: 1.0000\n",
      "loss: 0.0517  acc: 1.0000\n",
      "loss: 0.0481  acc: 1.0000\n",
      "loss: 0.0393  acc: 1.0000\n",
      "loss: 0.0220  acc: 1.0000\n",
      "loss: 0.0514  acc: 1.0000\n",
      "loss: 0.0487  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "epoch 48, train loss: 0.0424, train acc: 1.000\n",
      "test loss: 4.3556, test acc: 0.350\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0632  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0386  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.1333  acc: 1.0000\n",
      "loss: 0.1240  acc: 1.0000\n",
      "loss: 0.0259  acc: 1.0000\n",
      "loss: 0.0197  acc: 1.0000\n",
      "loss: 0.0185  acc: 1.0000\n",
      "loss: 0.0188  acc: 1.0000\n",
      "loss: 0.0335  acc: 1.0000\n",
      "loss: 0.0238  acc: 1.0000\n",
      "loss: 0.0574  acc: 1.0000\n",
      "loss: 0.0491  acc: 1.0000\n",
      "loss: 0.0818  acc: 1.0000\n",
      "loss: 0.1394  acc: 1.0000\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0191  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0554  acc: 1.0000\n",
      "loss: 0.0331  acc: 1.0000\n",
      "loss: 0.0312  acc: 1.0000\n",
      "loss: 0.0214  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0218  acc: 1.0000\n",
      "loss: 0.0504  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0257  acc: 1.0000\n",
      "loss: 0.0672  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0914  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0437  acc: 1.0000\n",
      "loss: 0.0524  acc: 1.0000\n",
      "epoch 49, train loss: 0.0361, train acc: 1.000\n",
      "test loss: 4.3003, test acc: 0.350\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0310  acc: 1.0000\n",
      "loss: 0.1590  acc: 1.0000\n",
      "loss: 0.0799  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0835  acc: 1.0000\n",
      "loss: 0.0450  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0473  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.1596  acc: 1.0000\n",
      "loss: 0.1105  acc: 1.0000\n",
      "loss: 0.0888  acc: 1.0000\n",
      "loss: 0.0394  acc: 1.0000\n",
      "loss: 0.0336  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0320  acc: 1.0000\n",
      "loss: 0.0387  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0154  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0330  acc: 1.0000\n",
      "loss: 0.0546  acc: 1.0000\n",
      "loss: 0.1465  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0229  acc: 1.0000\n",
      "loss: 0.0317  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0515  acc: 1.0000\n",
      "loss: 0.0311  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0282  acc: 1.0000\n",
      "loss: 0.0164  acc: 1.0000\n",
      "loss: 0.0524  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0815  acc: 1.0000\n",
      "epoch 50, train loss: 0.0396, train acc: 1.000\n",
      "test loss: 4.2777, test acc: 0.338\n",
      "loss: 0.0273  acc: 1.0000\n",
      "loss: 0.0259  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0737  acc: 1.0000\n",
      "loss: 0.0246  acc: 1.0000\n",
      "loss: 0.0142  acc: 1.0000\n",
      "loss: 0.0480  acc: 1.0000\n",
      "loss: 0.0520  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0507  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0514  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0333  acc: 1.0000\n",
      "loss: 0.0289  acc: 1.0000\n",
      "loss: 0.0241  acc: 1.0000\n",
      "loss: 0.0187  acc: 1.0000\n",
      "loss: 0.0186  acc: 1.0000\n",
      "loss: 0.0292  acc: 1.0000\n",
      "loss: 0.0503  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.1248  acc: 1.0000\n",
      "loss: 0.0404  acc: 1.0000\n",
      "loss: 0.0210  acc: 1.0000\n",
      "loss: 0.0980  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0175  acc: 1.0000\n",
      "loss: 0.0321  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0403  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0208  acc: 1.0000\n",
      "epoch 51, train loss: 0.0272, train acc: 1.000\n",
      "test loss: 4.3801, test acc: 0.350\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.1781  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0262  acc: 1.0000\n",
      "loss: 0.1015  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0452  acc: 1.0000\n",
      "loss: 0.0434  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0154  acc: 1.0000\n",
      "loss: 0.0344  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0352  acc: 1.0000\n",
      "loss: 0.0954  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0533  acc: 1.0000\n",
      "loss: 0.0149  acc: 1.0000\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0305  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0169  acc: 1.0000\n",
      "loss: 0.0356  acc: 1.0000\n",
      "loss: 0.0191  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0104  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0492  acc: 1.0000\n",
      "epoch 52, train loss: 0.0264, train acc: 1.000\n",
      "test loss: 4.2290, test acc: 0.362\n",
      "loss: 0.0178  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0208  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0205  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0282  acc: 1.0000\n",
      "loss: 0.0359  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0627  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0280  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0469  acc: 1.0000\n",
      "loss: 0.0159  acc: 1.0000\n",
      "loss: 2.9410  acc: 0.5000\n",
      "loss: 0.0490  acc: 1.0000\n",
      "loss: 0.1701  acc: 1.0000\n",
      "loss: 0.0545  acc: 1.0000\n",
      "loss: 0.1833  acc: 1.0000\n",
      "loss: 0.0707  acc: 1.0000\n",
      "loss: 0.0219  acc: 1.0000\n",
      "loss: 0.0347  acc: 1.0000\n",
      "loss: 0.1325  acc: 1.0000\n",
      "loss: 0.0587  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0552  acc: 1.0000\n",
      "loss: 0.0566  acc: 1.0000\n",
      "loss: 0.0529  acc: 1.0000\n",
      "loss: 0.0257  acc: 1.0000\n",
      "loss: 0.0862  acc: 1.0000\n",
      "loss: 0.0470  acc: 1.0000\n",
      "loss: 0.1358  acc: 1.0000\n",
      "loss: 0.0792  acc: 1.0000\n",
      "loss: 0.0366  acc: 1.0000\n",
      "loss: 0.0339  acc: 1.0000\n",
      "epoch 53, train loss: 0.1100, train acc: 0.988\n",
      "test loss: 4.2645, test acc: 0.338\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0303  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0320  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.1345  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0350  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0351  acc: 1.0000\n",
      "loss: 0.0084  acc: 1.0000\n",
      "loss: 0.0455  acc: 1.0000\n",
      "loss: 0.1033  acc: 1.0000\n",
      "loss: 0.0191  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0365  acc: 1.0000\n",
      "loss: 0.1776  acc: 1.0000\n",
      "loss: 0.0325  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0692  acc: 1.0000\n",
      "loss: 0.0450  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0258  acc: 1.0000\n",
      "loss: 0.0308  acc: 1.0000\n",
      "loss: 0.0334  acc: 1.0000\n",
      "loss: 0.0721  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.8550  acc: 1.0000\n",
      "loss: 0.0647  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0852  acc: 1.0000\n",
      "loss: 0.0354  acc: 1.0000\n",
      "loss: 0.0593  acc: 1.0000\n",
      "loss: 0.6786  acc: 0.5000\n",
      "loss: 0.0187  acc: 1.0000\n",
      "loss: 0.0378  acc: 1.0000\n",
      "loss: 0.0224  acc: 1.0000\n",
      "loss: 0.0354  acc: 1.0000\n",
      "loss: 0.2364  acc: 1.0000\n",
      "loss: 0.0359  acc: 1.0000\n",
      "loss: 0.0715  acc: 1.0000\n",
      "loss: 0.0216  acc: 1.0000\n",
      "loss: 0.3458  acc: 1.0000\n",
      "epoch 54, train loss: 0.0854, train acc: 0.988\n",
      "test loss: 4.6315, test acc: 0.300\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0197  acc: 1.0000\n",
      "loss: 0.0602  acc: 1.0000\n",
      "loss: 0.0983  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.1143  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0249  acc: 1.0000\n",
      "loss: 0.0928  acc: 1.0000\n",
      "loss: 0.0749  acc: 1.0000\n",
      "loss: 0.1417  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0361  acc: 1.0000\n",
      "loss: 0.0214  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0352  acc: 1.0000\n",
      "loss: 0.0511  acc: 1.0000\n",
      "loss: 0.0719  acc: 1.0000\n",
      "loss: 0.0403  acc: 1.0000\n",
      "loss: 0.0346  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0334  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0566  acc: 1.0000\n",
      "loss: 0.0321  acc: 1.0000\n",
      "loss: 0.0280  acc: 1.0000\n",
      "loss: 0.0442  acc: 1.0000\n",
      "loss: 0.0321  acc: 1.0000\n",
      "loss: 0.0210  acc: 1.0000\n",
      "loss: 0.0142  acc: 1.0000\n",
      "loss: 0.1306  acc: 1.0000\n",
      "loss: 0.0745  acc: 1.0000\n",
      "loss: 0.0292  acc: 1.0000\n",
      "loss: 0.0204  acc: 1.0000\n",
      "loss: 0.0607  acc: 1.0000\n",
      "loss: 0.0173  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.1337  acc: 1.0000\n",
      "loss: 0.0796  acc: 1.0000\n",
      "epoch 55, train loss: 0.0432, train acc: 1.000\n",
      "test loss: 4.4521, test acc: 0.325\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0267  acc: 1.0000\n",
      "loss: 0.0457  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0292  acc: 1.0000\n",
      "loss: 0.0221  acc: 1.0000\n",
      "loss: 0.0650  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0217  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0292  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0165  acc: 1.0000\n",
      "loss: 0.0209  acc: 1.0000\n",
      "loss: 0.0197  acc: 1.0000\n",
      "loss: 0.1156  acc: 1.0000\n",
      "loss: 0.0354  acc: 1.0000\n",
      "loss: 0.0543  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0167  acc: 1.0000\n",
      "loss: 0.0739  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0146  acc: 1.0000\n",
      "loss: 0.1112  acc: 1.0000\n",
      "loss: 0.0553  acc: 1.0000\n",
      "loss: 0.0682  acc: 1.0000\n",
      "loss: 0.0122  acc: 1.0000\n",
      "loss: 0.0795  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0547  acc: 1.0000\n",
      "loss: 0.0295  acc: 1.0000\n",
      "loss: 0.0431  acc: 1.0000\n",
      "loss: 0.0272  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "epoch 56, train loss: 0.0293, train acc: 1.000\n",
      "test loss: 4.4601, test acc: 0.325\n",
      "loss: 0.0206  acc: 1.0000\n",
      "loss: 0.0265  acc: 1.0000\n",
      "loss: 0.0233  acc: 1.0000\n",
      "loss: 0.0193  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0153  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0472  acc: 1.0000\n",
      "loss: 0.0146  acc: 1.0000\n",
      "loss: 0.0127  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0343  acc: 1.0000\n",
      "loss: 0.0458  acc: 1.0000\n",
      "loss: 0.0399  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0360  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0687  acc: 1.0000\n",
      "loss: 0.0611  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0122  acc: 1.0000\n",
      "loss: 0.0187  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0298  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0228  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0291  acc: 1.0000\n",
      "loss: 0.2289  acc: 1.0000\n",
      "loss: 0.0216  acc: 1.0000\n",
      "loss: 0.0359  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0180  acc: 1.0000\n",
      "epoch 57, train loss: 0.0255, train acc: 1.000\n",
      "test loss: 4.5093, test acc: 0.350\n",
      "loss: 0.1421  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0876  acc: 1.0000\n",
      "loss: 0.0178  acc: 1.0000\n",
      "loss: 0.0317  acc: 1.0000\n",
      "loss: 0.0410  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0847  acc: 1.0000\n",
      "loss: 0.0395  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0251  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0273  acc: 1.0000\n",
      "loss: 0.0675  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0194  acc: 1.0000\n",
      "loss: 0.0608  acc: 1.0000\n",
      "loss: 0.0289  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0457  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0608  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0163  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0184  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0283  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0380  acc: 1.0000\n",
      "epoch 58, train loss: 0.0255, train acc: 1.000\n",
      "test loss: 4.4918, test acc: 0.338\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0479  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0291  acc: 1.0000\n",
      "loss: 0.0054  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0234  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0510  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0176  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0311  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0328  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.0667  acc: 1.0000\n",
      "loss: 0.0698  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0575  acc: 1.0000\n",
      "loss: 0.0667  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0085  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "epoch 59, train loss: 0.0184, train acc: 1.000\n",
      "test loss: 4.5632, test acc: 0.350\n",
      "loss: 0.0275  acc: 1.0000\n",
      "loss: 0.0547  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0436  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0228  acc: 1.0000\n",
      "loss: 0.0156  acc: 1.0000\n",
      "loss: 0.0193  acc: 1.0000\n",
      "loss: 2.9105  acc: 0.5000\n",
      "loss: 0.0303  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0533  acc: 1.0000\n",
      "loss: 0.0415  acc: 1.0000\n",
      "loss: 0.0222  acc: 1.0000\n",
      "loss: 0.0426  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.0349  acc: 1.0000\n",
      "loss: 0.0829  acc: 1.0000\n",
      "loss: 0.1315  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0534  acc: 1.0000\n",
      "loss: 0.0454  acc: 1.0000\n",
      "loss: 0.0230  acc: 1.0000\n",
      "loss: 0.0747  acc: 1.0000\n",
      "loss: 0.3232  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0191  acc: 1.0000\n",
      "loss: 0.0234  acc: 1.0000\n",
      "loss: 0.0173  acc: 1.0000\n",
      "loss: 0.0325  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.1020  acc: 1.0000\n",
      "loss: 0.0620  acc: 1.0000\n",
      "loss: 0.0167  acc: 1.0000\n",
      "epoch 60, train loss: 0.1050, train acc: 0.988\n",
      "test loss: 4.3028, test acc: 0.338\n",
      "loss: 0.0655  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0422  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0175  acc: 1.0000\n",
      "loss: 0.0319  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0496  acc: 1.0000\n",
      "loss: 0.0427  acc: 1.0000\n",
      "loss: 0.0969  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0234  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.1093  acc: 1.0000\n",
      "loss: 0.1046  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0261  acc: 1.0000\n",
      "loss: 0.0211  acc: 1.0000\n",
      "loss: 0.0573  acc: 1.0000\n",
      "loss: 0.0501  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0948  acc: 1.0000\n",
      "loss: 0.0328  acc: 1.0000\n",
      "loss: 0.0313  acc: 1.0000\n",
      "loss: 0.0531  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0218  acc: 1.0000\n",
      "loss: 0.0502  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0595  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0829  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0172  acc: 1.0000\n",
      "loss: 0.0606  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "epoch 61, train loss: 0.0335, train acc: 1.000\n",
      "test loss: 4.2576, test acc: 0.362\n",
      "loss: 0.0206  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0209  acc: 1.0000\n",
      "loss: 0.0422  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0176  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0282  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.0239  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0461  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0368  acc: 1.0000\n",
      "loss: 0.0155  acc: 1.0000\n",
      "loss: 0.0084  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0382  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0153  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0204  acc: 1.0000\n",
      "loss: 0.0219  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0804  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0165  acc: 1.0000\n",
      "loss: 0.0293  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.1055  acc: 1.0000\n",
      "epoch 62, train loss: 0.0200, train acc: 1.000\n",
      "test loss: 4.2818, test acc: 0.362\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0372  acc: 1.0000\n",
      "loss: 0.0408  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0420  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0403  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0322  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0274  acc: 1.0000\n",
      "loss: 0.0223  acc: 1.0000\n",
      "loss: 0.0458  acc: 1.0000\n",
      "loss: 0.0188  acc: 1.0000\n",
      "loss: 0.0244  acc: 1.0000\n",
      "loss: 0.0111  acc: 1.0000\n",
      "loss: 0.0084  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0245  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0223  acc: 1.0000\n",
      "epoch 63, train loss: 0.0147, train acc: 1.000\n",
      "test loss: 4.2898, test acc: 0.375\n",
      "loss: 0.0660  acc: 1.0000\n",
      "loss: 0.0853  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0500  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0304  acc: 1.0000\n",
      "loss: 0.0203  acc: 1.0000\n",
      "loss: 0.0472  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0104  acc: 1.0000\n",
      "loss: 0.0469  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0135  acc: 1.0000\n",
      "loss: 0.0181  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0303  acc: 1.0000\n",
      "loss: 0.0823  acc: 1.0000\n",
      "loss: 0.0169  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0318  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0127  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0440  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0343  acc: 1.0000\n",
      "loss: 0.0348  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0364  acc: 1.0000\n",
      "loss: 0.0575  acc: 1.0000\n",
      "loss: 0.0156  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0471  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "epoch 64, train loss: 0.0238, train acc: 1.000\n",
      "test loss: 4.3090, test acc: 0.325\n",
      "loss: 0.0375  acc: 1.0000\n",
      "loss: 0.0218  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0269  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0383  acc: 1.0000\n",
      "loss: 0.0014  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0153  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0337  acc: 1.0000\n",
      "loss: 0.0307  acc: 1.0000\n",
      "loss: 0.0166  acc: 1.0000\n",
      "loss: 0.0173  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0254  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0231  acc: 1.0000\n",
      "loss: 0.0638  acc: 1.0000\n",
      "loss: 0.0622  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0054  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0358  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0650  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0709  acc: 1.0000\n",
      "epoch 65, train loss: 0.0195, train acc: 1.000\n",
      "test loss: 4.3743, test acc: 0.350\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0331  acc: 1.0000\n",
      "loss: 0.1903  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0222  acc: 1.0000\n",
      "loss: 0.0146  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0259  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0356  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0161  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0222  acc: 1.0000\n",
      "loss: 0.0143  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0279  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0222  acc: 1.0000\n",
      "loss: 0.0201  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0449  acc: 1.0000\n",
      "loss: 0.0319  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0495  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "epoch 66, train loss: 0.0194, train acc: 1.000\n",
      "test loss: 4.4170, test acc: 0.350\n",
      "loss: 0.0223  acc: 1.0000\n",
      "loss: 0.0164  acc: 1.0000\n",
      "loss: 0.0122  acc: 1.0000\n",
      "loss: 0.0259  acc: 1.0000\n",
      "loss: 0.0792  acc: 1.0000\n",
      "loss: 0.0334  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0333  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0251  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0607  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0599  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0165  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0173  acc: 1.0000\n",
      "loss: 0.0122  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0196  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0336  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0378  acc: 1.0000\n",
      "loss: 0.0281  acc: 1.0000\n",
      "loss: 0.0181  acc: 1.0000\n",
      "loss: 0.0265  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "epoch 67, train loss: 0.0183, train acc: 1.000\n",
      "test loss: 4.3829, test acc: 0.325\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0374  acc: 1.0000\n",
      "loss: 0.0363  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0085  acc: 1.0000\n",
      "loss: 0.0329  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0328  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0435  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0271  acc: 1.0000\n",
      "loss: 0.0173  acc: 1.0000\n",
      "loss: 0.0353  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.1194  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0172  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0181  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0391  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0085  acc: 1.0000\n",
      "epoch 68, train loss: 0.0165, train acc: 1.000\n",
      "test loss: 4.2921, test acc: 0.350\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0291  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0311  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.2474  acc: 1.0000\n",
      "loss: 0.0299  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0775  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0198  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0572  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0250  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0483  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0191  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "epoch 69, train loss: 0.0208, train acc: 1.000\n",
      "test loss: 4.3653, test acc: 0.338\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0258  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.4148  acc: 1.0000\n",
      "loss: 0.0420  acc: 1.0000\n",
      "loss: 0.0239  acc: 1.0000\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0302  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0284  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0135  acc: 1.0000\n",
      "loss: 0.0142  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "loss: 0.0269  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.0277  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0181  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0667  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0395  acc: 1.0000\n",
      "loss: 0.0501  acc: 1.0000\n",
      "loss: 0.0308  acc: 1.0000\n",
      "loss: 0.0530  acc: 1.0000\n",
      "loss: 0.0504  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0193  acc: 1.0000\n",
      "loss: 0.0373  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0161  acc: 1.0000\n",
      "loss: 0.0201  acc: 1.0000\n",
      "epoch 70, train loss: 0.0295, train acc: 1.000\n",
      "test loss: 4.4559, test acc: 0.287\n",
      "loss: 0.0241  acc: 1.0000\n",
      "loss: 0.0205  acc: 1.0000\n",
      "loss: 0.0349  acc: 1.0000\n",
      "loss: 0.0011  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0326  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0343  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0275  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0378  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0501  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0267  acc: 1.0000\n",
      "loss: 0.0243  acc: 1.0000\n",
      "loss: 0.0382  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0394  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0120  acc: 1.0000\n",
      "loss: 0.0214  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0288  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0385  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "epoch 71, train loss: 0.0166, train acc: 1.000\n",
      "test loss: 4.3250, test acc: 0.325\n",
      "loss: 0.0763  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0583  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0168  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0175  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0255  acc: 1.0000\n",
      "loss: 0.0255  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0216  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0104  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0298  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0542  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0014  acc: 1.0000\n",
      "loss: 0.0439  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0332  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0424  acc: 1.0000\n",
      "loss: 0.0229  acc: 1.0000\n",
      "loss: 0.0836  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0214  acc: 1.0000\n",
      "epoch 72, train loss: 0.0193, train acc: 1.000\n",
      "test loss: 4.3113, test acc: 0.350\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.1026  acc: 1.0000\n",
      "loss: 0.0111  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0245  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0296  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0205  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0342  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0319  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0186  acc: 1.0000\n",
      "loss: 0.0208  acc: 1.0000\n",
      "epoch 73, train loss: 0.0135, train acc: 1.000\n",
      "test loss: 4.2656, test acc: 0.350\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0011  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0231  acc: 1.0000\n",
      "loss: 0.0365  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0356  acc: 1.0000\n",
      "loss: 0.0164  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0224  acc: 1.0000\n",
      "loss: 0.0380  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0176  acc: 1.0000\n",
      "loss: 0.0194  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0419  acc: 1.0000\n",
      "loss: 0.0263  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0461  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "epoch 74, train loss: 0.0151, train acc: 1.000\n",
      "test loss: 4.3531, test acc: 0.350\n",
      "loss: 0.0172  acc: 1.0000\n",
      "loss: 0.0127  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0210  acc: 1.0000\n",
      "loss: 0.0169  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0230  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0452  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0054  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.1434  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0212  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0250  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0230  acc: 1.0000\n",
      "loss: 0.0136  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "epoch 75, train loss: 0.0145, train acc: 1.000\n",
      "test loss: 4.3341, test acc: 0.325\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0395  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0211  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0463  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0186  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0168  acc: 1.0000\n",
      "loss: 0.0157  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "epoch 76, train loss: 0.0103, train acc: 1.000\n",
      "test loss: 4.3385, test acc: 0.350\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0013  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0079  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.1242  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.3760  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0549  acc: 1.0000\n",
      "loss: 0.0163  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0320  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0274  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0478  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0190  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0863  acc: 1.0000\n",
      "epoch 77, train loss: 0.0239, train acc: 1.000\n",
      "test loss: 4.2677, test acc: 0.388\n",
      "loss: 0.0120  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0257  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0178  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0127  acc: 1.0000\n",
      "loss: 0.0169  acc: 1.0000\n",
      "loss: 0.0276  acc: 1.0000\n",
      "loss: 0.0290  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0143  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0290  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0313  acc: 1.0000\n",
      "loss: 0.0212  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.5784  acc: 1.0000\n",
      "loss: 0.0307  acc: 1.0000\n",
      "loss: 0.0697  acc: 1.0000\n",
      "loss: 0.1091  acc: 1.0000\n",
      "loss: 0.0259  acc: 1.0000\n",
      "loss: 0.0341  acc: 1.0000\n",
      "loss: 0.0168  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0271  acc: 1.0000\n",
      "loss: 0.0085  acc: 1.0000\n",
      "loss: 0.0257  acc: 1.0000\n",
      "loss: 0.0379  acc: 1.0000\n",
      "loss: 0.0341  acc: 1.0000\n",
      "epoch 78, train loss: 0.0326, train acc: 1.000\n",
      "test loss: 4.5094, test acc: 0.338\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0394  acc: 1.0000\n",
      "loss: 0.0345  acc: 1.0000\n",
      "loss: 0.0307  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0206  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0297  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0378  acc: 1.0000\n",
      "loss: 0.0288  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0165  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0230  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0290  acc: 1.0000\n",
      "loss: 0.0185  acc: 1.0000\n",
      "loss: 0.0429  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0213  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0149  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "epoch 79, train loss: 0.0145, train acc: 1.000\n",
      "test loss: 4.4463, test acc: 0.362\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0154  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0364  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0258  acc: 1.0000\n",
      "loss: 0.0243  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0143  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0236  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0602  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0267  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0054  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0155  acc: 1.0000\n",
      "loss: 0.0192  acc: 1.0000\n",
      "loss: 0.0111  acc: 1.0000\n",
      "loss: 0.0153  acc: 1.0000\n",
      "loss: 0.0351  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0174  acc: 1.0000\n",
      "epoch 80, train loss: 0.0129, train acc: 1.000\n",
      "test loss: 4.2944, test acc: 0.362\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0246  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0167  acc: 1.0000\n",
      "loss: 0.0151  acc: 1.0000\n",
      "loss: 0.0167  acc: 1.0000\n",
      "loss: 0.0196  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0639  acc: 1.0000\n",
      "loss: 0.0168  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0250  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0428  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0282  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "epoch 81, train loss: 0.0121, train acc: 1.000\n",
      "test loss: 4.1731, test acc: 0.412\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0286  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0170  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0054  acc: 1.0000\n",
      "loss: 0.0111  acc: 1.0000\n",
      "loss: 0.0210  acc: 1.0000\n",
      "loss: 0.0014  acc: 1.0000\n",
      "loss: 0.0248  acc: 1.0000\n",
      "loss: 0.0156  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0149  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0460  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0156  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0122  acc: 1.0000\n",
      "epoch 82, train loss: 0.0108, train acc: 1.000\n",
      "test loss: 4.3347, test acc: 0.338\n",
      "loss: 0.0577  acc: 1.0000\n",
      "loss: 0.0341  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0161  acc: 1.0000\n",
      "loss: 0.0132  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0120  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0154  acc: 1.0000\n",
      "loss: 0.0282  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0163  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0389  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0740  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0354  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0401  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0008  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0236  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "epoch 83, train loss: 0.0140, train acc: 1.000\n",
      "test loss: 4.1987, test acc: 0.362\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0153  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0120  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0331  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0245  acc: 1.0000\n",
      "loss: 0.0188  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0360  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0197  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0176  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "epoch 84, train loss: 0.0098, train acc: 1.000\n",
      "test loss: 4.2340, test acc: 0.362\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0091  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0311  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0233  acc: 1.0000\n",
      "loss: 0.0185  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0354  acc: 1.0000\n",
      "loss: 0.0297  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0239  acc: 1.0000\n",
      "loss: 1.2934  acc: 1.0000\n",
      "loss: 0.0880  acc: 1.0000\n",
      "loss: 0.0156  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.1108  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.0374  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "epoch 85, train loss: 0.0450, train acc: 1.000\n",
      "test loss: 4.5978, test acc: 0.362\n",
      "loss: 0.0315  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0268  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0194  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0104  acc: 1.0000\n",
      "loss: 0.0005  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0428  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0566  acc: 1.0000\n",
      "loss: 0.0155  acc: 1.0000\n",
      "loss: 0.0084  acc: 1.0000\n",
      "loss: 0.0363  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0162  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0221  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0288  acc: 1.0000\n",
      "loss: 0.0261  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "epoch 86, train loss: 0.0125, train acc: 1.000\n",
      "test loss: 4.3941, test acc: 0.350\n",
      "loss: 0.0249  acc: 1.0000\n",
      "loss: 0.0203  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0128  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0202  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0327  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0013  acc: 1.0000\n",
      "loss: 0.0216  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0440  acc: 1.0000\n",
      "loss: 0.0229  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0188  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0007  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0684  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0160  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0405  acc: 1.0000\n",
      "loss: 0.0013  acc: 1.0000\n",
      "loss: 0.0212  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0278  acc: 1.0000\n",
      "loss: 0.0251  acc: 1.0000\n",
      "loss: 0.0389  acc: 1.0000\n",
      "epoch 87, train loss: 0.0151, train acc: 1.000\n",
      "test loss: 4.4055, test acc: 0.375\n",
      "loss: 0.0161  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0540  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0074  acc: 1.0000\n",
      "loss: 0.0306  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0188  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0169  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0158  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0133  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0031  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0011  acc: 1.0000\n",
      "epoch 88, train loss: 0.0095, train acc: 1.000\n",
      "test loss: 4.3326, test acc: 0.350\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0004  acc: 1.0000\n",
      "loss: 0.0374  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0138  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0109  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0198  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "epoch 89, train loss: 0.0074, train acc: 1.000\n",
      "test loss: 4.2897, test acc: 0.350\n",
      "loss: 0.0099  acc: 1.0000\n",
      "loss: 0.0239  acc: 1.0000\n",
      "loss: 0.0053  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0008  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0274  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0164  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0196  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0096  acc: 1.0000\n",
      "loss: 0.0145  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0155  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0137  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0080  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0031  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0229  acc: 1.0000\n",
      "epoch 90, train loss: 0.0096, train acc: 1.000\n",
      "test loss: 4.4522, test acc: 0.362\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0031  acc: 1.0000\n",
      "loss: 0.0245  acc: 1.0000\n",
      "loss: 0.0013  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0217  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0231  acc: 1.0000\n",
      "loss: 0.0211  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0193  acc: 1.0000\n",
      "loss: 0.0443  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0031  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0277  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0175  acc: 1.0000\n",
      "loss: 0.0194  acc: 1.0000\n",
      "epoch 91, train loss: 0.0102, train acc: 1.000\n",
      "test loss: 4.4669, test acc: 0.362\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0217  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0260  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0266  acc: 1.0000\n",
      "loss: 0.0005  acc: 1.0000\n",
      "loss: 0.0232  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0117  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0224  acc: 1.0000\n",
      "loss: 0.0141  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0130  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0237  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "epoch 92, train loss: 0.0086, train acc: 1.000\n",
      "test loss: 4.2755, test acc: 0.388\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0106  acc: 1.0000\n",
      "loss: 1.5688  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0147  acc: 1.0000\n",
      "loss: 0.2191  acc: 1.0000\n",
      "loss: 0.0166  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0275  acc: 1.0000\n",
      "loss: 0.0979  acc: 1.0000\n",
      "loss: 0.0471  acc: 1.0000\n",
      "loss: 0.0398  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0194  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0308  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0129  acc: 1.0000\n",
      "loss: 0.1672  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0247  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0114  acc: 1.0000\n",
      "loss: 0.0177  acc: 1.0000\n",
      "loss: 0.0273  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0199  acc: 1.0000\n",
      "loss: 0.0292  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0143  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0234  acc: 1.0000\n",
      "loss: 0.0159  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "epoch 93, train loss: 0.0602, train acc: 1.000\n",
      "test loss: 4.8244, test acc: 0.325\n",
      "loss: 0.0497  acc: 1.0000\n",
      "loss: 0.0069  acc: 1.0000\n",
      "loss: 0.0155  acc: 1.0000\n",
      "loss: 0.0179  acc: 1.0000\n",
      "loss: 0.0108  acc: 1.0000\n",
      "loss: 0.0152  acc: 1.0000\n",
      "loss: 0.0627  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0021  acc: 1.0000\n",
      "loss: 0.0214  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0307  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0344  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0101  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0102  acc: 1.0000\n",
      "loss: 0.0143  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0220  acc: 1.0000\n",
      "loss: 0.0144  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0041  acc: 1.0000\n",
      "epoch 94, train loss: 0.0119, train acc: 1.000\n",
      "test loss: 4.5599, test acc: 0.362\n",
      "loss: 0.0005  acc: 1.0000\n",
      "loss: 0.0300  acc: 1.0000\n",
      "loss: 0.0084  acc: 1.0000\n",
      "loss: 0.0163  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0224  acc: 1.0000\n",
      "loss: 0.0095  acc: 1.0000\n",
      "loss: 0.0110  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0422  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0216  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0487  acc: 1.0000\n",
      "loss: 0.0148  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0086  acc: 1.0000\n",
      "loss: 0.0073  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0111  acc: 1.0000\n",
      "loss: 0.0011  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0058  acc: 1.0000\n",
      "loss: 0.0005  acc: 1.0000\n",
      "loss: 0.0040  acc: 1.0000\n",
      "loss: 0.0424  acc: 1.0000\n",
      "loss: 0.0008  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0219  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "epoch 95, train loss: 0.0108, train acc: 1.000\n",
      "test loss: 4.5753, test acc: 0.350\n",
      "loss: 0.0075  acc: 1.0000\n",
      "loss: 0.0107  acc: 1.0000\n",
      "loss: 0.0103  acc: 1.0000\n",
      "loss: 0.0174  acc: 1.0000\n",
      "loss: 0.0503  acc: 1.0000\n",
      "loss: 0.0252  acc: 1.0000\n",
      "loss: 0.0014  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0258  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0116  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0142  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0051  acc: 1.0000\n",
      "loss: 0.0134  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0046  acc: 1.0000\n",
      "loss: 0.0034  acc: 1.0000\n",
      "loss: 0.0204  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0120  acc: 1.0000\n",
      "loss: 0.0183  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0139  acc: 1.0000\n",
      "loss: 0.0038  acc: 1.0000\n",
      "loss: 0.0219  acc: 1.0000\n",
      "loss: 0.0413  acc: 1.0000\n",
      "epoch 96, train loss: 0.0103, train acc: 1.000\n",
      "test loss: 4.4774, test acc: 0.350\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0195  acc: 1.0000\n",
      "loss: 0.0098  acc: 1.0000\n",
      "loss: 0.0210  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0044  acc: 1.0000\n",
      "loss: 0.0064  acc: 1.0000\n",
      "loss: 0.0029  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0280  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0090  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0238  acc: 1.0000\n",
      "loss: 0.0238  acc: 1.0000\n",
      "loss: 0.0706  acc: 1.0000\n",
      "loss: 0.0009  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0055  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0131  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0226  acc: 1.0000\n",
      "loss: 0.0060  acc: 1.0000\n",
      "loss: 0.0123  acc: 1.0000\n",
      "loss: 0.0344  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0104  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0035  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0163  acc: 1.0000\n",
      "epoch 97, train loss: 0.0113, train acc: 1.000\n",
      "test loss: 4.5071, test acc: 0.362\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0083  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0082  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0007  acc: 1.0000\n",
      "loss: 0.0105  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0100  acc: 1.0000\n",
      "loss: 0.0065  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0076  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0207  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0052  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0049  acc: 1.0000\n",
      "loss: 0.0059  acc: 1.0000\n",
      "loss: 0.0314  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0018  acc: 1.0000\n",
      "loss: 0.0068  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0257  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0146  acc: 1.0000\n",
      "loss: 0.0056  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0024  acc: 1.0000\n",
      "loss: 0.0113  acc: 1.0000\n",
      "loss: 0.0171  acc: 1.0000\n",
      "loss: 0.0067  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0032  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "epoch 98, train loss: 0.0077, train acc: 1.000\n",
      "test loss: 4.4651, test acc: 0.362\n",
      "loss: 0.0050  acc: 1.0000\n",
      "loss: 0.0033  acc: 1.0000\n",
      "loss: 0.0094  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0115  acc: 1.0000\n",
      "loss: 0.0066  acc: 1.0000\n",
      "loss: 0.0837  acc: 1.0000\n",
      "loss: 0.0363  acc: 1.0000\n",
      "loss: 0.0112  acc: 1.0000\n",
      "loss: 0.0198  acc: 1.0000\n",
      "loss: 0.0077  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0020  acc: 1.0000\n",
      "loss: 0.0126  acc: 1.0000\n",
      "loss: 0.0119  acc: 1.0000\n",
      "loss: 0.0189  acc: 1.0000\n",
      "loss: 0.0270  acc: 1.0000\n",
      "loss: 0.0071  acc: 1.0000\n",
      "loss: 0.0124  acc: 1.0000\n",
      "loss: 0.0182  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0015  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0057  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0025  acc: 1.0000\n",
      "loss: 0.0048  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0293  acc: 1.0000\n",
      "loss: 0.0045  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0006  acc: 1.0000\n",
      "loss: 0.0031  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0019  acc: 1.0000\n",
      "loss: 0.0072  acc: 1.0000\n",
      "loss: 0.0023  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0061  acc: 1.0000\n",
      "loss: 0.0008  acc: 1.0000\n",
      "loss: 0.0089  acc: 1.0000\n",
      "loss: 0.0192  acc: 1.0000\n",
      "epoch 99, train loss: 0.0107, train acc: 1.000\n",
      "test loss: 4.3804, test acc: 0.350\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0093  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0087  acc: 1.0000\n",
      "loss: 0.0097  acc: 1.0000\n",
      "loss: 0.0030  acc: 1.0000\n",
      "loss: 0.0250  acc: 1.0000\n",
      "loss: 0.0088  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0047  acc: 1.0000\n",
      "loss: 0.0036  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0063  acc: 1.0000\n",
      "loss: 0.0165  acc: 1.0000\n",
      "loss: 0.0028  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0027  acc: 1.0000\n",
      "loss: 0.0062  acc: 1.0000\n",
      "loss: 0.0043  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0014  acc: 1.0000\n",
      "loss: 0.0070  acc: 1.0000\n",
      "loss: 0.0039  acc: 1.0000\n",
      "loss: 0.0279  acc: 1.0000\n",
      "loss: 0.0026  acc: 1.0000\n",
      "loss: 0.0125  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0022  acc: 1.0000\n",
      "loss: 0.0037  acc: 1.0000\n",
      "loss: 0.0012  acc: 1.0000\n",
      "loss: 0.0081  acc: 1.0000\n",
      "loss: 0.0016  acc: 1.0000\n",
      "loss: 0.0227  acc: 1.0000\n",
      "loss: 0.0121  acc: 1.0000\n",
      "loss: 0.0010  acc: 1.0000\n",
      "loss: 0.0118  acc: 1.0000\n",
      "loss: 0.0140  acc: 1.0000\n",
      "loss: 0.0078  acc: 1.0000\n",
      "loss: 0.0017  acc: 1.0000\n",
      "loss: 0.0092  acc: 1.0000\n",
      "loss: 0.0042  acc: 1.0000\n",
      "epoch 100, train loss: 0.0069, train acc: 1.000\n",
      "test loss: 4.3840, test acc: 0.350\n"
     ]
    }
   ],
   "source": [
    "#残差网络块\n",
    "#每个残差块都是两层\n",
    "#默认3*3卷积下padding为1，则大小不会变化，如变化则是步长引起的。\n",
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, nin, nout, size, stride=1, shortcut=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        #两层卷积层\n",
    "        #不同步长只有第一层卷积层不同\n",
    "        self.block1 = torch.nn.Sequential(torch.nn.Conv2d(nin, nout, size, stride, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(nout),\n",
    "                                          torch.nn.ReLU(inplace=True),\n",
    "                                          torch.nn.Conv2d(nout, nout, size, 1, padding=1),\n",
    "                                          torch.nn.BatchNorm2d(nout))\n",
    "        self.shortcut = shortcut\n",
    "        #解决通道数变化以及步长不为1引起的图片大小的变化\n",
    "        self.block2 = torch.nn.Sequential(torch.nn.Conv2d(nin, nout, size, stride, 1),\n",
    "                                          torch.nn.BatchNorm2d(nout))\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        out = self.block1(x)\n",
    "        '''''若输入输出维度相等直接相加，不相等改变输入的维度--包括大小和通道'''\n",
    "        if self.shortcut:\n",
    "            out = x + out\n",
    "        else:\n",
    "            out = out + self.block2(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "#定义给定的残差结构\n",
    "class resnet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(resnet, self).__init__()\n",
    "        self.block = torch.nn.Sequential(torch.nn.Conv2d(3, 64, 3, stride=1, padding=1),\n",
    "                                         torch.nn.BatchNorm2d(64),\n",
    "                                         torch.nn.ReLU())\n",
    "        #t表示2个相同的残差块,每个残差块两个卷积\n",
    "        self.d1 = self.make_layer(64, 64, 3, stride=1, t=2)\n",
    "        self.d2 = self.make_layer(64, 128, 3, stride=2, t=2)\n",
    "        self.d3 = self.make_layer(128, 256, 3, stride=2, t=2)\n",
    "        self.d4 = self.make_layer(256, 512, 3, stride=2, t=2)\n",
    "\n",
    "        self.avgp = torch.nn.AvgPool2d(8)\n",
    "        self.exit = torch.nn.Linear(512, 80)\n",
    "\n",
    "    def make_layer(self, in1, out1, ksize, stride, t):\n",
    "        layers = []\n",
    "        for i in range(0, t):\n",
    "            if i == 0 and in1 != out1:\n",
    "                layers.append(ResidualBlock(in1, out1, ksize, stride, None))\n",
    "            else:\n",
    "                layers.append(ResidualBlock(out1, out1, ksize, 1, True))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.block(input)  # 输出维度 64 * 64 * 64    C * H * W\n",
    "        x = self.d1(x)  # 输出维度 64 * 54 * 54\n",
    "        x = self.d2(x)  # i=0 步长为2，输出维度128 * 32 * 32\n",
    "        x = self.d3(x)  # i=0 步长为2，输出维度256 * 16 * 16\n",
    "        x = self.d4(x)  # i=0 步长为2，输出维度512 * 8 * 8\n",
    "        x = self.avgp(x)  # 512 * 1 * 1\n",
    "        #将张量out从shape batchx512x1x1 变为 batch x512\n",
    "        x = x.squeeze()\n",
    "        output = self.exit(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.current_device()\n",
    "import torchvision\n",
    "\n",
    "net = resnet()\n",
    "net.to(device)\n",
    "# 损失函数和优化器\n",
    "loss = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# 模型训练与测试\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # 下面老是报错 shape 不一致\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 在训练集上训练\n",
    "    train_loss, train_acc = train(net, data_loader=train_iter, device=device)\n",
    "    # 测试集上验证\n",
    "    test_loss, test_acc = test(net, data_loader=test_iter, device=device)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "    print('epoch %d, train loss: %.4f, train acc: %.3f' % (epoch + 1, train_loss, train_acc))\n",
    "    print('test loss: %.4f, test acc: %.3f' % (test_loss, test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzPklEQVR4nO3dd3xUVfrH8c+kJ6QRUmiB0HuTJiAiiiIiu2ABsQF2hV2VtRewo666WFDXAqz+REQRRUFcpLkUqdJ7DS0JNZ2Umfv742YmGVJImWSS4ft+veaVmXvv3HlyCblPznnOORbDMAxEREREPISXuwMQERERcSUlNyIiIuJRlNyIiIiIR1FyIyIiIh5FyY2IiIh4FCU3IiIi4lGU3IiIiIhH8XF3AFXNZrNx7NgxQkJCsFgs7g5HRERESsEwDFJTU6lfvz5eXiW3zVx0yc2xY8eIjY11dxgiIiJSDocPH6Zhw4YlHnPRJTchISGAeXFCQ0PdHI2IiIiURkpKCrGxsY77eEkuuuTG3hUVGhqq5EZERKSGKU1JiQqKRURExKMouRERERGPouRGREREPIqSGxEREfEoSm5ERETEoyi5EREREY+i5EZEREQ8ipIbERER8ShKbkRERMSjKLkRERERj+LW5Ob3339nyJAh1K9fH4vFwg8//HDB9yxdupRLLrkEf39/mjdvzvTp0ys9ThEREak53JrcpKen06lTJ6ZMmVKq4w8cOMDgwYPp378/Gzdu5JFHHuGee+7h119/reRIRUREpKZw68KZgwYNYtCgQaU+/uOPP6ZJkya8/fbbALRp04bly5fzr3/9i4EDB1ZWmCLVTsq5HFIyc0o8xstioV5YQKkWmSurU2lZZOZYXX7eyhYZ7E+Ar3eZ3pOWlcvZjGyXfH5MaAC+3sX/TWm1GRxPznTJZ4m4k5+PF9EhAW77/Bq1KviqVasYMGCA07aBAwfyyCOPFPuerKwssrKyHK9TUlIqKzyRKjF1+QEm/bKDHKtxwWM7xYbz6R1diQ513S+Z2euP8I9vN7nsfFUpLNCXD27tQt8WUaU6fs6fR3j6+y2cy7G55PPrhQXw6Z3daN8grNC+/SfSuOeLdew/ke6SzxJxp0sahfP9Q33c9vk1KrlJSEggJibGaVtMTAwpKSlkZmYSGBhY6D2TJk3ixRdfrKoQRSqN1Wbw8s/bmb7yIGD+ZVRSm0yO1camw2cZ9uFKpo3pTsuYkArHkJyZw6vzd5if7+1FJTQKVRqbYZCcmcOYaWt5dVh7RnRvVOyxhmHw/uK9vLNwN+Ca79VslTnH8H+v4v2RXbiqTf7vsrUHT3PvF+s4m5GDt5cFH68adGFFilBSC2VVqFHJTXk8/fTTjB8/3vE6JSWF2NhYN0YkUnYZ2bk8PHMjC7cnAvD0oNbcd3nTErucDp1KZ8y0tew/mc6NH63k37d3pXfzyArF8f6iPZxOz6Z5dDC/PNzX7b/AyiIr18pTs7cw58+jPDl7C4dPZ/KPa1oWuobZuTaembOF79YfAeD+y5vy5LWt8apgwpFyLoexX23gf3tOcu8X63jxL+24o1ccP206xj9mbSLbaqNTbDif3dmNqBD/Cn2WyMWuRiU3devWJTEx0WlbYmIioaGhRbbaAPj7++Pvr18UUnOdSM3inv+sZdORZPx8vPjX8M4M7ljvgu9rXKcWsx/szX1frmPtwTPcOXUNr9/YkZu6NixXHPtOpDlajZ6/vm2NSmwA/H28eWd4J2JrB/Le4r18sGQv8aczGNu/uaNVJtdq8Or87azYewovC7z41/bccWljl3x+aIAvU0d359k5W5i17gjP/7iN/25P5H97TgJwTdsY3r2lC4F+ZasJEpHCalRy06tXL+bPn++0beHChfTq1ctNEYlUruxcG7d99ge7E9OoHeTLZ6O60bVxRKnfX7uWH1/e3ZPHv9vMT5uO8di3mzh8OoNHBrQoc6Hxq/N2kGszuLJ1NP1alq5mpbqxWCyMv6YVDSOCeOb7LczddIy5m44VOi7Iz5spt15C/9bRLv18X28v3rixI40ignjrv7sdic1dfZrw7OA2eKs7SsQl3JrcpKWlsXfvXsfrAwcOsHHjRiIiImjUqBFPP/00R48e5YsvvgDggQce4IMPPuCJJ57grrvuYvHixcyaNYt58+a561sQqVRfrDrI7sQ06tTyY/aDvYmLrFXmcwT4evPuiM7E1g7kw6X7eHfRHg6fzuD1Gzvi51O61pelu5JYvDMJHy8Lzw1uU+YYqpvh3WKpHxbICz9t43S680io+uEBvH5DxyKLfl3BYrEw7soWxEYE8eGSfdx+aSPu6BVXKZ8lcrGyGIZx4SEXlWTp0qX079+/0PZRo0Yxffp0Ro8ezcGDB1m6dKnTex599FG2b99Ow4YNef755xk9enSpPzMlJYWwsDCSk5MJDQ11wXchUjlOpWVxxVtLST2Xy+s3dOCWHsUXwJbW12viee6HrVhtBr2a1uHj27sSFuRb4ntyrDaunfw7+06kc89lTXju+rYVjkNEpKzKcv92a3LjDkpupKZ4Zs4WZqyOp229UH7622Uu67JYtvsED/3fetKzrTSPDmba6O7ERgQVe/y0FQd48aft1Knlx+LHriAssORkSESkMpTl/l2zKgJFLhI7jqcwc008ABOHtHVpLUa/llF8+0Bv6oYGsDcpjWEfrmTzkbNFHrt8z0ne+nUXAP+4ppUSGxGpEZTciFQzhmHw0k/bsRkwuGM9ejat4/LPaFs/lDlje9OmXign07IY8e8/HMPM7WatPczoaWtIz7bSp3kdRnTXFAoiUjMouRGpZn7dlsCq/afw9/Hi6UGtK+1z6oUFMuv+S7m8ZRSZOVbu+3Id01ccwDAM3v7vLp6YvZlcm8HQzvWZOrq7RvKISI2hmhuRauRcjpWr/7WMw6cz+fuVzRl/TatK/8wcq40JP27l6zWHAWhdN4SdCakA/O3K5oy/uvBEdyIiVU01NyI11OfLD3D4dCZ1QwN44IpmVfKZvt5evDasA09ea7YS7UxIxdvLwhs3duAf17RSYiMiNU6NmsRPxJMlppxjyhJz3qcnB7UiyK/q/ntaLBYevKIZjesEMWN1PPf3a1rqxSVFRKobJTci1cSbC3aRkW2lS6NwhnZu4JYYrutQj+s6XHhpBxGR6kzdUiLVwKbDZ5m9wVyoceKQduoKEhGpACU3Im5mGAYv/rQNgBsuaUDn2HD3BiQiUsMpuRFxs7mbjrEh/ixBft6Ool4RESk/JTcibpSRncuk+TsBGNu/OTGhAW6OSESk5lNyI+JGHy/bT0LKORrWDuTuy5q4OxwREY+g5EbETZIzcvjk930APHNdGwJ8vd0ckYiIZ1ByI+Imczcd5VyOjVYxIQxqX9fd4YiIeAwlNyJu8s06c7mD4d1jNfRbRMSFlNyIuMG2Y8lsPZqCr7eFYV3cM2GfiIinUnIj4gbfrjMn7Lu6bQwRtfzcHI2IiGdRciNSxbJyrfyw8SgAw7vFujkaERHPo+RGpIot3J7I2Ywc6oYGaHFKEZFKoORGpIrNyuuSuqlrQ7y9VEgsIuJqSm5EqtDRs5n8b88JAG7u1tDN0YiIeCYlNyJVaPb6IxgGXNo0gsZ1ark7HBERj6TkRqSK2GwG367Pm9tGhcQiIpVGyY1IFflj/ykOn84kxN+HQe3ruTscERGPpeRGpIrMypuReEjn+gT6aR0pEZHKouRGpAokZ+bwy9YEAEaoS0pEpFIpuRGpAj9tOkZWrrlIZseGYe4OR0TEoym5EXGB+FMZ/GflQc7lWIvcb++SurlbQy2SKSJSyXzcHYCIJ3jz1538vPk4h09n8Nz1bZ327TiewuYjyVokU0SkiqjlRsQFjp7NBGD6yoPsO5HmtM++SOaANjHUCfav8thERC42Sm5EXOB0ejYAuTaDV+ftcGzPzrUx508zudHcNiIiVUPJjYgLnE7LdjxfvDOJJbuSAPhtRyJnMnKICfWnb4tId4UnInJRUXIjUkFZuVZSs3IBGJ63XtQrP28nx2pzFBLf1LUhPt767yYiUhX021akgs6k5wDg42Xh2cFtqVPLj30n0vnnr7v4fXfeIpld1SUlIlJVlNyIVNCp9CwAatfyIyzQl8cGtgLgk9/3YzOgR5MI4iK1SKaISFVRciNSQfZi4jq1/ACzcLhNvVDHfs1ILCJStZTciFSQPbmJyEtuvL0sTBxiznUTGuDDoA513RabiMjFSJP4iVTQqTTn5Abg0qZ1mHFPT8KD/Ajy038zEZGqpN+6IhVkr7mpUyC5AejdXEO/RUTcQd1SIhWU3y2l2YdFRKoDJTciFeTolgr2u8CRIiJSFZTciFTQ+aOlRETEvZTciFTQ+aOlRETEvZTciFTQKbXciIhUK0puRCogx2ojOdNcfkEtNyIi1YOSG5EKOJNhttpYLBAepORGRKQ6UHIjUgH2epvaQX54e1ncHI2IiICSG5EKOZ2mehsRkepGyY1IBZzSSCkRkWpHyY1IBTjmuNEEfiIi1YaSG5EKOJVmriullhsRkepDyY1IBZzSulIiItWOkhuRCtDSCyIi1Y+SG5EKUEGxiEj1o+RGpALUciMiUv0ouRG5gK/XxDN62hpSz+UU2udYNFOjpUREqg0lNyIlMAyDdxbuZumuEyzakeS0z2ozHMsvqFtKRKT6cHtyM2XKFOLi4ggICKBnz56sWbOmxOMnT55Mq1atCAwMJDY2lkcffZRz585VUbRysTl8OpMTqeZw7z1JqU77zmZkYxjm89paV0pEpNpwa3LzzTffMH78eCZOnMiGDRvo1KkTAwcOJCkpqcjjZ8yYwVNPPcXEiRPZsWMHn3/+Od988w3PPPNMFUcuF4t1h047nu9JTHPaZ++SCgv0xdfb7X8niIhIHrf+Rn7nnXe49957GTNmDG3btuXjjz8mKCiIqVOnFnn8ypUr6dOnD7feeitxcXFcc801jBw58oKtPSLlte7QGcfzvUnOyc0pFROLiFRLbktusrOzWb9+PQMGDMgPxsuLAQMGsGrVqiLf07t3b9avX+9IZvbv38/8+fO57rrriv2crKwsUlJSnB4ipbX+YH5yc/BUOlm5Vsfr0xoGLiJSLfm464NPnjyJ1WolJibGaXtMTAw7d+4s8j233norJ0+e5LLLLsMwDHJzc3nggQdK7JaaNGkSL774oktjl4tDcmYOu/PqbPx9vMjKtXHgZDqt64YCBVpuNFJKRKRaqVGFAkuXLuW1117jww8/ZMOGDXz//ffMmzePl19+udj3PP300yQnJzsehw8frsKIpSbbEH8Gw4C4OkG0bxAGONfdnE7T0gsiItWR21puIiMj8fb2JjEx0Wl7YmIidevWLfI9zz//PHfccQf33HMPAB06dCA9PZ377ruPZ599Fi+vwrmav78//v66+UjZ2bukujaOwNfbwvpDZ9hToO7mVLo5iko1NyIi1YvbWm78/Pzo2rUrixYtcmyz2WwsWrSIXr16FfmejIyMQgmMt7c3YM5HIuJK9pFS3eJq0zw6GIC9BYaDa+kFEZHqyW0tNwDjx49n1KhRdOvWjR49ejB58mTS09MZM2YMAHfeeScNGjRg0qRJAAwZMoR33nmHLl260LNnT/bu3cvzzz/PkCFDHEmOiCvkWG1sPHwWgG6Na3Ms2ZxLaXcR3VKquRERqV7cmtyMGDGCEydOMGHCBBISEujcuTMLFixwFBnHx8c7tdQ899xzWCwWnnvuOY4ePUpUVBRDhgzh1Vdfdde3IB5q+7EUzuXYCAv0pVlUMLX8zf8qB0+mk51rw8/HS6OlRESqKbcmNwDjxo1j3LhxRe5bunSp02sfHx8mTpzIxIkTqyAyuZjZ57fp2rg2Xl4W6oUFUMvPm/RsK4dOpdMiJkTdUiIi1VSNGi0lUlXW59XbdG1cGwCLxULzmBAA9iSlYSuwrlQdjZYSEalWlNyInMcwDNbljZTqlpfcALTIKyrek5hGyrkcrDaziL12Ld+qD1JERIql5EbkPEfOZJKUmoWvt4VOseGO7S1j8pKbpFRHl1SIvw/+PipmFxGpTpTciJzHPgS8Xf0wAnzzE5cW0Wa31N6ktPxiYo2UEhGpdpTciJynqC4pwDHXzf4T6ZxINSfwUzGxiEj1o+RG5Dzr80ZKdYtzTm4ahAcS6OtNttXGn/HmMZqdWESk+lFyI1JAcmYOuxLNWYi7No5w2uflZXG03vyx3+y6UsuNiEj1o+RGpICNh89iGNC4ThBRIYWHeNtHTG07lgxo0UwRkepIyY1IAQdPpgPQum5Ikfub542YyhsFTqQKikVEqh0lNyIFJKSYa0jVDQ0ocr99xJSduqVERKofJTciBSTmLZAZE1ZcchPs9FrJjYhI9aPkRqSAxNSSW25iI4Lw98n/b6OlF0REqh8lNyIFJCSXnNx4e1loFpXfeqNJ/EREqh8lNyIFJKaYk/MV1y0F0CImP7nRPDciItWPkhuRPGlZuaRl5QIQU0zLDeTX3QT5eTstzyAiItWDkhuRPPYuqWB/H4L9fYo9rnneiCkVE4uIVE/F/wYXucgk5g0DjwktuUj4shaRXNo0ggFtYqoiLBERKSMlNyJ57MlN3RLqbcBs2Zl5X6+qCElERMpB3VIieRIcLTclJzciIlK9KbkRyZN4gWHgIiJSMyi5EcmjlhsREc+g5EYkT4J9jhslNyIiNZqSG5E8SaUsKBYRkepNyY0IYLUZJKWaLTequRERqdmU3IgAp9KysNoMvCwQqfWiRERqNCU3IuQXE0cG++Pjrf8WIiI1mX6Li1BgNXDV24iI1HhKbkSAxFSNlBIR8RRKbkTQBH4iIp5EyY0I+TU36pYSEan5lNyIUHBFcCU3IiI1nZIbEfILimNC/d0ciYiIVJSSGxHyW25UcyMiUvMpuZGLXma2lZRzuQDEqOZGRKTGU3IjFz17MXGQnzch/j5ujkZERCpKyY1c9BIKDAO3WCxujkZERCpKyY1c9Oz1NtEqJhYR8QhKbuSil6BiYhERj6LkRi56jjluVEwsIuIRlNzIRU/DwEVEPIuSG7noJWhdKRERj6LkRi56iSnmiuDRSm5ERDyCkhu5qNlsRn63lGpuREQ8gpIbuaidzsgm12ZgsUB0iIaCi4h4AiU3clGz19vUqeWPr7f+O4iIeAL9NpeLWn6XlFptREQ8hZIbuahpAj8REc+j5EYuaonJ9qUXlNyIiHgKJTdyUbMPA1fLjYiI51ByIxc1dUuJiHgeJTdyUdO6UiIinkfJjVy0DMPg6NlMAOopuRER8RhKbuSilZSaReq5XLws0LhOkLvDERERF1FyIxetPYlpAMTVqYW/j7eboxEREVdRciMXrT1JqQC0iAl2cyQiIuJKSm7korU7r+WmRXSImyMRERFXUnIjF629arkREfFIbk9upkyZQlxcHAEBAfTs2ZM1a9aUePzZs2cZO3Ys9erVw9/fn5YtWzJ//vwqilY8hWEYjpab5tFKbkREPImPOz/8m2++Yfz48Xz88cf07NmTyZMnM3DgQHbt2kV0dHSh47Ozs7n66quJjo7mu+++o0GDBhw6dIjw8PCqD15qtJNp2SRn5mCxQLMoJTciIp7ErcnNO++8w7333suYMWMA+Pjjj5k3bx5Tp07lqaeeKnT81KlTOX36NCtXrsTX1xeAuLi4qgxZPIS9mLhRRBABvhopJSLiSdzWLZWdnc369esZMGBAfjBeXgwYMIBVq1YV+Z65c+fSq1cvxo4dS0xMDO3bt+e1117DarUW+zlZWVmkpKQ4PUT2JtmLidVqIyLiadyW3Jw8eRKr1UpMTIzT9piYGBISEop8z/79+/nuu++wWq3Mnz+f559/nrfffptXXnml2M+ZNGkSYWFhjkdsbKxLvw+pmfY46m00UkpExNO4vaC4LGw2G9HR0XzyySd07dqVESNG8Oyzz/Lxxx8X+56nn36a5ORkx+Pw4cNVGLFUV7sTzW6plhopJSLicdxWcxMZGYm3tzeJiYlO2xMTE6lbt26R76lXrx6+vr54e+fXSLRp04aEhASys7Px8/Mr9B5/f3/8/f1dG7zUePndUmq5ERHxNOVquVmyZEmFP9jPz4+uXbuyaNEixzabzcaiRYvo1atXke/p06cPe/fuxWazObbt3r2bevXqFZnYiBTlVFoWp9KzAWgWXcvN0YiIiKuVK7m59tpradasGa+88kqFunnGjx/Pp59+yn/+8x927NjBgw8+SHp6umP01J133snTTz/tOP7BBx/k9OnTPPzww+zevZt58+bx2muvMXbs2HLHIBcfe6tNw9qBBPm5dcCgiIhUgnIlN0ePHmXcuHF89913NG3alIEDBzJr1iyys7PLdJ4RI0bw1ltvMWHCBDp37szGjRtZsGCBo8g4Pj6e48ePO46PjY3l119/Ze3atXTs2JG///3vPPzww0UOGxcpzh6NlBIR8WgWwzCMipxgw4YNTJs2ja+//hqAW2+9lbvvvptOnTq5JEBXS0lJISwsjOTkZEJDQ90djrjBC3O3MX3lQe67vCnPXNfG3eGIiEgplOX+XeHRUpdccglPP/0048aNIy0tjalTp9K1a1f69u3Ltm3bKnp6EZezT+CnZRdERDxTuZObnJwcvvvuO6677joaN27Mr7/+ygcffEBiYiJ79+6lcePG3Hzzza6MVcQl8lcDV3IjIuKJypXc/O1vf6NevXrcf//9tGzZkj///JNVq1Zxzz33UKtWLeLi4njrrbfYuXOnq+MVKdKf8We45ZNV7EwoeQbqsxnZnEjNAqBFjIaBi4h4onINFdm+fTvvv/8+N9xwQ7FzyERGRrpkyLhIacxcc5g/9p/mo6X7ePeWLsUeZx8pVT8sgGB/jZQSEfFE5frtXnBummJP7ONDv379ynN6kTJLSj0HwLLdJ7DaDLy9LEUeZx8p1VytNiIiHqtc3VKTJk1i6tSphbZPnTqVN954o8JBiZTViTSzq+lsRg4bD58t9rg9qrcREfF45Upu/v3vf9O6detC29u1a1fiOk8ilSUpJcvxfNmupGKPs4+UUnIjIuK5ypXcJCQkUK9evULbo6KinCbdE6kKVpvhWE4BYMmuE8Ue61hTSgtmioh4rHIlN7GxsaxYsaLQ9hUrVlC/fv0KByVSFqfTs7Ha8uei3HI02VGDU1DKuRyOJ5vbm0ep5kZExFOVq6D43nvv5ZFHHiEnJ4crr7wSMIuMn3jiCf7xj3+4NECRC7EP7Y4M9qNeWCBbjiazbNcJbu4W63ScvdUmOsSfsCDfKo9TRESqRrmSm8cff5xTp07x0EMPOdaTCggI4Mknn3Ra6FKkKthbaaJCAujfKootR5NZurtwcrPzuFlv01IjpUREPFq5uqUsFgtvvPEGJ06c4I8//mDTpk2cPn2aCRMmuDo+kQtKymu5iQrx54rW0QD8vvsEuVab45jsXBufLd8PQNfGtas+SBERqTIVmsUsODiY7t27uyoWkXKxd0tFh/jTqWE4tYN8OZORw4b4s/RoEgHAF6sOsv9EOnVq+XF33ybuDFdERCpZuZObdevWMWvWLOLj4x1dU3bff/99hQMTKa2CyY23l4XLW0bx48ZjLNmVRI8mEZxKy+LdRXsAeHxgK0IDVG8jIuLJytUtNXPmTHr37s2OHTuYM2cOOTk5bNu2jcWLFxMWFubqGEVKdKJAtxRA/1Zm19TSvCHh7yzcTeq5XNrWCy1UhyMiIp6nXMnNa6+9xr/+9S9++ukn/Pz8ePfdd9m5cyfDhw+nUaNGro5RpET2guLokAAALm8ZhcUCO46nsGRXEl+viQdg4pC2xS7LICIinqNcyc2+ffsYPHgwAH5+fqSnp2OxWHj00Uf55JNPXBqgyIWc33ITUcuPTg3DARj31QZsBgzuUI+eTeu4K0QREalC5UpuateuTWqqOay2QYMGbN26FYCzZ8+SkZHhuuhESiGpQM2Nnb1rKj3bir+PF08NKrxciIiIeKZyJTeXX345CxcuBODmm2/m4Ycf5t5772XkyJFcddVVLg1QpCRpWblkZFuB/JYbgP6toxzP77u8KbERQVUem4iIuEe5Rkt98MEHnDtn1jk8++yz+Pr6snLlSm688Uaee+45lwYoUhJ7l1QtP29q+ef/OLevH0bPJhGkZ+fyQL9m7gpPRETcoMzJTW5uLj///DMDBw4EwMvLi6eeesrlgYmURlJKXjFxaIDTdi8vC9/c38sdIYmIiJuVuVvKx8eHBx54wNFyI+JOJ9LyiomD/S9wpIiIXCzKVXPTo0cPNm7c6OJQRMouKSUvuQlVciMiIqZy1dw89NBDjB8/nsOHD9O1a1dq1arltL9jx44uCU7kQtRyIyIi5ytXcnPLLbcA8Pe//92xzWKxYBgGFosFq9XqmuhELsDechOtlhsREclTruTmwIEDro5DpFzssxOr5UZEROzKldw0btzY1XGIlItj0czzRkuJiMjFq1zJzRdffFHi/jvvvLNcwYiU1YkiZicWEZGLW7mSm4cfftjpdU5ODhkZGfj5+REUFKTkRqpEjtXG6YxswHl2YhERubiVayj4mTNnnB5paWns2rWLyy67jK+//trVMYoU6VRaNoYB3l4WIoL83B2OiIhUE+VKborSokULXn/99UKtOiKVxd4lFRnsh5eXxc3RiIhIdeGy5AbM2YuPHTvmylOKFMs+Uio6RMXEIiKSr1w1N3PnznV6bRgGx48f54MPPqBPnz4uCUzkQpJUTCwiIkUoV3IzdOhQp9cWi4WoqCiuvPJK3n77bVfEJXJB9m4pFROLiEhB5UpubDabq+MQKbP8biklNyIiks+lNTciVUktNyIiUpRyJTc33ngjb7zxRqHtb775JjfffHOFgxIpjSRHcqOCYhERyVeu5Ob333/nuuuuK7R90KBB/P777xUOSqQ01HIjIiJFKVdyk5aWhp9f4UnTfH19SUlJqXBQIhdiGIZGS4mISJHKldx06NCBb775ptD2mTNn0rZt2woHJXIhKZm5ZOeahe1quRERkYLKNVrq+eef54YbbmDfvn1ceeWVACxatIivv/6ab7/91qUBihTlRJo5Uio0wIcAX283RyMiItVJuZKbIUOG8MMPP/Daa6/x3XffERgYSMeOHfntt9/o16+fq2MUKSQpJa9LKlTFxCIi4qxcyQ3A4MGDGTx4sCtjESm1E2l5xcTB6pISERFn5aq5Wbt2LatXry60ffXq1axbt67CQYlcSH7LjZIbERFxVq7kZuzYsRw+fLjQ9qNHjzJ27NgKByVyIWq5ERGR4pQrudm+fTuXXHJJoe1dunRh+/btFQ5K5EKSUvKWXlDLjYiInKdcyY2/vz+JiYmFth8/fhwfn3KX8YiUWv4cNyooFhERZ+VKbq655hqefvppkpOTHdvOnj3LM888w9VXX+2y4ESKo9mJRUSkOOVqZnnrrbe4/PLLady4MV26dAFg48aNxMTE8OWXX7o0QJGiaHZiEREpTrmSmwYNGrB582a++uorNm3aRGBgIGPGjGHkyJH4+vq6OkYRJ1m5VpIzcwC13IiISGHlLpCpVasWl112GY0aNSI7OxuAX375BYC//OUvrolOpAgb488C4O/jRVigkmkREXFWruRm//79DBs2jC1btmCxWDAMA4vF4thvtVpdFqBIQTabwavzdwAwtHMDp587ERERKGdB8cMPP0yTJk1ISkoiKCiIrVu3smzZMrp168bSpUtdHKJIvu//PMrmI8kE+/vw2MBW7g5HRESqoXK13KxatYrFixcTGRmJl5cX3t7eXHbZZUyaNIm///3v/Pnnn66OU4S0rFzeWLATgL9d2Vz1NiIiUqRytdxYrVZCQkIAiIyM5NixYwA0btyYXbt2uS46kQKmLNnLidQsGtcJYnSfOHeHIyIi1VS5Wm7at2/Ppk2baNKkCT179uTNN9/Ez8+PTz75hKZNm7o6RhHiT2Xw+f8OAPDc4Lb4+3i7OSIRqdGSj0DyUWjU092RSCUoV3Lz3HPPkZ6eDsBLL73E9ddfT9++falTpw7ffPONSwMUAXh1/nayrTb6tohkQJtod4cjIjXd17dAwlZ4YDnUbe/uaMTFytUtNXDgQG644QYAmjdvzs6dOzl58iRJSUlceeWVZT7flClTiIuLIyAggJ49e7JmzZpSvW/mzJlYLBaGDh1a5s+UmmPl3pP8ui0Rby8Lz1/fViOkRKRizh6GhC2AAUdKd7+RmqVcyU1RIiIiynXT+eabbxg/fjwTJ05kw4YNdOrUiYEDB5KUlFTi+w4ePMhjjz1G3759yxuy1BDTVh4E4NYejWgZE+LeYESk5tu/JP954jb3xSGVxmXJTXm988473HvvvYwZM4a2bdvy8ccfExQUxNSpU4t9j9Vq5bbbbuPFF19Ujc9FYMsRcw2zv3Su7+ZIRMQj7F+a/1zJjUdya3KTnZ3N+vXrGTBggGObl5cXAwYMYNWqVcW+76WXXiI6Opq77777gp+RlZVFSkqK00NqjlNpWSSknMNigTb1Qt0djojUdDbbecnNdjAMt4UjlcOtyc3JkyexWq3ExMQ4bY+JiSEhIaHI9yxfvpzPP/+cTz/9tFSfMWnSJMLCwhyP2NjYCsctVWfbMTMZjatTi2D/cq8WIiJiStwKGafAtxZ4+UJWsjlySjyK27ulyiI1NZU77riDTz/9lMjIyFK95+mnnyY5OdnxOHz4cCVHKa5kT27a1VerjYi4gL3eJu4yiGxpPlfXlMdx65/CkZGReHt7k5iY6LQ9MTGRunXrFjp+3759HDx4kCFDhji22Ww2AHx8fNi1axfNmjVzeo+/vz/+/prJtqbadsyst2lXP8zNkYiIR9iXl9w0vQKO/QlJ28zWnFbXujUscS23ttz4+fnRtWtXFi1a5Nhms9lYtGgRvXr1KnR869at2bJlCxs3bnQ8/vKXv9C/f382btyoLicPpJYbEXGwWWH2PTD37+Wrk8k5B/F59ZzN+kNMO/N50nbXxVjV1n4Onw2A1MQLH3sRcXsRw/jx4xk1ahTdunWjR48eTJ48mfT0dMaMGQPAnXfeSYMGDZg0aRIBAQG0b+882VJ4eDhAoe1S86Vl5XLgpDlZpJIbEeHgctjyrfm8998hsnnZ3n/4D8g9B8F1Iap1fq1NTe2WSj4CC54GaxZs+x4ufdDdEVUbbk9uRowYwYkTJ5gwYQIJCQl07tyZBQsWOIqM4+Pj8fKqUaVB4iI7jputNnVDA6gTrK5FkYve5ln5z/cvKXtyYx8l1fQKsFjyW25O7oHcLPCpYb9nlk4yExuAI+vcG0s14/bkBmDcuHGMGzeuyH1Lly4t8b3Tp093fUBSLWw7aq+3UauNyEUvJxN2zM1/vX8p9Li3bOew19s0629+DakHgbUh8wyc2AX1Orok1CqRtBM2zsh/fVTJTUFqEpFqy1Fv00DFxCIOO36GRS+BNdfdkVSt3QsgKwV8As3XB34v2zXIOA3HN5nPm15hfrVYICavpKEyu6aSj8AvT8LpA64756KXwLDlfy9nDkL6Sdedv4ZTciPVloqJRc6z6kP45jb439uw+xd3R1O1NufV2vS8DwLCzUTn2J+lf/+BZYABUW0gpMBo3Oi25tfEra6KtLC5f4fVH8MPD7pmwsD41bBrHli8YNA/84e0H11f8XN7CCU3Ui1l5VrZk5QKKLkRwTBg8avw69P52/YtKf54T5NxGvb813ze6VZocrn5fH8ZrsH5XVJ29rqbymq52b8M9uWNCI5fBbt/rdj5DAN+e8F83uV2iGoJDbqZr1V346DkRqqlPYlp5FgNwgJ9aRAe6O5wRNzHZjO7NH5/03zd4hrza1lu7DXdtjlgy4G6HSC6dX6CUpYEz1FMfH5yk9ctVRnDwQsmIkF5E88uetEc0l5eu3+F+JXgEwBX5CW7DbuaX1V341AtCopFzpc/eV9ouVabFynk9H7IPAsNLnF3JMVL2gHxfzhvO7DMvLljgcFvQYeb4Y0m5vdz5hDUbuyWUKuUffh3h+HmV3udyZE1kJUG/sHOxyduh8Or819np8HZQ+ZyC417Ox8b3RqwQFoipJ2A4Kj8fakJZovRhZKRgDBoPbjwaKvtP8KxDeZSD3ctgM+uMpOozbOg88gLf99nDppJWcGurD8+Mr/2vB9C8xYTtrfcHF1vJsPnjzDOPAuHVkDzAWUbEVbUz6O3H7QaBEERpT+PGyi5kWrJXm/TXsXE4go758O3o8GaDXcvhNju7o6osIzTMHUgnEsuvM/iDcP+DR1vNl837GbevPcvha6jqjTMKnfmUN7EexbocJO5LaIphDc2E5ZDK6DlwPzj00+Z1zGriEWSY3sUToT8akFEXrKYtA2CrzC322wwYwQc31i6OOP6wsivwT/EfG3NhcUvm897j4PIFnDZePhtIix5DdrfUHKiceB/8PVIyE4tvC8gDC57NP91TDuzJedcMpzeZ35WQb88AZu/gSb94JYZha9BUbZ+D9/fZ7aYnS+8Mdz5o3ndqiklN1ItqZhYXGbTN3mFnHl/ff/2Aoz+2RwpU50sf8e8OYU2gPpd8rd7+0LX0fmtFWA+v1iSG3urTZO++S0VYF6DDf8xr0HB5OZ/b5mJTWhDqN85f7uXD/T+W9GfEdPOTG4St+Vf5+1zzMTGt1bhOp3z7V8KB/8H//kL3D7bbNX480s4tReC6kCvvKlOet4Pq/8NyfHmzMK9Hir6fLt+gVmjzDlsots5JxEWC3S5wxzCbuftC/U6mT8TR9Y5JzfnUswWJDBbAb/4K9z2bcktL+unw0+PAAY06GoOmbc7ttFMKqdeC3fMgZi2JV8bN1FyI9WO1WY4JvBTciMVsvoT+OVx83mbv5j1CoeWw97foMXV7o2toOQjZqwA10+GlteUfHzT/rDsDfNmVVQ3hKcwjPyJ+zqOcN7XrL+Z3BSsuzlzCNZ+Zj7/6/vQ7MrSfU50O9jxk9mdBZCbDYvyWl0uewT6PVHy+49ugP+70eyCmjbIbB1Z+rq57/InICDv95hvIPR/Gub+DX7/J3S5zWyFKWjzLJjzgJmMt7oObpoGvgEX/h4a5LXmHV3n3OW182dzVubQBpCTYe6fPthMTEIKr+HIindh4QTzedcxMPht8PLO35+aAF/eYLZyTRtkJnMNu104viqm5EaqnYOn0snIthLo602TyFI0n4pnO5cCaUnO2/yDi/7FbGcY5s1jyavm6x73w7Wvw28TYOX7ZutNs6tclxRkp5vzr5R0vpxz5teiblT2mWYb9yld0tWwG/gFQ8YpSNxi/tVekM0GZw+aXwsKjy25KyQn06ypKHgzKw9rDpyNv/Cw54BQCI4ufn/CZji5C7z9oc0Q531N+gEWOLEDUo5DaD2zu8eabe4rbWIDBUZM5Q0H3/AfOHMAakXDpcW0rhTU4BIY8wt8OQxO7IQPe5n/nuGNoNsY52M73Wr+DJ7cDcveNBMIu70LzeUUMKDjLfDXKeBdytu0vaj4/BFT9uSw62jzGn45zKz7mToQbvzcHFZv9+eXsGKy+fyyR+GqiYVbOEPqmi2fM4bDkbVma9UNn5jLWRTk42/+vLmJkhupdrbmzUzcul4I3l7VrOtAqlbaCfiga9F1KL3GwTWvFP7la7PBf5+DP6aYr/s9BVc8ZR532XhY/4V5E9vyLXQaUfi8ZXU2Hj69yvwL/LZvi65DOLrBvBlYvMxjCiYjBWeaHfBi6brLvH0h7jJzYrt9SwonN3PHwcavCr+vbke4d0nRN8zMs/DJFeYyBLfNMkcmlYdhwP/dYE6ydyEWL7j9++K7ff78P/Nrq2sLt3AERZjf9/GNZgtWTHuzrgRgwAtli9me3JzYaSbTy/JGpvV7onT1KWAWJt+1AL4canZxAfR/rnAy6e0DV02Ab26HVR+Yj/PZk/GyJN/2ouLErWYi7RtgtrIcWGZu73CTWat01wKza+rMQbPAuSgDXjRbrIoTFGHW3My8zRy1981thY9p2APuWVj6+F3MQ9sypSbbrnobsdv6nZnYePmaNzf7A8ybwo/jnGepteaaN3Z7YjNwktkNYE8YgiLgsofN50teMW/kFbXkNUhPglN7zDqEpB3O+w/k1WKknzBH5Ey/Hg6tyt9vn2m29fVlK3S2D2k+f0j4sT/zE5uC18zibbaEbPq66POtmGy2VqQeg2mDC4+SKa1d883ExuLl/PnnP3yDzO/7v88XbmECM2lcP9183nV00Z9VcEj4opcAA9oOLfuIuNpNzHhyz8H8x8x/z9pxcEkZ65lqN4YxC8zh+h1uNh9FaX09tL+x8DWpFWUmPoPeKHurYngj8/22XPPfGWDrbPMaN+xhJjZgfl93/Wq2bp3/+aEN4C/vl5zY2PnVglu/Met/AsILn6u0SWElUcuNVDuOkVL1NVLqomf/S3zga+bMtHZ/fpXXOvF/kJVsNq8DzL7brJ2weJlN+p1vLXzOng+a9S1n42HdNLj0gfLHl7gNNs00n9eOM/8anjYIbpttdhMULAyN62sOKY5faXYNjPgS/EPzZ5q9akLZPtte+Hpoldmd5Js3H9RvL5pfOwyHGz/NP37l+2aL1tJJ5l/x9uPB7Nb54+O876OJmeR8MRRG/B+0GFD6mGzWvCSDvG6NEr6njNPwbiezW23rd9BxuPN+exdTXN/Cc9PYNe0Py/9lFszmZpoJ3JXPlz5eOy8viG5jDqW2/8xd+Tz4+JX9XCExZutcSSwWuGlq2c99oXM26GbOXH1knTkyzFGvdN61DakLo+YWPkdZ+fjDXz8wH9WMWm6kWjEMo8AcN0puLmon95itEBZvc9hsQV1ug+FfmPUhO34yh+zOGGE+9/aD4V8WndgA+AWZ3VRgTox3roghw6X124s4WgvuXWKOLMk8A/8ZYu6beZuZ2LS6Dm77ziy+bHGNeSP++haYc795ns63QVSrsn12VCtzFIs1K7+VZd8SsyXHyxeufNb5+O73miOIUo7Cmk+d9y173Ywp9lJ4MG8+FHuMW78vfUybvja7dgJrQ5+HSz42KCL/mMUvO7eiFUwary6hqy62pzkEOjfTfN11VNlXCrezd02B2X3X7obij62uCk7md2K32WXn5QPthrk1LHdQciPVyvHkc5zJyMHHy0LLuiomLtKpfWb3x//eds06NdWV/a/O5ldBrcjC+9sMgVtnmUN19+fd1H1rmX81t7m+5HN3uQPqNDcLcv/4sHzxHVoJe37Nby0IioA755rN/Tnp5tBuw2qO8hn+hVkD4RcEI74yuyRsuWYLScGZZsvCYslvvdm/1Ozasc+G2+0usyWpIN8As4sOzJ+dzLPm85N7YMOX5vOrXzS7G2752ry523Lgu7tg14ILx5OTaba2APT9R+EamaJc+iAEx+S3otk5upj+aiaMxfENyJ+UzzcI+j154c8sjn2mYjBrdmriCLSCyzBsyfv/06yY/z8ergb+64kns3dJNY8Oxt+ngiM2PNWvz5iTmi16Ceb9o+h6hZrOMPJ/OXcYXvxxzfqbhY2BEeZj1Fzn+WCK4+2Tn1Cs/0/Zp8M3DFg40Xx+yZ35rQX+wXnJ1V/M1z3uh6EfmwXAdj5+cMOn0P0e8/Xlj0FYg7J9vl3BupvtP5h/qfsFw+WPF318p5HmwpHnzuaPiln0kpmEtRwEjS7Nj/HGz8wROxj5w6tLsuZTs1UotKHZSlQafrUKtKL9E7JSzW623QvyksZSdNXZfz4uf6zkEXQX0vQKs5Wj1XVlG2lVndjnRzp7yPy5hsJdUhcJ1dxItWIfKaUuqWI4fvF7mTfYdZ+bE5YN/cj5BlrTHVln1q/41oLW15V8bGx3eDRvCK9frdJ/RpshZutC6jFzllv7YoylsWu+OfW/T2Dh1gIff7OlJv2k81T+BXl5m/OHXPmc82RsZdW0n/n1+Ob8ZKv330r+3KsmwMyRZo1Ng26wYy5gKVwf4+VtnmvzTPP65GYVP4w886zZGgTQ/5nSzcti1+UOWPmBObPuyg/yC6QvuaN0XUydbjFb90oaUl4aUa3gsT3mDMPVbYLH0goMN1cIP7nbLIr2CzaTtYuQWm6kWtHMxCUwDHPqdjBbC2763PxLc8u3Zm1HTqZ743Mle1Fnm+tLl7D41SpbYgPmjbrtUOfPKw1rbn7R7qUPmvOrnM9iKT7BKKgiiQ2YLRXRbQHDnPU2KBJ6jS35Pa0GmbU1uZkw605zW6eRRc80G9POHIGTk2HOaVKcFe+arUFRbcxkoyy8feGqvCLg3/9pTkTnE2gO4S8Ni6XiiY1dUETN/yPB3jUF5qgsvyD3xeJGSm6kWtmeV0xcrdeUso8I2fFT1X7url+cf/G3vxFGzjRf7/nVnCHVFUObz7duqjnSpqTur40z4Pe3ylYDlJVqJgk75ztvt+bAtrwi1spuUrfPert9bv4kewXlnINfnzXrTuyPr24yJ5YLCL9w0WxVKDiSqN8T+WsbFcdiyZ8HxrCaBdj9i6n5KVjXU9wK3CnH8xdzvGpC+SYAbDvU7FKxL5Fx6QNFJ41yYQ0L1ChdpF1SoORGqpEz6dkcSzZvMG3qXeAXtDvtX2o2wX9zu9mMXhVsVlhkby0o8Iu/xdVwx/fmkOJDK/LX4XGVU/vg50fNIcQ/PGAmHgXZa09+eNAc8ZK4rXTnTT9ljiha/k5eF8lH+fv2LTYLfWtFQZMrXPWdFK1RL7NGJCvF7O473x8fmvPpbJ2d/7B3m1z+mNkN4G72dZVqxznPdluSxr3yuyt63GfOkVIcR13P0qL3r5+eN9Kqp9kqVB4WizlxHOSNtHqkfOcRaHwZYDHnrGnSz93RuI1qbqTasHdJxdUJIiSgGjcNJ2zJf/7fZ82hv1c+V7n99PYhtgHhhX/xN+5ttiAsftkcYdTldtd9bsFkafM35rDpm6ebNRU2q1nQvL7AKJeTu6Bu+0KncZJyzJxD5eSuvGG852DBU2bdxhVP5Y+San9T6aeeLy8vL3POlxWTze+13dD8fRmnYflk83mP+/InQQPz36G6/FXctB/c+q05T0tZ5mUZ9jHsWWiOSCrx/FeYX49tMH/WC3alFSz87n5Pxf4PNO1nFocH160eSWNNFd0aRv1kLjJa2f9/qjG13Ei1UWPmt0nKW1wvOm9ejP+9BfMfr7xRSznnYMkk83nffxT9i98+E+qB383kwRUMI78WpfPtZiKy+xezWybjNMy+Jy+xseTf+E/uKfmcp/aZa9qc3AUh9eH+36F/3nwsy16HeeNh5zzzdcdiZnd1NXvX1O5fze/Lbvk75gSB0e3MqfAvfTD/0XlkxddfcqWW15R9HZ+AMDOxu1CNSVgDs0jVsJmzLRd0dL251IBvLWg9uGyfX5SmV5g3Z6mYJn2hTjN3R+FWSm6k2tia13LTtroXE9u7Xq563hzxggXWfmpOyFZwKQBXWfsppBwxm5l73Ff0MbUbm10sGGbXiSsc3ZB34woyp4O/fTb4hcDB/8G/2pt1MV6+cPM0s8AZSk5uEreZ8/OcjTeTobt/NUeo9HsCBuWt5bNuqtnFUac51C/jFPrlFdPWnOPElmPOdAvOq3QPeKF6JTLuUFzXlL2VrfXgshd0i1QiJTdSbeS33FTj5CY3G07sMp/HtDOb4m/8LG/U0ixzVV1XsuaYI1HAnJelpCG29tabsoz8KYn9PK0Hm/O3xF0Go38y55PJyVsFe+RMc/bTyJbmsadKSG5+ftQcnhrTwVzbpmCdR8/7Ydi/zblNwJy7pCqH49q7mOw367Ku0u3pHJMFFigqtubkJ9LVpYtOJI+SG6kW0rNyOXAyHajm3VKn9ph/4fuHQlheN0CHm/K7VopblLC89i0xF1ysFWUO1y1Ju2FmS0rClsKLN5aVNTd/xFLBSfTqd4G7/2smdaPn5a87VKeF+fXUvqJHTFlzzaUUAIb/p+ihu51ugdu/M2fX7Xl/xeIvq/Y3ARZz3ac9v5V9lW5PF3eZmXie3g9nDpnb9i+FjJPm8PPi1n4ScRMlN1It7ExIwTAgJtSfqJBiJgqrDhLt9TZtnW96nW4BLOZQ7TMHXfd59taT9jdeuDgwKCK/lcHeAlFe+5eaSVVQZP7Ky3aRLczuuIJDTmvHmTe/7DRIPV74fKf2mosg+gWbCzMWp9mVcP2/qr6gNKyBeQMHc+6X8qzS7ckCQqFh3vwp9q6psvxsilQxJTdSLeRP3leNW20AEvNmwi24yB6YIxPsM9y6ajh2Vmp+cW1JSxAUZO8e2PJt6Qqcs9KKnhvHceO6oXSTmvn45a9ldHJ34f326xbdtvqu2WO/djnp5Vul29MVXOohK61A4fcI98UkUoxq+ltGLjb5yy5U43obyC8mPj+5Aee6DVcsaLlznllcG9EMGpSyuLbltWbRb/JhOPxHycemn4TJ7WFKD7M7yS4rDXb+bD4vy43LXndTVFFxSdetumjzF/DOazUszyrdns5Rd7PMnMAyJ8MsDC/tz6ZIFVJyI9VCjVl2oaSbdJsh5nDpk7vh+KaKf5a9a6ljGYprfQOhbd6ijRcqLN7ynTlvyZmD5iimhLzWlV3zzRtX7SYlr8h8Pvs6QKf2Ft5nHz5fnZObwHBzYr4GXfNrqCRfw25m4px5Gpbmrf7dcYRqkqRaUnIjbpeda2N3YipQzbulMk6biyyCOWHa+QLCzJYTqHjNS1pS/siUDmWc78XegrTth5KXY7BPvuYXbI5imn4dHF5TIKkq443LXlRcU1tuwByWfu9iTf1fFG/f/Lqks/Hm17L+bIpUESU34nZ7klLJsRqEBfrSsHagu8Mpnr31IbyRmcgUxd6Ns/U7cwbf8to62yxqbdCt7JNxxfU1Z3k9d9acgbYop/aZE7BZvOG+pebU+eeS4Yu/mssfQNmH90baR0ydl9xknjW7ySBvkUepsexdU2C2cF3kE8VJ9aXkRtzO3iXVtl4olurcxO1ofShheYHmA8zp6dMS4cCyks9ns8KcB+GbO5xnxgXn1pOy8vI2h6cDbJ5Z9DH28zfrbyYld8yBZleZ3VGGtXw3LnvLzdnDziuU25PCsFhNq1/TFRw5p0JiqcaU3Ijbba9p9TYltT74+JnzzQBsvsCoqS3fwqYZsGMuTLvOXF0Z4ORecx0fi3f+ucqq863m1x0/59fS2BVcVsF+g/KrlT8hH0C3u8v+mbUizTWXMJwLlEtz3aRmiGwJ9Tqb8y61v9Hd0YgUS8mNuJ1jpFSDGpLcXKhuxD5se8dPkJ1R9DG5WbD4VfO5ly+c2GGuuXR6f34tTPOrIDiqfLHGtIO2QwEDFr3kvO/oejhzwFxWwb4yNJiJ2U3T4B+78pOjsrBYiu6aqin1NnJhFos5u/Tf/zSTWZFqSsmNuJXNZrDjeA2Y48ZmKzDi5wKrXsf2NOtyslPNhSaLsm4qJMdDSD14YLk5MunsIXPU0p//Zx5T2rltinPl82brz55f4dDK/O2OZRWuN5dVKMhigZC65R8B4ygqLjBiSsmNZ/ENAP8Qd0chUiIlN+JWB0+lk55tJcDXi6aR1WDhvZxzsHACHFnvvP3MAbMexScgfwXs4nh55Y8i2fh14TlvzqXA7/80n/d70lwF+a5fzdWn0xIh5WjeKsvXUSGRzfMXtFw40YzDmgNb85ZVqIz1gBzDwfNabsqSFIqIuIiSG3ErezFx67qh+HhXgx/HHT+ZC1V+c5tzl5L9Bh3VqnRTzdtrWfYuhP8+55zgrPoAMk6ZK193ucPcFhIDY+ZBwx7m63ZDXbPKcr8nzQUuj6wx56/Zt6Ry1wM6fzj42UPmkgzefhpZIyJVphrcTeRiVu0m77MPWU49Dmv+nb+9NCOlCopqBde+YT5f9QH8OM5cPDItCVZ+YG6/aoJzohRYG+78EW78HAa+WrHvwy60Hlz6oPl80Uv5C3tW1npAjtXB95oJnVNSWIplHEREXECrnYlbbTl6FqhG9TZpifnPl/8LLhllLkhZ3JpSJbn0AbM2Ye442Ph/kJUMgRHm2kUNuprT/Z/PLyh/GLer9HnYrPE5sdN8QOUN441oYq7LlJViXsuyJoUiIi6glhtxm6TUc/yx35zfpWfTiIqdLCfTXIvJmlOx86Qm5D8/l2wmOFD+otgut8HwL8xumR0/wYb/mNsHvFB109bblxWwq8z1gHz8Ibyx+fzkHucFM0VEqoiSG3GbORuOYrUZXNIonGZRwRd+Q0l+ewFm3grL3qzYeewtN53yhkKv+cS8SZ8+YL6OLseInzZD4NZZZpEwmBP92VcQryrd74XQhubzDmVYq6o8Cg4H10gpEXEDJTfiFoZh8M06s75leLfYip0sNzt/ePPGGeYInfKyt9xccgc06g2552D2PYABtaLLP+9Ms/4wZj5c+hD85f3yx1devgFw83TocR/0Glu5n2Wvu0nYkj+Zn7qlRKQKqeZG3GJD/Bn2n0gn0Neb6zvVr9jJ9v5mrm4NkHIE4ldBXJ+yn8cw8ltugmPg6hfh86vh+EZzW0VbH+p3Nh/uEtvdfFS2OnnDwXfOAwxzZFZwdOV/rohIHrXciFvMWnsEgMEd6xHsX8Ec295q4+Xr/LqsslLNuWzAnMgutoc50Z2dulZKx94tZU8UY9pWXX2RiAhKbsQN0rNy+XnzMcAFXVLnUmD3AvP5VRPMr9t/MJc3KCv7zdgvJH+OmasmmKN/QMlNadnnurFTl5SIVDElN1Ll5m05Tnq2lSaRtegeV7tiJ9vxk1kXE9nKrCUJqWeOctqzsOznstfbhMTkb4tqBf2fhfqXQMtrKxbrxSI4GvwLzFukpFBEqpiSG6ly3+YVEt/crSGWinZXOFa3vhm8vPNXKi5P15Sj3qau8/bLH4P7lpjz3ciFFVxAEzQMXESqnJIbqVL7TqSx9uAZvCxw4yUNK3aylONw4HfzuX0tJ/vkdLt/hcyzZTufveVGxa8VZ++asnhBVGv3xiIiFx0lN1Klvl1nFhL3bxVNTGhAxU629TvAgNhLoXacua1uB/Nmas2CHXPLdr40e7dU3ZKPkwuzL6AZ0cycdVlEpAopuZEqk2u1MXuDmdzcXNFCYoDNs8yvHW/O32ax5K92bd9fWmlJ5tfgmJKPkwuLy5uksNmV7o1DRC5KSm6kyvyyNYETqVnUqeXHla0r2PWTtBMSNoOXD7S7wXmfvYvq4HJIPlr6c6aq5cZlGvWEx/fBta+7OxIRuQgpuZEqcS7HyhsLzEUbb7+0MX4+FfzR25LXKtP86sKFvuGNzNmFMfK6rkqp4AR+UnG1IsFLv2JEpOpphmKpEp8vP8CRM5nUDQ3g/n5Ny/bm+D9g3TSwZudvO7DM/FqwS6qgjjdD/EpY9SEc25i/3SfAXCU7uogiV7XciIh4BCU3UukSU84xZcleAJ6+rjVBfmX4sds+F2bf7ZzY2AWEQctBRb+v7VBY8IxZJLzte+d9tly48VPnbTnn4NxZ87labkREajQlN1Lp3liwk4xsK5c0CucvZVlH6s+vYO44MGxmEtOsv/P+Rr2KH4kTFAGj58HRdfnbTu6GtZ/BiZ2Fj7d3SXn7Q2AFJxYUERG3UnIjlerP+DN8v8Es6p04pF3pJ+1b9SH8+rT5vMsdMORdc5K+smjY1XzYndxjJjen9pmLZBaMpWC9jdZBEhGp0VTtJ5XGZjN48aftgDlhX6fY8NK9ccmk/MSm99/gL++XPbEpSu04c3RVTjqkHHPeV9TSCyIiUiMpuZFK8+Omo2w8fJZaft48eW2r0r3pyDpYljd8+Mrn4eqXXdeS4u0LtZuYz0/tcd6nkVIiIh6jWiQ3U6ZMIS4ujoCAAHr27MmaNWuKPfbTTz+lb9++1K5dm9q1azNgwIASjxf3efc3M4F4qH9zoks7G/HuX82vbf9qrunk6i4i+5pHJ89LbjRSSkTEY7g9ufnmm28YP348EydOZMOGDXTq1ImBAweSlJRU5PFLly5l5MiRLFmyhFWrVhEbG8s111zD0aNlmKxNKt3x5EwOnsrA28vCqN5xpX/j/qXm1+ZXV0ZYUCdvWYDzkxv70gvnL5opIiI1jtuTm3feeYd7772XMWPG0LZtWz7++GOCgoKYOnVqkcd/9dVXPPTQQ3Tu3JnWrVvz2WefYbPZWLRoURVHLiVZd/AMAG3qhRDsX8q69XPJcHS9+bzpFZUTmL3l5vxuqdS8binV3IiI1HhuTW6ys7NZv349AwYMcGzz8vJiwIABrFq1qlTnyMjIICcnh4iIiCL3Z2VlkZKS4vSQyrf+kJncdGtc9L9LkQ4uB8Nqtq6Eu2DtqaLYV6s+udd5u1puREQ8hluTm5MnT2K1WomJcf5rOSYmhoSEhFKd48knn6R+/fpOCVJBkyZNIiwszPGIja2km6Y4WXfoNABdG5dhzph9S8yvTfuXfFxF2FtukuMhOyN/u1puREQ8htu7pSri9ddfZ+bMmcyZM4eAgKILVp9++mmSk5Mdj8OHD1dxlBef9KxcdhxPBaBbXBmSm/325OYK1wdlF1Qnf5K+0/vMrzYrZJw0n6vlRkSkxnPrJH6RkZF4e3uTmJjotD0xMZG6dUu+ybz11lu8/vrr/Pbbb3Ts2LHY4/z9/fH393dJvFI6Gw+fxWozaBAeSL2wwNK96exhOLUXLF7QpG/lBWexmF1TR9aYRcV1O0D6CXMWZIuXudijiIjUaG5tufHz86Nr165OxcD24uBevXoV+74333yTl19+mQULFtCtW7eqCFXKwF5MXKYuKfsoqQZdzTWjKpOjqDiv7sY+DLxWtGsmCxQREbdy+/IL48ePZ9SoUXTr1o0ePXowefJk0tPTGTNmDAB33nknDRo0YNKkSQC88cYbTJgwgRkzZhAXF+eozQkODiY4ONht34fks9fblK1Laqn5tTLrbezOHw6epnobERFP4vbkZsSIEZw4cYIJEyaQkJBA586dWbBggaPIOD4+Hi+v/Aamjz76iOzsbG666San80ycOJEXXnihKkOXIlhtBn/GnwXK0HJjs+UnN+cvjlkZHBP57Ta/2ltuNDuxiIhHcHtyAzBu3DjGjRtX5L6lS5c6vT548GDlByTltishlbSsXIL9fWhdN7R0b0rcahb0+taCBlXQzRjZ0vx6aq+5gKaWXhAR8Sg1erSUuM+hxZ+xf8EHhbavz+uS6tIoHG+vUi6dYG+1ibsMfPxcFGEJajcBizdkp5mtNlp6QUTEo1SLlhupWc4lHaDx7/8AYGfj/rRu086xb92h8hQTV8EQ8IJ8/KB2Yzi935ypWC03IiIeRS03UmZJK790PJ+34CcMw3C8to+UKvXMxDnn4FDebNRVUW9jV6fAAppquRER8ShKbqRsDIPg3d87Xoae2sTPm48DkJB8jqNnM/GyQOdG4aU73+HVkJtpTp4X1boSAi5GwdXBHS03Sm5ERDyBkhspm4TNRGQccLzs7LWXSfN3kJltdQwBb1MvtPSLZRbskrKUskbHFQqOmNJQcBERj6LkRsrE2DQLgO22xgB09DpIUnIan/y+v0CXVCnrbay5sP1H83lV1dvY2buljq4Ha7b5XDU3IiIeQcmNlJ7NinXztwB8xE0Y/qH4k00ry2E+WraXxTuTAOgaV8p6mz+/NIt6g+pA68GVFXXR7C03586aXwNrg4+W6RAR8QRKbqT0Dv4Pn4xEzhq1ONuwP5YGXQEYGnWcczk24k+bq2yXquUmOwOWvm4+v/xxCCjlnDiuUisK/Ass86B6GxERj6HkRkpvs9klNd/aky5x0dDQnHDvppgER7lM/bAA6oeXYrHM1R9DWgKEN4Jud1VWxMWzWCCyef5r1duIiHgMJTdSOjmZsH0uAHOsl5ldT3mzCdc+s5nhXWMBuLRpnQufK+M0LJ9sPu//rPu6g+x1N6CWGxERD6JJ/KR0dv0C2akcMSJZT0u6NAoHa95SCSd3M/HOhrStH8q17UuRJCx/B7KSIboddLi5UsMuUWSB5EYtNyIiHkMtN1I6W8xC4h+tvWkZE0ZogC/UioRwc9RU0IlNjOodR0xoQMnnST4Cqz8xnw+YCF7elRl1ySLVciMi4omU3Eghs9Yd5tdtCfkbMk7Dnv8C8IP1MrrFFSgYzqu74ei60p186SSwZkHjPtDiGhdFXE511HIjIuKJ1C0lTuJPZfDEd5vxssC8v/elTb1Q2DYHbLns92nKnnMNGVtwaYUG3WDrbDiyvvDJ5j8B27533pZ+0vw64MWqnbSvKBFNAQtgqOVGRMSDqOVGnOxMSAHAZsDLP283143KGyX1zblewHmLYhZsuSmwxhSHVsKaf0P6CecHBrS7AWK7V8W3UzLfAGjcG/xCILqNu6MREREXUcuNONmTlOZ4vnLfKX5fu55+h//AwMIPub2ICfWnYe0CQ73rdgQvHzNxORtvrrZtGLBworm/063Q+2/5x3t5Q0SzKvpuSuHOHyE7HQLD3R2JiIi4iFpuxMmexFQAIoPN4dm7Fk4F4Gh4NxKJoFvjCCwFu5N8AyCmvfncXnezaz4cWQO+QWbRcEzb/EdUK/CuRjm1t68SGxERD6PkRpzYW26eG9yG6GA/+mctBeC/3pcD53VJ2dm7po6sN9eL+u1F8/WlD0KIallERKRqKbkRB6vNYG9ectMpNpzXekMLr6NkGb58erIDgPNIKbsGBepuNn0NJ3eZazX1ebiqQhcREXGoRv0D4m5Hz2SSlWvDz8eL2NqBxGUvAeA3WxeO5/gR6Ottjp46n73l5vgmcx4bgL6PQUBY4WNFxGNYrVZycnLcHYZ4ED8/P7y8Kt7uouRGHPYkmfU2TSNr4WMxzCHemHPbAHSODcfXu4gfuohmZiJzLhlSjkJoQ+h+T5XFLSJVyzAMEhISOHv2rLtDEQ/j5eVFkyZN8PPzq9B5lNyIg73epkVMCBz43VzYMiCc8NbXwcYk+raMLPqNXl7QoCvsW2y+7v+MWWgsIh7JnthER0cTFBTkPMhApJxsNhvHjh3j+PHjNGrUqEI/V0puxGFPopnctIwOhi2fmhvbDePVQV0Z1PkEfZoXk9wANOxhJjdRbaDTLVUQrYi4g9VqdSQ2deqUYqFckTKIiori2LFj5Obm4uvrW+7zKLkRh7153VKtIn1gtbkCOB2H4+fjxZWtL7A8Qc/7IeMUdLvLvetFiUilstfYBAUFuTkS8UT27iir1arkRirOZjMc3VId01dBdiqENYLYS0t3gqAIGPxWJUYoItWJuqKkMrjq50pDwQWAY8mZZGRb8fW2EH3gR3Njh5vMehoREZEaRHcuAfKLiYeG78Nr32/mxo7D3RiRiIhI+Si5EQD2JqZxtdc6Xst4EWy50HKQFpMUESlGXFwckydPdncYUgzV3AgAIbu+4yPfyfgYNmg1GG6a6u6QRERc6oorrqBz584uSUrWrl1LrVq1Kh6UVAq13Ais/je3HH0VH4uNw43+CsO/0Dw1InLRMQyD3NzcUh0bFRXl0SPGsrOz3R1ChSi5uZgZBix7E355AoBpuQPJGPR+9Vq1W0SqPcMwyMjOrfKHYRiljnH06NEsW7aMd999F4vFgsViYfr06VgsFn755Re6du2Kv78/y5cvZ9++ffz1r38lJiaG4OBgunfvzm+//eZ0vvO7pSwWC5999hnDhg0jKCiIFi1aMHfu3FLFZrVaufvuu2nSpAmBgYG0atWKd999t9BxU6dOpV27dvj7+1OvXj3GjRvn2Hf27Fnuv/9+YmJiCAgIoH379vz8888AvPDCC3Tu3NnpXJMnTyYuLs7p+gwdOpRXX32V+vXr06pVKwC+/PJLunXrRkhICHXr1uXWW28lKSnJ6Vzbtm3j+uuvJzQ0lJCQEPr27cu+ffv4/fff8fX1JSEhwen4Rx55hL59+5bq2pSX7mIXK8OAX5+FP6YAMDn3Bt633cSOqBA3ByYiNU1mjpW2E36t8s/d/tJAgvxKdxt799132b17N+3bt+ell14CzJsywFNPPcVbb71F06ZNqV27NocPH+a6667j1Vdfxd/fny+++IIhQ4awa9cuGjVqVOxnvPjii7z55pv885//5P333+e2227j0KFDRERElBibzWajYcOGfPvtt9SpU4eVK1dy3333Ua9ePYYPNwd2fPTRR4wfP57XX3+dQYMGkZyczIoVKxzvHzRoEKmpqfzf//0fzZo1Y/v27Xh7l23OsUWLFhEaGsrChQsd23Jycnj55Zdp1aoVSUlJjB8/ntGjRzN//nwAjh49yuWXX84VV1zB4sWLCQ0NZcWKFeTm5nL55ZfTtGlTvvzySx5//HHH+b766ivefPPNMsVWVkpuLkbWXPjpYdj4fwDsveQ5Jq9sS7OoWvj5qDFPRDxPWFgYfn5+BAUFUbduXQB27twJwEsvvcTVV1/tODYiIoJOnTo5Xr/88svMmTOHuXPnOrWWnG/06NGMHDkSgNdee4333nuPNWvWcO2115YYm6+vLy+++KLjdZMmTVi1ahWzZs1yJDevvPIK//jHP3j44Ycdx3Xv3h2A3377jTVr1rBjxw5atmwJQNOmTS98Uc5Tq1YtPvvsM6d1ne666y7H86ZNm/Lee+/RvXt30tLSCA4OZsqUKYSFhTFz5kzHpHv2GADuvvtupk2b5khufvrpJ86dO+f4viqLkpuLTW4WzL4bdvwEFi/46xSWpfUCttMiWq02IlJ2gb7ebH9poFs+1xW6devm9DotLY0XXniBefPmcfz4cXJzc8nMzCQ+Pr7E83Ts2NHxvFatWoSGhhbqwinOlClTmDp1KvHx8WRmZpKdne3oSkpKSuLYsWNcddVVRb5348aNNGzY0CmpKI8OHToUWrBy/fr1vPDCC2zatIkzZ85gs9kAiI+Pp23btmzcuJG+ffsWO5vw6NGjee655/jjjz+49NJLmT59OsOHD6/0YmwlNxeTrDT45nbYvwS8/eCmadDmevZ+vxmAFjHBbg5QRGoii8VS6u6h6uj8G+1jjz3GwoULeeutt2jevDmBgYHcdNNNFyyyPf8Gb7FYHMlASWbOnMljjz3G22+/Ta9evQgJCeGf//wnq1evBiAwMLDE919ov5eXV6H6JPsyGgWdfx3S09MZOHAgAwcO5KuvviIqKor4+HgGDhzouBYX+uzo6GiGDBnCtGnTaNKkCb/88gtLly4t8T2uUHN/GqVsMk7DjOFwZC341oKRM6DpFUD+gpnNo5XciIjn8vPzw2q1XvC4FStWMHr0aIYNGwaYLTkHDx6stLhWrFhB7969eeihhxzb9u3b53geEhJCXFwcixYton///oXe37FjR44cOcLu3buLbL2JiooiISEBwzAcyxts3LjxgnHt3LmTU6dO8frrrxMbGwvAunXrCn32f/7zH3JycoptvbnnnnsYOXIkDRs2pFmzZvTp0+eCn11RKrDwYJ/8vo8+ry9myGvfsu+f/eDIWlII5tOmk8ludDlgjnKwz06sbikR8WRxcXGsXr2agwcPcvLkyWJbVVq0aMH333/Pxo0b2bRpE7feemupWmDKq0WLFqxbt45ff/2V3bt38/zzz7N27VqnY1544QXefvtt3nvvPfbs2cOGDRt4//33AejXrx+XX345N954IwsXLuTAgQP88ssvLFiwADDn9zlx4gRvvvkm+/btY8qUKfzyyy8XjKtRo0b4+fnx/vvvs3//fubOncvLL7/sdMy4ceNISUnhlltuYd26dezZs4cvv/ySXbt2OY4ZOHAgoaGhvPLKK4wZM6ail6tUlNx4qL1JqbyxYBeW5EO8f+5ZmhnxJBrh3JT1PK9uqsXoaWtIzszhRFoWyZk5eFmgaZQmpBIRz/XYY4/h7e1N27ZtHV0sRXnnnXeoXbs2vXv3ZsiQIQwcOJBLLrmk0uK6//77ueGGGxgxYgQ9e/bk1KlTTq04AKNGjWLy5Ml8+OGHtGvXjuuvv549e/Y49s+ePZvu3bszcuRI2rZtyxNPPOFopWrTpg0ffvghU6ZMoVOnTqxZs4bHHnvsgnFFRUUxffp0vv32W9q2bcvrr7/OW285L5Bcp04dFi9eTFpaGv369aNr1658+umnTq04Xl5ejB49GqvVyp133lmRS1VqFqMsEwV4gJSUFMLCwkhOTiY0NNTd4biOYcD2HyD9JAbwxapD7E9K4ZGAn6ltPUV2SCz7B81gd04kT8/eTHq2lRbRwdx7eVOe+G4zcXWCWPp44eZOEZGCzp07x4EDB2jSpAkBAZrsU0rn7rvv5sSJExec+6ekn6+y3L9Vc+Mptn0P35lD9izAKABfwApEtcHvjjm0Dq1Ha6B5VDB3TV/LnqQ0nvjOLCZuri4pERFxseTkZLZs2cKMGTNKPamhK6hbylNsnAGArV5nlnn3Zp61BzsiroSeD8KY+RBaz3Fo2/qhzBnbm9Z18xOalhopJSJSKR544AGCg4OLfDzwwAPuDq9S/fWvf+Waa67hgQcecJpLqLKp5cYTpCXBviUAfBf3Ak8cyCAy2J8l9/WDgKKr1+uFBfLtA70YN+NPft9zgsuaR1ZlxCIiF42XXnqp2BoXjyqPKEJVDPsuipIbT7D1ezCs5NS9hJdXmXMPPDGwFSHFJDZ2IQG+TB/TnZRzuYQFlnysiIiUT3R0NNHR0e4O46KibilPsGUWAAu8+pJ6Lpf2DUK5qWvDUr3VYrEosREREY+ilhs3MgyDT/+3n2krDnLf5U0Z06dJkcfNXn+E1+bvIOVc4RklG3Oc33zXk2t48eL+1gBMHNIOLy9LpcYuIiJSXSm5cZNcq40Jc7cxY7U5z8KLP23n0KkMnr++Ld55iYlhGEz+bQ/vLtpT7Hmu91kOwHJbB04Sxs1dG9I9ruQVaEVERDyZkhs3SMvKZexXG1i2+wQWC1zXoR7zNh9n+sqDHDmTyXsjO+Pj5cVTszfz/Z9HAXjwimaM6hXnfCLDIHLaM5AMnQffx5p2VxEV7F/135CIiEg1ouSmiiUkn2PM9LXsOJ5CgK8X793ShWva1eW69sd5dNZGftuRyC2f/EEtPx9W7T+Ft5eFV4a2Z2SPRoVPdmQdJB8E3yDCuwwFf02oJSIiouSmCiVn5nDjRys5ejaTyGA/Ph/VnU6x4QAM7liPumH+3POfdWw+kgxALT9vPry9K/1aRhV9ws3fmF9bXw/+mqdGREQENFqqSr2/aA9Hz2YSGxHInIf6OBIbu66NI5jzUB9axYTQKCKIbx/oXXxiY80xh4ADdBxeuYGLiHiAK664gkceecRl5xs9ejRDhw512fnEddRyU0X2nUhj+sqDALwytAOxEUFFHhcXWYsFj/TFMCh5xNO+JZBxEoIioanWhBIRkbLLzs7Gz8/P3WG4nFpuKtOhVfD9fTBrFKenjWSy92S+jfiYfmd/MBe6LIbFYrnwUO5N5nILtL8RvJWjiogbGQZkp1f9owzrPo8ePZply5bx7rvvYrFYsFgsHDx4kK1btzJo0CCCg4OJiYnhjjvu4OTJk473fffdd3To0IHAwEDq1KnDgAEDSE9P54UXXuA///kPP/74o+N8pZmN98knn6Rly5YEBQXRtGlTnn/+eXJynKf5+Omnn+jevTsBAQFERkYybNgwx76srCyefPJJYmNj8ff3p3nz5nz++ecATJ8+nfDwcKdz/fDDD1gs+feTF154gc6dO/PZZ585LU65YMECLrvsMsLDw6lTpw7XX389+/btczrXkSNHGDlyJBEREdSqVYtu3bqxevVqDh48iJeXF+vWrXM6fvLkyTRu3BibzXbB6+JquitWlh0/mQtZWs0Zg7sDeAMZwPzfIWk7XPc2eJUxvzQMWPIqbJtjvu40woVBi4iUQ04GvFa/6j/3mWPgV6tUh7777rvs3r2b9u3b89JLLwHg6+tLjx49uOeee/jXv/5FZmYmTz75JMOHD2fx4sUcP36ckSNH8uabbzJs2DBSU1P53//+h2EYPPbYY+zYsYOUlBSmTZsGQETEhafhCAkJYfr06dSvX58tW7Zw7733EhISwhNPPAHAvHnzGDZsGM8++yxffPEF2dnZzJ8/3/H+O++8k1WrVvHee+/RqVMnDhw44JSMlcbevXuZPXs233//Pd7e3gCkp6czfvx4OnbsSFpaGhMmTGDYsGFs3LgRLy8v0tLS6NevHw0aNGDu3LnUrVuXDRs2YLPZiIuLY8CAAUybNo1u3bo5PmfatGmMHj0ar7Le51xAyU1l+PMrmDsODBu2FgP5IL4xJ9Ky6d2sDoMa2WD5ZFg3Fc6lwLCPwbuUMwTbbPDLE7D2U/P1VROgQddK+zZERDxFWFgYfn5+BAUFUbduXQBeeeUVunTpwmuvveY4burUqcTGxrJ7927S0tLIzc3lhhtuoHHjxgB06NDBcWxgYCBZWVmO85XGc88953geFxfHY489xsyZMx3Jzauvvsott9zCiy++6DiuU6dOAOzevZtZs2axcOFCBgwYAEDTpk3LeinIzs7miy++ICoqv6bzxhtvdDpm6tSpREVFsX37dtq3b8+MGTM4ceIEa9eudSRxzZs3dxx/zz338MADD/DOO+/g7+/Phg0b2LJlCz/++GOZ43MFJTeu9sdHsOAp83nn2/miziO8s2U3dWr58djIKyDQF+p2MLurtn4HWakw/D/gG1jyea058OPYvBFSFhj8FnS/p7K/GxGRC/MNMltR3PG5FbBp0yaWLFlCcHDh0ab79u3jmmuu4aqrrqJDhw4MHDiQa665hptuuonatWuX+zO/+eYb3nvvPfbt2+dIngounrlx40buvffeIt+7ceNGvL296devX7k/H6Bx48ZOiQ3Anj17mDBhAqtXr+bkyZOOrqT4+Hjat2/Pxo0b6dKlS7GtU0OHDmXs2LHMmTOHW265henTp9O/f3/i4uIqFGt5KblxkaycXLIWvkromncASO1yP8d7Psc7H68C4B/XtMpfw6n9jeAfCt/cAXt+hS9vgPY3lPwBe/5rPrx8YNi/ocNNlfntiIiUnsVS6u6h6iQtLY0hQ4bwxhtvFNpXr149vL29WbhwIStXruS///0v77//Ps8++yyrV6+mSZOil8spyapVq7jtttt48cUXGThwIGFhYcycOZO3337bcUxgYPF/6Ja0D8DLywvjvDqk8+t5AGrVKvxvNWTIEBo3bsynn35K/fr1sdlstG/fnuzs7FJ9tp+fH3feeSfTpk3jhhtuYMaMGbz77rslvqcyKblxkYSln9I4L7F5K+dmPlh1Oaz6HwBt6oUyonus8xtaXA13fA8zRkD8SvNxIT4BMPwLaDnQ1eGLiHg8Pz8/rFar4/Ull1zC7NmziYuLw8en6NuhxWKhT58+9OnThwkTJtC4cWPmzJnD+PHjC53vQlauXEnjxo159tlnHdsOHTrkdEzHjh1ZtGgRY8aMKfT+Dh06YLPZWLZsmaNbqqCoqChSU1NJT093JDAbN268YFynTp1i165dfPrpp/Tt2xeA5cuXF4rrs88+4/Tp08W23txzzz20b9+eDz/80NGd5y7VYrTUlClTiIuLIyAggJ49e7JmzZoSj//2229p3bo1AQEBdOjQwanYyl3ONBvKaltbXrKO5lPLjfj7eOPv40VksB+vDG3vWC/KSePeMOYX6HwbtP1ryY+OI2DUz0psRETKKS4uzjG65+TJk4wdO5bTp08zcuRI1q5dy759+/j1118ZM2YMVquV1atX89prr7Fu3Tri4+P5/vvvOXHiBG3atHGcb/PmzezatYuTJ08W2UpSUIsWLYiPj2fmzJns27eP9957jzlz5jgdM3HiRL7++msmTpzIjh072LJli6NlKS4ujlGjRnHXXXfxww8/cODAAZYuXcqsWbMA6NmzJ0FBQTzzzDPs27ePGTNmMH369Atel9q1a1OnTh0++eQT9u7dy+LFixk/frzTMSNHjqRu3boMHTqUFStWsH//fmbPns2qVascx7Rp04ZLL72UJ598kpEjR16wtadSGW42c+ZMw8/Pz5g6daqxbds249577zXCw8ONxMTEIo9fsWKF4e3tbbz55pvG9u3bjeeee87w9fU1tmzZUqrPS05ONgAjOTnZld+GyZrr+nOKiFQjmZmZxvbt243MzEx3h1Jmu3btMi699FIjMDDQAIwDBw4Yu3fvNoYNG2aEh4cbgYGBRuvWrY1HHnnEsNlsxvbt242BAwcaUVFRhr+/v9GyZUvj/fffd5wvKSnJuPrqq43g4GADMJYsWXLBGB5//HGjTp06RnBwsDFixAjjX//6lxEWFuZ0zOzZs43OnTsbfn5+RmRkpHHDDTc49mVmZhqPPvqoUa9ePcPPz89o3ry5MXXqVMf+OXPmGM2bNzcCAwON66+/3vjkk0+Mgrf6iRMnGp06dSoU18KFC402bdoY/v7+RseOHY2lS5cagDFnzhzHMQcPHjRuvPFGIzQ01AgKCjK6detmrF692uk8n3/+uQEYa9asueC1KEpJP19luX9bDKMMEwVUgp49e9K9e3c++OADAGw2G7Gxsfztb3/jqaeeKnT8iBEjSE9P5+eff3Zsu/TSS+ncuTMff/zxBT8vJSWFsLAwkpOTnYq4RETkws6dO8eBAwec5kgRsXv55Zf59ttv2bx5c7neX9LPV1nu327tlsrOzmb9+vVOfYdeXl4MGDDAqamroFWrVhXqaxw4cGCxx2dlZZGSkuL0EBEREddJS0tj69atfPDBB/ztb39zdzjuTW5OnjyJ1WolJibGaXtMTAwJCQlFvichIaFMx0+aNImwsDDHIzY2tsjjREREKuK1114jODi4yMegQYPcHV6lGjduHF27duWKK67grrvucnc4nj9a6umnn3YqjEpJSVGCIyIiLvfAAw8wfHjRCxm7tbi2CkyfPr1UxctVxa3JTWRkJN7e3iQmJjptT0xMLHbGx7p165bpeH9/f/z9/V0TsIiISDEiIiJKtQSDVD63dkv5+fnRtWtXFi1a5Nhms9lYtGgRvXr1KvI9vXr1cjoeYOHChcUeLyIirufmsSjioVz1c+X2bqnx48czatQounXrRo8ePZg8eTLp6emOCYzuvPNOGjRowKRJkwB4+OGH6devH2+//TaDBw9m5syZrFu3jk8++cSd34aIyEXB19ecaT0jI8Pju1qk6tlnRLYv6Flebk9uRowYwYkTJ5gwYQIJCQl07tyZBQsWOIqG4+PjnVYU7d27NzNmzOC5557jmWeeoUWLFvzwww+0b9/eXd+CiMhFw9vbm/DwcJKSkgAICgrCYiliklKRMrLZbJw4cYKgoKBiZ4wuLbfPc1PVNM+NiEjFGIZBQkICZ8+edXco4mG8vLxo0qQJfn5+hfaV5f7t9pYbERGpWSwWC/Xq1SM6OvqCSw6IlIWfn59Tb015KbkREZFy8fb2rnBthEhlqBYLZ4qIiIi4ipIbERER8ShKbkRERMSjXHQ1N/bBYVpAU0REpOaw37dLM8j7oktuUlNTAbS+lIiISA2UmppKWFhYicdcdPPc2Gw2jh07RkhIiMsnnrIvynn48GHNoVOJdJ2rhq5z1dB1rjq61lWjsq6zYRikpqZSv379Cw4Xv+habry8vGjYsGGlfkZoaKj+41QBXeeqoetcNXSdq46uddWojOt8oRYbOxUUi4iIiEdRciMiIiIeRcmNC/n7+zNx4kT8/f3dHYpH03WuGrrOVUPXueroWleN6nCdL7qCYhEREfFsarkRERERj6LkRkRERDyKkhsRERHxKEpuRERExKMouXGRKVOmEBcXR0BAAD179mTNmjXuDqlGmzRpEt27dyckJITo6GiGDh3Krl27nI45d+4cY8eOpU6dOgQHB3PjjTeSmJjopog9w+uvv47FYuGRRx5xbNN1dp2jR49y++23U6dOHQIDA+nQoQPr1q1z7DcMgwkTJlCvXj0CAwMZMGAAe/bscWPENY/VauX555+nSZMmBAYG0qxZM15++WWn9Yh0ncvu999/Z8iQIdSvXx+LxcIPP/zgtL801/T06dPcdttthIaGEh4ezt13301aWlrlBGxIhc2cOdPw8/Mzpk6damzbts249957jfDwcCMxMdHdodVYAwcONKZNm2Zs3brV2Lhxo3HdddcZjRo1MtLS0hzHPPDAA0ZsbKyxaNEiY926dcall15q9O7d241R12xr1qwx4uLijI4dOxoPP/ywY7uus2ucPn3aaNy4sTF69Ghj9erVxv79+41ff/3V2Lt3r+OY119/3QgLCzN++OEHY9OmTcZf/vIXo0mTJkZmZqYbI69ZXn31VaNOnTrGzz//bBw4cMD49ttvjeDgYOPdd991HKPrXHbz5883nn32WeP77783AGPOnDlO+0tzTa+99lqjU6dOxh9//GH873//M5o3b26MHDmyUuJVcuMCPXr0MMaOHet4bbVajfr16xuTJk1yY1SeJSkpyQCMZcuWGYZhGGfPnjV8fX2Nb7/91nHMjh07DMBYtWqVu8KssVJTU40WLVoYCxcuNPr16+dIbnSdXefJJ580LrvssmL322w2o27dusY///lPx7azZ88a/v7+xtdff10VIXqEwYMHG3fddZfTthtuuMG47bbbDMPQdXaF85Ob0lzT7du3G4Cxdu1axzG//PKLYbFYjKNHj7o8RnVLVVB2djbr169nwIABjm1eXl4MGDCAVatWuTEyz5KcnAxAREQEAOvXrycnJ8fpurdu3ZpGjRrpupfD2LFjGTx4sNP1BF1nV5o7dy7dunXj5ptvJjo6mi5duvDpp5869h84cICEhASnax0WFkbPnj11rcugd+/eLFq0iN27dwOwadMmli9fzqBBgwBd58pQmmu6atUqwsPD6datm+OYAQMG4OXlxerVq10e00W3cKarnTx5EqvVSkxMjNP2mJgYdu7c6aaoPIvNZuORRx6hT58+tG/fHoCEhAT8/PwIDw93OjYmJoaEhAQ3RFlzzZw5kw0bNrB27dpC+3SdXWf//v189NFHjB8/nmeeeYa1a9fy97//HT8/P0aNGuW4nkX9LtG1Lr2nnnqKlJQUWrdujbe3N1arlVdffZXbbrsNQNe5EpTmmiYkJBAdHe2038fHh4iIiEq57kpupNobO3YsW7duZfny5e4OxeMcPnyYhx9+mIULFxIQEODucDyazWajW7duvPbaawB06dKFrVu38vHHHzNq1Cg3R+c5Zs2axVdffcWMGTNo164dGzdu5JFHHqF+/fq6zhcRdUtVUGRkJN7e3oVGjyQmJlK3bl03ReU5xo0bx88//8ySJUto2LChY3vdunXJzs7m7NmzTsfrupfN+vXrSUpK4pJLLsHHxwcfHx+WLVvGe++9h4+PDzExMbrOLlKvXj3atm3rtK1NmzbEx8cDOK6nfpdUzOOPP85TTz3FLbfcQocOHbjjjjt49NFHmTRpEqDrXBlKc03r1q1LUlKS0/7c3FxOnz5dKdddyU0F+fn50bVrVxYtWuTYZrPZWLRoEb169XJjZDWbYRiMGzeOOXPmsHjxYpo0aeK0v2vXrvj6+jpd9127dhEfH6/rXgZXXXUVW7ZsYePGjY5Ht27duO222xzPdZ1do0+fPoWmM9i9ezeNGzcGoEmTJtStW9fpWqekpLB69Wpd6zLIyMjAy8v51ubt7Y3NZgN0nStDaa5pr169OHv2LOvXr3ccs3jxYmw2Gz179nR9UC4vUb4IzZw50/D39zemT59ubN++3bjvvvuM8PBwIyEhwd2h1VgPPvigERYWZixdutQ4fvy445GRkeE45oEHHjAaNWpkLF682Fi3bp3Rq1cvo1evXm6M2jMUHC1lGLrOrrJmzRrDx8fHePXVV409e/YYX331lREUFGT83//9n+OY119/3QgPDzd+/PFHY/PmzcZf//pXDVEuo1GjRhkNGjRwDAX//vvvjcjISOOJJ55wHKPrXHapqanGn3/+afz5558GYLzzzjvGn3/+aRw6dMgwjNJd02uvvdbo0qWLsXr1amP58uVGixYtNBS8unv//feNRo0aGX5+fkaPHj2MP/74w90h1WhAkY9p06Y5jsnMzDQeeugho3bt2kZQUJAxbNgw4/jx4+4L2kOcn9zoOrvOTz/9ZLRv397w9/c3WrdubXzyySdO+202m/H8888bMTExhr+/v3HVVVcZu3btclO0NVNKSorx8MMPG40aNTICAgKMpk2bGs8++6yRlZXlOEbXueyWLFlS5O/kUaNGGYZRumt66tQpY+TIkUZwcLARGhpqjBkzxkhNTa2UeC2GUWDaRhEREZEaTjU3IiIi4lGU3IiIiIhHUXIjIiIiHkXJjYiIiHgUJTciIiLiUZTciIiIiEdRciMiIiIeRcmNiIiIeBQlNyJy0Vu6dCkWi6XQAqEiUjMpuRERERGPouRGREREPIqSGxFxO5vNxqRJk2jSpAmBgYF06tSJ7777DsjvMpo3bx4dO3YkICCASy+9lK1btzqdY/bs2bRr1w5/f3/i4uJ4++23nfZnZWXx5JNPEhsbi7+/P82bN+fzzz93Omb9+vV069aNoKAgevfuza5duyr3GxeRSqHkRkTcbtKkSXzxxRd8/PHHbNu2jUcffZTbb7+dZcuWOY55/PHHefvtt1m7di1RUVEMGTKEnJwcwExKhg8fzi233MKWLVt44YUXeP7555k+fbrj/XfeeSdff/017733Hjt27ODf//43wcHBTnE8++yzvP3226xbtw4fHx/uuuuuKvn+RcS1tCq4iLhVVlYWERER/Pbbb/Tq1cux/Z577iEjI4P77ruP/v37M3PmTEaMGAHA6dOnadiwIdOnT2f48OHcdtttnDhxgv/+97+O9z/xxBPMmzePbdu2sXv3blq1asXChQsZMGBAoRiWLl1K//79+e2337jqqqsAmD9/PoMHDyYzM5OAgIBKvgoi4kpquRERt9q7dy8ZGRlcffXVBAcHOx5ffPEF+/btcxxXMPGJiIigVatW7NixA4AdO3bQp08fp/P26dOHPXv2YLVa2bhxI97e3vTr16/EWDp27Oh4Xq9ePQCSkpIq/D2KSNXycXcAInJxS0tLA2DevHk0aNDAaZ+/v79TglNegYGBpTrO19fX8dxisQBmPZCI1CxquRERt2rbti3+/v7Ex8fTvHlzp0dsbKzjuD/++MPx/MyZM+zevZs2bdoA0KZNG1asWOF03hUrVtCyZUu8vb3p0KEDNpvNqYZHRDyXWm5ExK1CQkJ47LHHePTRR7HZbFx22WUkJyezYsUKQkNDady4MQAvvfQSderUISYmhmeffZbIyEiGDh0KwD/+8Q+6d+/Oyy+/zIgRI1i1ahUffPABH374IQBxcXGMGjWKu+66i/fee49OnTpx6NAhkpKSGD58uLu+dRGpJEpuRMTtXn75ZaKiopg0aRL79+8nPDycSy65hGeeecbRLfT666/z8MMPs2fPHjp37sxPP/2En58fAJdccgmzZs1iwoQJvPzyy9SrV4+XXnqJ0aNHOz7jo48+4plnnuGhhx7i1KlTNGrUiGeeecYd366IVDKNlhKRas0+kunMmTOEh4e7OxwRqQFUcyMiIiIeRcmNiIiIeBR1S4mIiIhHUcuNiIiIeBQlNyIiIuJRlNyIiIiIR1FyIyIiIh5FyY2IiIh4FCU3IiIi4lGU3IiIiIhHUXIjIiIiHuX/ARZmHYt8tqpXAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_(x, train_Y, test_Y, ylabel):\n",
    "    plt.plot(x, train_Y, label='train_' + ylabel, linewidth=1.5)\n",
    "    plt.plot(x, test_Y, label='test_' + ylabel, linewidth=1.5)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()  # 加上图例\n",
    "    plt.savefig('./'+ylabel+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# 绘制loss曲线\n",
    "x = np.linspace(0, len(train_loss_list), len(train_loss_list))\n",
    "draw_(x, train_loss_list, test_loss_list, 'loss')\n",
    "draw_(x, train_acc_list, test_acc_list, 'accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "torch.save(net, \"./model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # 变为tensor\n",
    "     # 对数据按通道进行标准化，即减去均值，再除以方差, [0-1]->[-1,1]\n",
    "     transforms.Normalize(mean=[0.4686, 0.4853, 0.5193], std=[0.1720, 0.1863, 0.2175])\n",
    "     ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label, trans):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):  # 根据索引返回数据和对应的标签\n",
    "        return self.trans(self.data[index]), self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 调用自己创建的Dataset\n",
    "train_dataset = MyDataset(train_data, train_labels, transform)\n",
    "test_dataset = MyDataset(test_data, test_labels, transform)\n",
    "\n",
    "# 生成data loader\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9690],\n",
      "        [ 6.3604],\n",
      "        [-3.8278],\n",
      "        [-1.2188],\n",
      "        [21.2518],\n",
      "        [-2.0500],\n",
      "        [-2.9523],\n",
      "        [ 3.0764],\n",
      "        [-4.0141],\n",
      "        [ 1.9290],\n",
      "        [-0.8069],\n",
      "        [ 6.8149],\n",
      "        [-2.0128],\n",
      "        [ 0.2217],\n",
      "        [-4.1256],\n",
      "        [-3.6803],\n",
      "        [ 3.1191],\n",
      "        [-1.9025],\n",
      "        [ 2.8264],\n",
      "        [ 1.3222],\n",
      "        [-5.4291],\n",
      "        [ 2.2751],\n",
      "        [-1.0715],\n",
      "        [ 1.8027],\n",
      "        [-4.2632],\n",
      "        [-1.3508],\n",
      "        [-1.7561],\n",
      "        [ 7.8243],\n",
      "        [ 2.2372],\n",
      "        [ 5.8363],\n",
      "        [-2.9629],\n",
      "        [-3.7306],\n",
      "        [ 6.8932],\n",
      "        [ 3.2095],\n",
      "        [ 0.4880],\n",
      "        [-1.1550],\n",
      "        [-2.3469],\n",
      "        [-0.8806],\n",
      "        [-0.2054],\n",
      "        [ 0.9674],\n",
      "        [-4.5230],\n",
      "        [ 4.7652],\n",
      "        [-0.0688],\n",
      "        [-3.1219],\n",
      "        [ 2.2917],\n",
      "        [-4.0371],\n",
      "        [ 6.4076],\n",
      "        [ 0.4200],\n",
      "        [ 2.4114],\n",
      "        [ 3.4084],\n",
      "        [-5.3392],\n",
      "        [-6.1299],\n",
      "        [ 0.6896],\n",
      "        [-5.1696],\n",
      "        [-3.2813],\n",
      "        [-1.2632],\n",
      "        [ 2.0073],\n",
      "        [-1.4490],\n",
      "        [-0.2230],\n",
      "        [ 2.5679],\n",
      "        [-9.1239],\n",
      "        [ 2.0197],\n",
      "        [-3.9405],\n",
      "        [-1.3235],\n",
      "        [-2.0272],\n",
      "        [-4.1509],\n",
      "        [-2.0798],\n",
      "        [ 5.7864],\n",
      "        [-2.3451],\n",
      "        [-1.9994],\n",
      "        [-1.7486],\n",
      "        [-4.2900],\n",
      "        [-0.9746],\n",
      "        [-2.8727],\n",
      "        [-0.2237],\n",
      "        [ 4.6118],\n",
      "        [-1.1289],\n",
      "        [ 1.9009],\n",
      "        [ 1.1324],\n",
      "        [ 3.0035]], device='cuda:0', grad_fn=<ViewBackward>)\n",
      "tensor(4, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\dl\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./model.pt\")\n",
    "model.eval()\n",
    "img = Image.open(\"D:/aaaaaaaaaaaaaaaaa/nus/fyp/Q2A-master/Q2A-master/encoder/configs/train/bicycle_g8h94/images/bicycle-user.jpg\").convert(\"RGB\")  # img shape: (120, 85, 3) 高、宽、通道\n",
    "img = img.resize((64, 64), Image.ANTIALIAS)  # 宽、高\n",
    "data=[]\n",
    "data.append(img)\n",
    "label=[]\n",
    "label.append(1)\n",
    "train_dataset = MyDataset(data, label, transform)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "for data, target in train_iter:\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model(data)\n",
    "    output = output.reshape(output.shape[0],1)\n",
    "    print(output)\n",
    "    prediction=torch.argmax(output)\n",
    "    # _, prediction = torch.max(output, dim=1)\n",
    "    # prediction=prediction.cpu()\n",
    "    # print(prediction)\n",
    "    # prediction = prediction.numpy()[0]\n",
    "    print(prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycle_g8h94\n"
     ]
    }
   ],
   "source": [
    "print(classes[prediction])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20764\\789803050.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[0mnum_ftrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_ft\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0min_features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[0mmodel_ft\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclassifier\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLinear\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnum_ftrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m80\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m \u001B[0mmodel_ft\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDEVICE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m \u001B[1;31m# 选择简单暴力的Adam优化器，学习率调低\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[0moptimizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moptim\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mAdam\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel_ft\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodellr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mto\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    850\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 852\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    853\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    854\u001B[0m     def register_backward_hook(\n",
      "\u001B[1;32mE:\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    528\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    550\u001B[0m                 \u001B[1;31m# `with torch.no_grad():`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    551\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 552\u001B[1;33m                     \u001B[0mparam_applied\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    553\u001B[0m                 \u001B[0mshould_use_set_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    554\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0mshould_use_set_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\anaconda\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mconvert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    848\u001B[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001B[0;32m    849\u001B[0m                             non_blocking, memory_format=convert_to_format)\n\u001B[1;32m--> 850\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    851\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    852\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "# 设置全局参数\n",
    "modellr = 1e-4\n",
    "BATCH_SIZE =  2\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 数据预处理\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "\n",
    "])\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, label, trans):\n",
    "        self.len = len(data)\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.trans = trans\n",
    "\n",
    "    def __getitem__(self, index):  # 根据索引返回数据和对应的标签\n",
    "        return self.trans(self.data[index]), self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 调用自己创建的Dataset\n",
    "train_dataset = MyDataset(train_data, train_labels, transform)\n",
    "test_dataset = MyDataset(test_data, test_labels, transform)\n",
    "\n",
    "# 生成data loader\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# 导入数据\n",
    "# train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 实例化模型并且移动到GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model_ft = densenet121(pretrained=True)\n",
    "num_ftrs = model_ft.classifier.in_features\n",
    "model_ft.classifier = nn.Linear(num_ftrs, 80)\n",
    "model_ft.to(DEVICE)\n",
    "# 选择简单暴力的Adam优化器，学习率调低\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=modellr)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    modellrnew = modellr * (0.1 ** (epoch // 50))\n",
    "    print(\"lr:\", modellrnew)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = modellrnew\n",
    "\n",
    "\n",
    "# 定义训练过程\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    sum_loss = 0\n",
    "    total_num = len(train_loader.dataset)\n",
    "    print(total_num, len(train_loader))\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print_loss = loss.data.item()\n",
    "        sum_loss += print_loss\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                       100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "    ave_loss = sum_loss / len(train_loader)\n",
    "    print('epoch:{},loss:{}'.format(epoch, ave_loss))\n",
    "\n",
    "\n",
    "# 验证过程\n",
    "def val(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total_num = len(test_loader.dataset)\n",
    "    print(total_num, len(test_loader))\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            correct += torch.sum(pred == target)\n",
    "            print_loss = loss.data.item()\n",
    "            test_loss += print_loss\n",
    "        correct = correct.data.item()\n",
    "        acc = correct / total_num\n",
    "        avgloss = test_loss / len(test_loader)\n",
    "        print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            avgloss, correct, len(test_loader.dataset), 100 * acc))\n",
    "\n",
    "\n",
    "# 训练\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(model_ft, DEVICE, train_iter, optimizer, epoch)\n",
    "    val(model_ft, DEVICE, test_iter)\n",
    "torch.save(model_ft, 'model1.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "dl",
   "language": "python",
   "display_name": "'Python(dl)'"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}